{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting to load all data from base directory: 'C:\\Users\\james\\Downloads\\Yearly' ===\n",
      "Found year directories: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021\n",
      "--- Starting to load data for the year: 2000 ---\n",
      "Found 253 daily data files to process for 2000.\n",
      "Combining daily files for 2000 into a yearly DataFrame...\n",
      "--- Successfully loaded 791,871 rows for 2000. ---\n",
      "--- Starting to load data for the year: 2001 ---\n",
      "Found 248 daily data files to process for 2001.\n",
      "Combining daily files for 2001 into a yearly DataFrame...\n",
      "--- Successfully loaded 735,575 rows for 2001. ---\n",
      "--- Starting to load data for the year: 2002 ---\n",
      "Found 253 daily data files to process for 2002.\n",
      "Combining daily files for 2002 into a yearly DataFrame...\n",
      "--- Successfully loaded 737,188 rows for 2002. ---\n",
      "--- Starting to load data for the year: 2003 ---\n",
      "Found 253 daily data files to process for 2003.\n",
      "Combining daily files for 2003 into a yearly DataFrame...\n",
      "--- Successfully loaded 732,265 rows for 2003. ---\n",
      "--- Starting to load data for the year: 2004 ---\n",
      "Found 252 daily data files to process for 2004.\n",
      "Combining daily files for 2004 into a yearly DataFrame...\n",
      "--- Successfully loaded 743,862 rows for 2004. ---\n",
      "--- Starting to load data for the year: 2005 ---\n",
      "Found 253 daily data files to process for 2005.\n",
      "Combining daily files for 2005 into a yearly DataFrame...\n",
      "--- Successfully loaded 765,183 rows for 2005. ---\n",
      "--- Starting to load data for the year: 2006 ---\n",
      "Found 252 daily data files to process for 2006.\n",
      "Combining daily files for 2006 into a yearly DataFrame...\n",
      "--- Successfully loaded 768,574 rows for 2006. ---\n",
      "--- Starting to load data for the year: 2007 ---\n",
      "Found 250 daily data files to process for 2007.\n",
      "Combining daily files for 2007 into a yearly DataFrame...\n",
      "--- Successfully loaded 773,565 rows for 2007. ---\n",
      "--- Starting to load data for the year: 2008 ---\n",
      "Found 254 daily data files to process for 2008.\n",
      "Combining daily files for 2008 into a yearly DataFrame...\n",
      "--- Successfully loaded 768,662 rows for 2008. ---\n",
      "--- Starting to load data for the year: 2009 ---\n",
      "Found 253 daily data files to process for 2009.\n",
      "Combining daily files for 2009 into a yearly DataFrame...\n",
      "--- Successfully loaded 736,979 rows for 2009. ---\n",
      "--- Starting to load data for the year: 2010 ---\n",
      "Found 253 daily data files to process for 2010.\n",
      "Combining daily files for 2010 into a yearly DataFrame...\n",
      "--- Successfully loaded 737,302 rows for 2010. ---\n",
      "--- Starting to load data for the year: 2011 ---\n",
      "Found 253 daily data files to process for 2011.\n",
      "Combining daily files for 2011 into a yearly DataFrame...\n",
      "--- Successfully loaded 741,657 rows for 2011. ---\n",
      "--- Starting to load data for the year: 2012 ---\n",
      "Found 250 daily data files to process for 2012.\n",
      "Combining daily files for 2012 into a yearly DataFrame...\n",
      "--- Successfully loaded 738,023 rows for 2012. ---\n",
      "--- Starting to load data for the year: 2013 ---\n",
      "Found 253 daily data files to process for 2013.\n",
      "Combining daily files for 2013 into a yearly DataFrame...\n",
      "--- Successfully loaded 753,728 rows for 2013. ---\n",
      "--- Starting to load data for the year: 2014 ---\n",
      "Found 253 daily data files to process for 2014.\n",
      "Combining daily files for 2014 into a yearly DataFrame...\n",
      "--- Successfully loaded 770,916 rows for 2014. ---\n",
      "--- Starting to load data for the year: 2015 ---\n",
      "Found 253 daily data files to process for 2015.\n",
      "Combining daily files for 2015 into a yearly DataFrame...\n",
      "--- Successfully loaded 773,361 rows for 2015. ---\n",
      "--- Starting to load data for the year: 2016 ---\n",
      "Found 253 daily data files to process for 2016.\n",
      "Combining daily files for 2016 into a yearly DataFrame...\n",
      "--- Successfully loaded 753,505 rows for 2016. ---\n",
      "--- Starting to load data for the year: 2017 ---\n",
      "Found 252 daily data files to process for 2017.\n",
      "Combining daily files for 2017 into a yearly DataFrame...\n",
      "--- Successfully loaded 735,494 rows for 2017. ---\n",
      "--- Starting to load data for the year: 2018 ---\n",
      "Found 251 daily data files to process for 2018.\n",
      "Combining daily files for 2018 into a yearly DataFrame...\n",
      "--- Successfully loaded 726,529 rows for 2018. ---\n",
      "--- Starting to load data for the year: 2019 ---\n",
      "Found 253 daily data files to process for 2019.\n",
      "Combining daily files for 2019 into a yearly DataFrame...\n",
      "--- Successfully loaded 721,686 rows for 2019. ---\n",
      "--- Starting to load data for the year: 2020 ---\n",
      "Found 254 daily data files to process for 2020.\n",
      "Combining daily files for 2020 into a yearly DataFrame...\n",
      "--- Successfully loaded 709,402 rows for 2020. ---\n",
      "--- Starting to load data for the year: 2021 ---\n",
      "Found 253 daily data files to process for 2021.\n",
      "Combining daily files for 2021 into a yearly DataFrame...\n",
      "--- Successfully loaded 695,905 rows for 2021. ---\n",
      "\n",
      ">>> Combining all yearly DataFrames into a single master DataFrame...\n",
      "=== Successfully loaded a total of 16,411,232 rows from 22 years. ===\n",
      "\n",
      "--- Final Data Inspection ---\n",
      "Shape of the final loaded data: (16411232, 23)\n",
      "\n",
      "First 5 rows:\n",
      "   Unnamed: 0 ticker       open       high        low      close     volume  \\\n",
      "0         1.0    SPY  148.25000  148.25000  143.87500  145.43750  8164299.0   \n",
      "1         2.0    XLF   23.71875   23.71875   22.71875   22.87500   963200.0   \n",
      "2         3.0    XLB   26.67188   26.68750   25.93750   25.96875   140500.0   \n",
      "3         4.0    XLK   55.62500   55.62500   53.50000   55.43750   938100.0   \n",
      "4         5.0    XLV   31.00000   31.00000   30.06250   30.29688   246700.0   \n",
      "\n",
      "       OPCL    pvCLCL  prevAdjClose  ...  SICCD  PERMCO  prevRawOpen  \\\n",
      "0 -0.019154 -0.009787        146.87  ...   6726   46699    146.84375   \n",
      "1 -0.036221 -0.037475         23.77  ...   6726   34957     24.00000   \n",
      "2 -0.026716 -0.022928         26.58  ...   6726   34957     26.00000   \n",
      "3 -0.003376  0.029002         53.88  ...   6726   34957     54.15625   \n",
      "4 -0.022942 -0.019221         30.89  ...   6726   34957     30.59375   \n",
      "\n",
      "  prevRawClose  prevAdjOpen       date  volume_notional  mddv21 rhov  dhl  \n",
      "0    146.87500       146.84 2000-01-03              NaN     NaN  NaN  NaN  \n",
      "1     23.76563        24.00 2000-01-03              NaN     NaN  NaN  NaN  \n",
      "2     26.57813        26.01 2000-01-03              NaN     NaN  NaN  NaN  \n",
      "3     53.87500        54.16 2000-01-03              NaN     NaN  NaN  NaN  \n",
      "4     30.89063        30.60 2000-01-03              NaN     NaN  NaN  NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Last 5 rows:\n",
      "          Unnamed: 0 ticker    open       high        low   close     volume  \\\n",
      "16411227      2733.0    ZTS  245.16  246.99001  243.74001  244.03  1073372.0   \n",
      "16411228      2734.0    ZUO   19.02   19.27000   18.59000   18.68   598408.0   \n",
      "16411229      2735.0    ZVO    1.32    1.36000    1.27000    1.27    56025.0   \n",
      "16411230      2736.0    ZWS   36.12   36.87000   35.85000   36.40   543326.0   \n",
      "16411231      2737.0   ZYME   16.02   17.20000   15.92000   16.39  1689398.0   \n",
      "\n",
      "              OPCL    pvCLCL  prevAdjClose  ...  SICCD  PERMCO  prevRawOpen  \\\n",
      "16411227 -0.004620 -0.004609        245.16  ...   2834   54327       247.05   \n",
      "16411228 -0.018038 -0.018392         19.03  ...   7372   56314        19.10   \n",
      "16411229 -0.038615 -0.030534          1.31  ...   8221   53132         1.26   \n",
      "16411230  0.007722  0.004415         36.24  ...   3569   54014        36.71   \n",
      "16411231  0.022833  0.012979         16.18  ...   8731   55912        15.05   \n",
      "\n",
      "         prevRawClose  prevAdjOpen       date  volume_notional  mddv21 rhov  \\\n",
      "16411227       245.16       247.06 2021-12-31              NaN     NaN  NaN   \n",
      "16411228        19.03        19.10 2021-12-31              NaN     NaN  NaN   \n",
      "16411229         1.31         1.26 2021-12-31              NaN     NaN  NaN   \n",
      "16411230        36.24        36.71 2021-12-31              NaN     NaN  NaN   \n",
      "16411231        16.18        15.09 2021-12-31              NaN     NaN  NaN   \n",
      "\n",
      "          dhl  \n",
      "16411227  NaN  \n",
      "16411228  NaN  \n",
      "16411229  NaN  \n",
      "16411230  NaN  \n",
      "16411231  NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Data types and non-null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16411232 entries, 0 to 16411231\n",
      "Data columns (total 23 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   Unnamed: 0       float64       \n",
      " 1   ticker           object        \n",
      " 2   open             float64       \n",
      " 3   high             float64       \n",
      " 4   low              float64       \n",
      " 5   close            float64       \n",
      " 6   volume           float64       \n",
      " 7   OPCL             float64       \n",
      " 8   pvCLCL           float64       \n",
      " 9   prevAdjClose     float64       \n",
      " 10  SPpvCLCL         float64       \n",
      " 11  sharesOut        int64         \n",
      " 12  PERMNO           int64         \n",
      " 13  SICCD            object        \n",
      " 14  PERMCO           int64         \n",
      " 15  prevRawOpen      float64       \n",
      " 16  prevRawClose     float64       \n",
      " 17  prevAdjOpen      float64       \n",
      " 18  date             datetime64[ns]\n",
      " 19  volume_notional  float64       \n",
      " 20  mddv21           float64       \n",
      " 21  rhov             float64       \n",
      " 22  dhl              float64       \n",
      "dtypes: datetime64[ns](1), float64(17), int64(3), object(2)\n",
      "memory usage: 2.8+ GB\n",
      "\n",
      "Data spans from 2000-01-03 to 2021-12-31\n",
      "\n",
      "Average daily volume for SPY across all years: 110,092,397 shares\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def load_yearly_data(year: int, base_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads all daily data files for a specific year into a single pandas DataFrame.\n",
    "\n",
    "    This function navigates to the specified year's directory, reads all '.csv.gz'\n",
    "    and '.csv' files, adds a 'date' column based on the filename, and concatenates\n",
    "    them into one large DataFrame.\n",
    "\n",
    "    Args:\n",
    "        year (int): The year for which to load data (e.g., 2020).\n",
    "        base_path (str): The root directory containing the yearly data folders.\n",
    "                         (e.g., '/path/to/your/data_directory')\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all data for the specified year,\n",
    "                      with an added 'date' column. Returns an empty DataFrame\n",
    "                      if the directory is not found or contains no data files.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting to load data for the year: {year} ---\")\n",
    "\n",
    "    # Construct the path to the year's data directory\n",
    "    year_str = str(year)\n",
    "    year_path = os.path.join(base_path, year_str)\n",
    "\n",
    "    # Check if the directory for the year exists\n",
    "    if not os.path.isdir(year_path):\n",
    "        print(f\"Error: Directory not found at '{year_path}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Find all relevant data files (both .csv.gz and .csv)\n",
    "    # Using glob is robust and finds all matching files.\n",
    "    # We primarily look for .gz files as they are the standard for most days.\n",
    "    # The set() will automatically handle the duplicate day (raw .csv and .gz)\n",
    "    # on the first business day of the year.\n",
    "\n",
    "    file_pattern = os.path.join(year_path, f\"{year_str}*.gz\")\n",
    "    gz_files = glob.glob(file_pattern)\n",
    "\n",
    "    # Also check for the uncompressed CSV for the first day of the year\n",
    "    csv_pattern = os.path.join(year_path, f\"{year_str}*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "    # Combine lists and get unique file paths\n",
    "    all_files = sorted(list(set(gz_files + csv_files)))\n",
    "\n",
    "    if not all_files:\n",
    "        print(f\"No data files found in '{year_path}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    daily_dataframes = []\n",
    "    total_files = len(all_files)\n",
    "\n",
    "    print(f\"Found {total_files} daily data files to process for {year}.\")\n",
    "\n",
    "    for i, file_path in enumerate(all_files):\n",
    "        # Extract the date string (e.g., '20200102') from the filename\n",
    "        # os.path.basename -> '20200102.csv.gz'\n",
    "        # .split('.')[0]  -> '20200102'\n",
    "        filename = os.path.basename(file_path)\n",
    "        date_str = filename.split('.')[0]\n",
    "\n",
    "        # This handles the case where we might process both the .csv and .csv.gz\n",
    "        # for the first day. We only want to process each date once.\n",
    "        # We check if the last processed date is the same as the current one.\n",
    "        if daily_dataframes and daily_dataframes[-1]['date'].iloc[0] == pd.to_datetime(date_str, format='%Y%m%d'):\n",
    "            continue\n",
    "\n",
    "        # print(f\"  ({i+1}/{total_files}) Reading file: {filename}\") # This can be too verbose when loading all years\n",
    "\n",
    "        try:\n",
    "            # pandas' read_csv can automatically handle .gz compression\n",
    "            # The documentation lists the column headers, which we can trust\n",
    "            # are in the file.\n",
    "            daily_df = pd.read_csv(file_path, compression='infer')\n",
    "\n",
    "            # Add the date column for analysis\n",
    "            daily_df['date'] = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "\n",
    "            daily_dataframes.append(daily_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Could not read or process file {filename}. Error: {e}\")\n",
    "\n",
    "    if not daily_dataframes:\n",
    "        print(f\"No data was successfully loaded for {year}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all the daily DataFrames into a single one for the whole year\n",
    "    print(f\"Combining daily files for {year} into a yearly DataFrame...\")\n",
    "    yearly_df = pd.concat(daily_dataframes, ignore_index=True)\n",
    "\n",
    "    print(f\"--- Successfully loaded {len(yearly_df):,} rows for {year}. ---\")\n",
    "\n",
    "    return yearly_df\n",
    "\n",
    "def load_all_data(base_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads data from all available year directories into a single DataFrame.\n",
    "\n",
    "    This function scans the base_path for subdirectories named as years\n",
    "    (e.g., '2019', '2020'), uses the `load_yearly_data` function for each,\n",
    "    and concatenates all of them into one large DataFrame.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The root directory containing the yearly data folders.\n",
    "                         (e.g., '/path/to/your/data_directory')\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single DataFrame containing all data from all found years.\n",
    "                      Returns an empty DataFrame if the base directory is not found\n",
    "                      or no valid year subdirectories are found.\n",
    "    \"\"\"\n",
    "    print(f\"=== Starting to load all data from base directory: '{base_path}' ===\")\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Error: Base directory not found at '{base_path}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Find all subdirectories that look like years (e.g., '2020', '2021')\n",
    "    try:\n",
    "        # Use os.scandir for efficiency and filter for directories that are 4-digit numbers\n",
    "        year_dirs = sorted([\n",
    "            d.name for d in os.scandir(base_path)\n",
    "            if d.is_dir() and d.name.isdigit() and len(d.name) == 4\n",
    "        ])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Cannot access directory '{base_path}'.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not year_dirs:\n",
    "        print(f\"No year directories (e.g., '2020', '2021') found in '{base_path}'.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found year directories: {', '.join(year_dirs)}\")\n",
    "\n",
    "    all_dataframes = []\n",
    "    for year_str in year_dirs:\n",
    "        year = int(year_str)\n",
    "        # Call the existing function to load data for one year\n",
    "        yearly_df = load_yearly_data(year=year, base_path=base_path)\n",
    "\n",
    "        if not yearly_df.empty:\n",
    "            all_dataframes.append(yearly_df)\n",
    "        else:\n",
    "            print(f\"Warning: No data was loaded for the year {year}. Skipping.\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        print(\"No data could be loaded from any of the year directories.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all the yearly DataFrames into a single master DataFrame\n",
    "    print(\"\\n>>> Combining all yearly DataFrames into a single master DataFrame...\")\n",
    "    master_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    print(f\"=== Successfully loaded a total of {len(master_df):,} rows from {len(year_dirs)} years. ===\")\n",
    "\n",
    "    return master_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # =================================================================\n",
    "    # HOW TO USE THE FUNCTIONS\n",
    "    # =================================================================\n",
    "\n",
    "    # 1. Set the path to the main folder containing your yearly data folders.\n",
    "    #    For example, if your files are in 'D:/FinancialData/2020/20200102.csv.gz',\n",
    "    #    then your base_path is 'D:/FinancialData'.\n",
    "    #    (Using a raw string r'...' or forward slashes '/' is good practice for paths)\n",
    "    DATA_DIRECTORY = r'C:\\Users\\james\\Downloads\\Yearly' # Example for a local 'Data/Yearly' subfolder\n",
    "\n",
    "\n",
    "    # 2. Call the function to load all data from all year folders\n",
    "    all_data = load_all_data(base_path=DATA_DIRECTORY)\n",
    "\n",
    "    # 3. Inspect the resulting DataFrame\n",
    "    if not all_data.empty:\n",
    "        print(\"\\n--- Final Data Inspection ---\")\n",
    "        print(f\"Shape of the final loaded data: {all_data.shape}\")\n",
    "\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(all_data.head())\n",
    "\n",
    "        print(\"\\nLast 5 rows:\")\n",
    "        print(all_data.tail())\n",
    "\n",
    "        print(\"\\nData types and non-null counts:\")\n",
    "        all_data.info()\n",
    "\n",
    "        if 'SICCD' in all_data.columns:\n",
    "            print(\"Cleaning 'SICCD' column...\")\n",
    "            # Step 1: Convert to numeric, forcing errors to become 'NaN'\n",
    "            all_data['SICCD'] = pd.to_numeric(all_data['SICCD'], errors='coerce')\n",
    "\n",
    "            # Step 2: Convert the column to a nullable integer type.\n",
    "            # This preserves the numbers as integers while properly handling missing values.\n",
    "            all_data['SICCD'] = all_data['SICCD'].astype('Int64')\n",
    "            print(\"'SICCD' column converted to Int64.\")\n",
    "        else:\n",
    "            print(\"Column 'SICCD' not found, skipping cleaning step.\")\n",
    "\n",
    "        print(\"\\n--- Data Inspection (After Cleaning) ---\")\n",
    "        # Notice the change in dtype for SICCD from 'object' to 'Int64'\n",
    "        all_data.info()\n",
    "\n",
    "        # Verify the date range\n",
    "        min_date = all_data['date'].min().strftime('%Y-%m-%d')\n",
    "        max_date = all_data['date'].max().strftime('%Y-%m-%d')\n",
    "        print(f\"\\nData spans from {min_date} to {max_date}\")\n",
    "\n",
    "        # Example of a simple analysis across all years:\n",
    "        if 'SPY' in all_data['ticker'].values:\n",
    "            spy_avg_volume = all_data[all_data['ticker'] == 'SPY']['volume'].mean()\n",
    "            print(f\"\\nAverage daily volume for SPY across all years: {spy_avg_volume:,.0f} shares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_parquet(\n",
    "    'all_data.parquet',\n",
    "    compression='snappy'  # Compresses the file to save space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load file from: C:\\Users\\james\\ICAIF_25\\Current_Code\\Data\\all_data_consolidated.parquet\n",
      "\n",
      "File loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>OPCL</th>\n",
       "      <th>pvCLCL</th>\n",
       "      <th>prevAdjClose</th>\n",
       "      <th>...</th>\n",
       "      <th>SICCD</th>\n",
       "      <th>PERMCO</th>\n",
       "      <th>prevRawOpen</th>\n",
       "      <th>prevRawClose</th>\n",
       "      <th>prevAdjOpen</th>\n",
       "      <th>date</th>\n",
       "      <th>volume_notional</th>\n",
       "      <th>mddv21</th>\n",
       "      <th>rhov</th>\n",
       "      <th>dhl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SPY</td>\n",
       "      <td>148.25000</td>\n",
       "      <td>148.25000</td>\n",
       "      <td>143.87500</td>\n",
       "      <td>145.43750</td>\n",
       "      <td>8164299.0</td>\n",
       "      <td>-0.019154</td>\n",
       "      <td>-0.009787</td>\n",
       "      <td>146.87</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>46699</td>\n",
       "      <td>146.84375</td>\n",
       "      <td>146.87500</td>\n",
       "      <td>146.84</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>XLF</td>\n",
       "      <td>23.71875</td>\n",
       "      <td>23.71875</td>\n",
       "      <td>22.71875</td>\n",
       "      <td>22.87500</td>\n",
       "      <td>963200.0</td>\n",
       "      <td>-0.036221</td>\n",
       "      <td>-0.037475</td>\n",
       "      <td>23.77</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>34957</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>23.76563</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>XLB</td>\n",
       "      <td>26.67188</td>\n",
       "      <td>26.68750</td>\n",
       "      <td>25.93750</td>\n",
       "      <td>25.96875</td>\n",
       "      <td>140500.0</td>\n",
       "      <td>-0.026716</td>\n",
       "      <td>-0.022928</td>\n",
       "      <td>26.58</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>34957</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>26.57813</td>\n",
       "      <td>26.01</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>XLK</td>\n",
       "      <td>55.62500</td>\n",
       "      <td>55.62500</td>\n",
       "      <td>53.50000</td>\n",
       "      <td>55.43750</td>\n",
       "      <td>938100.0</td>\n",
       "      <td>-0.003376</td>\n",
       "      <td>0.029002</td>\n",
       "      <td>53.88</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>34957</td>\n",
       "      <td>54.15625</td>\n",
       "      <td>53.87500</td>\n",
       "      <td>54.16</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>XLV</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>30.06250</td>\n",
       "      <td>30.29688</td>\n",
       "      <td>246700.0</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.019221</td>\n",
       "      <td>30.89</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>34957</td>\n",
       "      <td>30.59375</td>\n",
       "      <td>30.89063</td>\n",
       "      <td>30.60</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ticker       open       high        low      close     volume  \\\n",
       "0         1.0    SPY  148.25000  148.25000  143.87500  145.43750  8164299.0   \n",
       "1         2.0    XLF   23.71875   23.71875   22.71875   22.87500   963200.0   \n",
       "2         3.0    XLB   26.67188   26.68750   25.93750   25.96875   140500.0   \n",
       "3         4.0    XLK   55.62500   55.62500   53.50000   55.43750   938100.0   \n",
       "4         5.0    XLV   31.00000   31.00000   30.06250   30.29688   246700.0   \n",
       "\n",
       "       OPCL    pvCLCL  prevAdjClose  ...  SICCD  PERMCO  prevRawOpen  \\\n",
       "0 -0.019154 -0.009787        146.87  ...   6726   46699    146.84375   \n",
       "1 -0.036221 -0.037475         23.77  ...   6726   34957     24.00000   \n",
       "2 -0.026716 -0.022928         26.58  ...   6726   34957     26.00000   \n",
       "3 -0.003376  0.029002         53.88  ...   6726   34957     54.15625   \n",
       "4 -0.022942 -0.019221         30.89  ...   6726   34957     30.59375   \n",
       "\n",
       "   prevRawClose  prevAdjOpen       date  volume_notional  mddv21 rhov  dhl  \n",
       "0     146.87500       146.84 2000-01-03              NaN     NaN  NaN  NaN  \n",
       "1      23.76563        24.00 2000-01-03              NaN     NaN  NaN  NaN  \n",
       "2      26.57813        26.01 2000-01-03              NaN     NaN  NaN  NaN  \n",
       "3      53.87500        54.16 2000-01-03              NaN     NaN  NaN  NaN  \n",
       "4      30.89063        30.60 2000-01-03              NaN     NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os # Import the 'os' module\n",
    "\n",
    "# Your directory path\n",
    "DATA_DIRECTORY = r'C:\\Users\\james\\ICAIF_25\\Current_Code\\Data'\n",
    "\n",
    "# The name of your file\n",
    "file_name = 'all_data_consolidated.parquet'\n",
    "\n",
    "# Create the full, correct path to the file\n",
    "full_path = os.path.join(DATA_DIRECTORY, file_name)\n",
    "\n",
    "# --- Check that the path is correct before loading (optional but good practice) ---\n",
    "print(f\"Attempting to load file from: {full_path}\")\n",
    "\n",
    "# Now, use the full path to read the file\n",
    "new = pd.read_parquet(full_path, engine='pyarrow')\n",
    "\n",
    "print(\"\\nFile loaded successfully!\")\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.equals(all_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "285J",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
