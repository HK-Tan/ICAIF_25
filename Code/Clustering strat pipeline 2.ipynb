{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STANDARD NOTEBOOK TO GET RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import process\n",
    "import numpy as np\n",
    "# Jerome path : r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\OPCL_20000103_20201231.csv'\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/OPCL_20000103_20201231.csv'\n",
    "# James path : r'C:\\Users\\james\\ICAIF_25\\Data\\OPCL_20000103_20201231.csv'\n",
    "df = pd.read_csv(r'C:\\Users\\james\\ICAIF_25\\Data\\OPCL_20000103_20201231.csv')\n",
    "\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns.str[1:], format='%Y%m%d').strftime('%d/%m/%Y')\n",
    "\n",
    "df_cleaned = df.dropna() # Utilisez la méthode fillna(0) pour remplacer les NaN par 0\n",
    "\n",
    "df_cleaned = df_cleaned.transpose() ## WE WANT COLUMNS TO BE VECTOR OF RETURN FOR A GIVEN TICKER\n",
    "\n",
    "df_cleaned.iloc[5025,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the code\n",
    "\n",
    "Once the run is completed, you have: \n",
    "* the overall return -> overall return\n",
    "* the PnL associated to the strategy\n",
    "* the daily PnL associated to the strategy\n",
    "* the Sharpe Ratio associated to the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from PyFolioC import PyFolioC\n",
    "warnings.filterwarnings(\"ignore\") ## so that there are no polluting warnings as output of this cell\n",
    "\n",
    "##################################################################### PARAMETERS #####################################################################\n",
    "historical_data = df_cleaned\n",
    "number_of_repetitions = 10\n",
    "lookback_window = [3190,3265]  ## new lookback_window\n",
    "evaluation_window = 5\n",
    "number_of_clusters = 24\n",
    "cov_method = 'signed_laplacian'\n",
    "sigma = 0.01 ## on a fait bouger sigma ici\n",
    "eta = 0.01\n",
    "markowitz_type = 'expected_returns'\n",
    "beta = 0.999\n",
    "K = 4  # Number of fold for the cross validation\n",
    "tc=0.0001\n",
    "    ##################################################################### PORTFOLIO ######################################################################\n",
    "portfolio = PyFolioC(number_of_repetitions=number_of_repetitions, historical_data=historical_data, lookback_window=lookback_window, evaluation_window=evaluation_window, number_of_clusters=number_of_clusters, sigma=sigma, eta=eta, EWA_cov=True, beta=beta, short_selling=True, cov_method=cov_method, markowitz_type=markowitz_type, transaction_cost_rate=tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = portfolio.cluster_returns.transpose()\n",
    "_, n_days = X.shape\n",
    "means = X.mean(axis=1)\n",
    "for i in range(len(means)):\n",
    "    X.iloc[i, :] -= means[i]\n",
    "\n",
    "cov = ((1 - portfolio.beta)/(1 - portfolio.beta ** n_days)) * sum((portfolio.beta**(n_days - t)*np.outer(X.iloc[:, t - 1].values, X.iloc[:, t - 1].values)) for t in range(1, n_days + 1))\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from PyFolioC import PyFolioC\n",
    "warnings.filterwarnings(\"ignore\") ## so that there are no polluting warnings as output of this cell\n",
    "\n",
    "##################################################################### PARAMETERS #####################################################################\n",
    "historical_data = df_cleaned\n",
    "number_of_repetitions = 10\n",
    "lookback_window = [3190,3265]  ## new lookback_window\n",
    "evaluation_window = 5\n",
    "number_of_clusters = 24\n",
    "cov_method = 'signed_laplacian'\n",
    "sigma = 0.01 ## on a fait bouger sigma ici\n",
    "eta = 0.01\n",
    "markowitz_type = 'expected_returns'\n",
    "beta = 0.999\n",
    "K = 4  # Number of fold for the cross validation\n",
    "tc=0.0001\n",
    "    ##################################################################### PORTFOLIO ######################################################################\n",
    "portfolio = PyFolioC(number_of_repetitions=number_of_repetitions, historical_data=historical_data, lookback_window=lookback_window, evaluation_window=evaluation_window, number_of_clusters=number_of_clusters, sigma=sigma, eta=eta, EWA_cov=True, beta=beta, short_selling=True, cov_method=cov_method, markowitz_type=markowitz_type, transaction_cost_rate=tc)\n",
    "overall_return, PnL, portfolio_value, daily_PnL, Turnovers= portfolio.sliding_window_past_indep(352,include_transaction_costs=True)\n",
    "## year 2008-2009 ==> 2007:2262 if evaluation_window == 2\n",
    "## year 2008-2009 ==> 2007:2265 if evaluation_window == 5\n",
    "## year 2012-2013 ==> 3016:3265\n",
    "## year 2018-2019 ==> 4524:4774\n",
    "## year 2016-2019 ==> 4021:4774\n",
    "## year 2010-2020 ==> 2512:5279 (we go until 5277 to have a multiple of 5 for the difference)\n",
    "######### year 2013-2019 ==> 3265:5025 352 window of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.lookback_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PnL = pd.DataFrame(PnL, columns=['PnL'])\n",
    "\n",
    "\n",
    "df_PnL.to_csv(f'PnL_signed_laplacian-Short-cov_75-5-0.01_clusters=24.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from PyFolioC import PyFolioC\n",
    "warnings.filterwarnings(\"ignore\") ## so that there are no polluting warnings as output of this cell\n",
    "\n",
    "##################################################################### PARAMETERS #####################################################################\n",
    "historical_data = df_cleaned\n",
    "number_of_repetitions = 10\n",
    "lookback_window = [3190,3265]  ## new lookback_window\n",
    "evaluation_window = 5\n",
    "number_of_clusters = 17\n",
    "cov_method = 'SPONGE'\n",
    "sigma = 0.01 ## on a fait bouger sigma ici\n",
    "eta = 0\n",
    "markowitz_type = 'expected_returns'\n",
    "beta = 0.999\n",
    "K = 4  # Number of fold for the cross validation\n",
    "tc=0.0001\n",
    "for cov_method in ['SPONGE_sym', 'signed_laplacian']:\n",
    "    ##################################################################### PORTFOLIO ######################################################################\n",
    "    portfolio = PyFolioC(number_of_repetitions=number_of_repetitions, historical_data=historical_data, lookback_window=lookback_window, evaluation_window=evaluation_window, number_of_clusters=number_of_clusters, sigma=sigma, eta=eta, EWA_cov=False, beta=beta, short_selling=True, cov_method=cov_method, markowitz_type=markowitz_type, transaction_cost_rate=tc)\n",
    "    overall_return, PnL, portfolio_value, daily_PnL, Turnovers= portfolio.sliding_window_past_indep(352,include_transaction_costs=True)\n",
    "    df_PnL = pd.DataFrame(PnL, columns=['PnL'])\n",
    "    df_PnL.to_csv(f'PnL_{cov_method}-Short-cov_75-5-0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation level and estimation of future returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_window = [4265,4265]\n",
    "L1=[]\n",
    "L2=[]\n",
    "for j in range(663):\n",
    "    L1.append([])\n",
    "    L2.append([])\n",
    "    for i in range(50):\n",
    "        L1[j].append(df_cleaned.iloc[3265+i,j])\n",
    "        L2[j].append(df_cleaned.iloc[3215+i:3265+i,j].mean())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Corels=[]\n",
    "for j in range (663):\n",
    "    asset_returns = np.array(L1[j])\n",
    "    Corel=0\n",
    "    for i in range(50):\n",
    "        # Initialisation du DataFrame pour stocker les rendements bruités\n",
    "        noised_returns = asset_returns.copy()\n",
    "        noise_std_dev = 0.1\n",
    "        # Génération du bruit\n",
    "        noise=[]\n",
    "        for i in range(50):\n",
    "            noise.append( np.random.normal(0, noise_std_dev))\n",
    "        # Ajout du bruit aux rendements de l'actif\n",
    "        noised_returns = asset_returns + noise\n",
    "        noised_returns\n",
    "        # Calculer min et max de x et y\n",
    "        x_min, x_max = asset_returns.min(), asset_returns.max()\n",
    "        y_min, y_max = noised_returns.min(), noised_returns.max()\n",
    "        rescale=max(np.abs(y_min), y_max)\n",
    "        # Mise à l'échelle de y pour qu'elle corresponde à l'échelle de x\n",
    "        y_scaled = (noised_returns- y_min) * (x_max - x_min) / (y_max - y_min) + x_min\n",
    "\n",
    "        correlation_matrix = np.corrcoef(asset_returns, y_scaled)\n",
    "        Corel+=correlation_matrix[0, 1]/50\n",
    "    Corels.append(Corel)\n",
    "print(np.array(Corels).mean())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Données pour la première courbe\n",
    "x1 = [i for i in range(1, 51)]\n",
    "y1= asset_returns\n",
    "\n",
    "# Données pour la deuxième courbe\n",
    "y2 = y_scaled\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure()\n",
    "\n",
    "# Ajout des courbes au graphique\n",
    "plt.plot(x1, y1, label='Real returns')\n",
    "plt.plot(x1, y2, label='Noised returns')\n",
    "\n",
    "# Ajout d'un titre et des labels pour les axes\n",
    "plt.title('Comparison of noised returns and real returns for a 1% correlation level')\n",
    "plt.xlabel('Trading days')\n",
    "plt.ylabel('Daily returns')\n",
    "\n",
    "# Ajout d'une légende\n",
    "plt.legend()\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Génération de données pour les deux courbes\n",
    "x = np.array([0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.85, 0.9, 0.92, 0.95, 1])\n",
    "y1 = np.array([0.219, 0.241, 0.251, 0.258, 0.263, 0.269, 0.271, 0.276, 0.284, 0.291, 0.294, 0.306, 0.322, 0.344, 0.405])\n",
    "y2=np.array([20.6, ])\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(8, 4))  # Taille du graphique\n",
    "\n",
    "# Tracé de la première courbe\n",
    "plt.plot(x, y1, color='blue')  # Courbe sin(x) en bleu\n",
    "\n",
    "\n",
    "\n",
    "# Ajout de titres et de légendes\n",
    "plt.title('Annual return in 2013 weekly strategy in function of eta')  # Titre du graphique\n",
    "plt.xlabel('Eta: quality of estimation of future returns')  # Étiquette de l'axe des abscisses\n",
    "plt.ylabel('Annual return in 2013')\n",
    "\n",
    "\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Génération de données pour les deux courbes\n",
    "x = np.array([ 0.01, 0.02, 0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "y1 = np.array([0.269, 0.272, 0., 0., 0., 0.218, 0.22, 0.222])\n",
    "\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(8, 4))  # Taille du graphique\n",
    "\n",
    "# Tracé de la première courbe\n",
    "plt.plot(x, y1, color='blue')  # Courbe sin(x) en bleu\n",
    "\n",
    "\n",
    "\n",
    "# Ajout de titres et de légendes\n",
    "plt.title('Noise in function of the correlation between the returns and the noised returns')  # Titre du graphique\n",
    "plt.xlabel('Eta')  # Étiquette de l'axe des abscisses\n",
    "plt.ylabel('Sigma')\n",
    "\n",
    "\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_two_lines(points1, points2, ax, labels):\n",
    "    \"\"\" Fonction pour calculer et tracer deux droites à partir de points donnés sur un axe. \"\"\"\n",
    "    # Calcul et tracé pour chaque paire de points\n",
    "    for points, label in zip([points1, points2], labels):\n",
    "        p1, p2 = points\n",
    "        m = (p2[1] - p1[1]) / (p2[0] - p1[0])  # Calcul de la pente\n",
    "        b = p1[1] - m * p1[0]                # Calcul de l'ordonnée à l'origine\n",
    "        x_values = np.linspace(0, 30, 100)  # Valeurs x sur l'intervalle souhaité\n",
    "        y_values = m * x_values + b          # Valeurs y correspondantes\n",
    "        ax.plot(x_values, y_values, label=label)          # Tracer la droite\n",
    "    ax.legend()\n",
    "\n",
    "# Créer la figure et les axes pour les deux graphiques\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "labels=['weekly', 'daily']\n",
    "\n",
    "# Données pour le premier graphique\n",
    "points_graph1_droite1 = [(0, 0.224913002325013), (1, 0.21588269119)]\n",
    "points_graph1_droite2 = [(0, 0.2170871111), (1, 0.1828922402)]\n",
    "plot_two_lines(points_graph1_droite1, points_graph1_droite2, ax1, labels)\n",
    "ax1.set_title('Impact of transaction costs on portfolio returns')\n",
    "ax1.set_xlabel('Transaction costs per dollar traded in basepoint')\n",
    "ax1.set_ylabel('Annual return in 2013')\n",
    "ax1.set_xlim(0, 20)\n",
    "ax1.set_ylim(-0.2, 0.24)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Données pour le second graphique\n",
    "points_graph2_droite1 = [(0, 4.171589588), (10, 2.281795)]\n",
    "points_graph2_droite2 = [(0, 4.05045159), (1, 3.3046)]\n",
    "plot_two_lines(points_graph2_droite1, points_graph2_droite2, ax2, labels)\n",
    "ax2.set_title('Impact of transaction costs on portfolio Sharpe ratios')\n",
    "ax2.set_xlabel('Transaction costs per dollar traded in basepoint')\n",
    "ax1.set_ylabel('Sharpe ratios in 2013')\n",
    "ax2.set_xlim(0, 20)\n",
    "ax2.set_ylim(-2, 4.5)\n",
    "ax2.grid(True)\n",
    "\n",
    "# Affichage du plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Outputs\\2013-2019 jeje\\PnL_Signed_lap-Short-cov_75-5-0.01.csv')\n",
    "P=df.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_performance_metrics(PnL_Serie,duree_strat):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for a series of end-of-day portfolio values.\n",
    "\n",
    "    Parameters:\n",
    "    PnL_Serie (pd.Series): Series of end-of-day values for the portfolio\n",
    "\n",
    "    Returns:\n",
    "    dict: Performance metrics including daily returns, volatility, total return,\n",
    "          annualized return, Sharpe ratio, and Sortino ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the input is a pandas Series\n",
    "    if not isinstance(PnL_Serie, pd.Series):\n",
    "        raise TypeError(\"The end-of-day values input must be a pandas Series.\")\n",
    "\n",
    "    # Calculate daily returns\n",
    "    daily_returns = PnL_Serie.pct_change()\n",
    "\n",
    "    # Calculate the daily mean return and daily volatility\n",
    "    daily_mean_return = daily_returns.mean()\n",
    "    daily_volatility = daily_returns.std()\n",
    "\n",
    "    # Calculate total and annualized returns\n",
    "    total_return = PnL_Serie.iloc[-1]  - 1\n",
    "    annualized_return = (1 + total_return) ** (252 / duree_strat) - 1\n",
    "\n",
    "    # Calculate annualized daily mean return and volatility\n",
    "    annualized_daily_mean_return = daily_mean_return * 252\n",
    "    annualized_volatility = daily_volatility * np.sqrt(252)\n",
    "\n",
    "    # Sharpe ratio (using annualized figures)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "\n",
    "    # Calculate Sortino ratio\n",
    "    # First, get the negative returns only\n",
    "    negative_returns = daily_returns[daily_returns < 0]\n",
    "    annualized_downside_volatility = negative_returns.std() * np.sqrt(252)\n",
    "\n",
    "    # Sortino ratio uses the same mean return as Sharpe but only the downside volatility\n",
    "    sortino_ratio = annualized_return / annualized_downside_volatility\n",
    "\n",
    "    # Compile results in a dictionary\n",
    "    metrics = {\n",
    "        'Daily Returns': daily_returns,\n",
    "        'Volatility': daily_volatility,\n",
    "        'Annualized Volatility': annualized_volatility,\n",
    "        'Total Return': total_return,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Sortino Ratio': sortino_ratio\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "calculate_performance_metrics(pd.Series(P+1),1760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_returns(portfolio):\n",
    "\n",
    "    # Compute cumulative returns\n",
    "    cumulative_returns_res = (1 + portfolio).cumprod()\n",
    "\n",
    "    return cumulative_returns_res\n",
    "\n",
    "res = cumulative_returns(overall_return)\n",
    "\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save the results to csv files (APPLY THE CELLS ONLY ONCE AS IT CREATES 3 FILES !!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2008-2020'\n",
    "clustering_method = 'SPONGE_sym_short_selling'\n",
    "\n",
    "## process.save_to_csv(year=year, clustering_method=clustering_method)\n",
    "\n",
    "df_daily = pd.DataFrame(daily_PnL, columns=['Daily PnL'])\n",
    "\n",
    "df_daily.to_csv(f'daily_{year}_{clustering_method}_{evaluation_window}.csv', index=False)\n",
    "\n",
    "df_PnL = pd.DataFrame(PnL, columns=['PnL'])\n",
    "\n",
    "df_PnL.to_csv(f'PnL_{year}_{clustering_method}_{evaluation_window}.csv', index=False)\n",
    "\n",
    "df_overall_return = pd.DataFrame(overall_return.values, columns=['Return'])\n",
    "\n",
    "df_overall_return.to_csv(f'Overall_return_{year}_{clustering_method}_{evaluation_window}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get SP500 data for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2013-01-01'\n",
    "end_date = '2019-12-27'\n",
    "year = '2008-2020'\n",
    "clustering_method = 'SPONGE_sym_Short_selling'\n",
    "\n",
    "## NAIVE: SP500\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2019-12-31'\n",
    "sp500_PnL = process.get_sp500_PnL(start_date, end_date)\n",
    "\n",
    "# Generate the 1510 dates between the start and end dates\n",
    "dates = pd.date_range(start=start_date, end=end_date, periods=1510)\n",
    "\n",
    "# Interpolate the values from the original sp500_PnL vector\n",
    "interpolated_sp500_PnL = np.interp(np.linspace(0, len(sp500_PnL) - 1, 1510), np.arange(len(sp500_PnL)), sp500_PnL)\n",
    "\n",
    "\n",
    "\n",
    "portfolio_value_sp500 = cumulative_returns(interpolated_sp500_PnL)\n",
    "\n",
    "print(len(overall_return))\n",
    "print(len(portfolio_value_sp500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_returns(portfolio):\n",
    "\n",
    "    # Compute cumulative returns\n",
    "    cumulative_returns_res = (1 + portfolio).cumprod()\n",
    "\n",
    "    return cumulative_returns_res\n",
    "\n",
    "portfolio_value_clustering = cumulative_returns(overall_return)\n",
    "\n",
    "year = '2013-2019'\n",
    "clustering_method = 'SPONGE'\n",
    "\n",
    "portfolio_mark = pd.DataFrame(portfolio_value_clustering.values, columns=['PnL'])\n",
    "\n",
    "portfolio_mark.to_csv(f'PnL_{year}_{clustering_method}_shortselling_eval_window={evaluation_window}_eta={eta}_sigma={sigma}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Portfolio Cumulative PnL vs SP500 Cumulative PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the cumulative returns\n",
    "portfolio_cumulative = pd.DataFrame(index = portfolio_value_sp500.index, columns=[f'Portfolio Performance in {year}'], data=PnL)\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot the relative performance using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=portfolio_cumulative)\n",
    "sns.lineplot(data=portfolio_value_sp500, label=f'S&P 500 Index in {year}')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Cumulated Profits and Losses')\n",
    "plt.title(f'Portfolio Relative Performance vs S&P 500 in {year} - {clustering_method}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plot portfolio value alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Création de l'axe des abscisses (nombre de jours)\n",
    "\n",
    "portfolio_value = pd.DataFrame(index = sp500_PnL.index, columns=[f'Portfolio Value in {year}'], data=portfolio_value)\n",
    "\n",
    "# Configuration de seaborn pour un style agréable\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Tracer la PnL cumulative avec seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(portfolio_value, color='blue')\n",
    "\n",
    "# Ajouter des titres et des légendes\n",
    "plt.title(f'Portfolio Value of Time for an Initial Investment of $1 in {year} - {clustering_method}')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Portfolio Value')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Barplot Daily PnL vs SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Création de l'axe des abscisses (nombre de jours)\n",
    "days = np.arange(1, len(daily_PnL) + 1)\n",
    "\n",
    "# Configuration de seaborn pour un style agréable\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Tracer l'évolution quotidienne de la PnL sous forme de diagramme à barres avec seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x=days, y=daily_PnL, color='blue', width=0.8, label='Portfolio Daily PnL')  # Ajustez la largeur ici\n",
    "ax = sns.barplot(x=days, y=sp500_PnL, color='red', width=0.8, label='SP500 Index Daily PnL')\n",
    "\n",
    "# Rotation des étiquettes de l'axe des abscisses de 45 degrés avec un ajustement\n",
    "ax.set_xticks(np.arange(0,251,10))\n",
    "ax.set_xticklabels(ax.get_xticks(), rotation=90, ha='right', rotation_mode='anchor')\n",
    "\n",
    "# Ajouter des titres et des légendes\n",
    "plt.title(f'Daily PnL Evolution - {year} - {clustering_method}')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Daily PnL')\n",
    "plt.legend()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plot everything (SPONGE vs Signed Laplacian vs SP500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_PnL.iloc[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "PnL_SPONGE_5 = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Outputs/2013-2016/PnL_2013-24_05_2016_SPONGE_5_2.csv')\n",
    "PnL_SPONGE_1 = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Outputs/2018/PnL_2018_SPONGE_1.csv')\n",
    "PnL_SPONGE_sym_5 = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Outputs/2013-2016/PnL_2013-24_05_2016_SPONGE_sym_5_2.csv')\n",
    "PnL_SL_5 = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Outputs/2013-2016/PnL_2013-24_05_2016_signed_laplacian_5_2.csv')\n",
    "\n",
    "#PnL_SPONGE_5_2 = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Outputs/2019/PnL_2019_SPONGE_5_2.csv')\n",
    "#PnL_SPONGE_sym_5_2 = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Outputs/2019/PnL_2019_SPONGE_sym_5_2.csv')\n",
    "#PnL_SL_5_2 = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Outputs/2019/PnL_2019_signed_laplacian_5_2.csv')\n",
    "\n",
    "# Calculate the cumulative returns\n",
    "portfolio_cumulative_SL_5 = pd.DataFrame(index=sp500_PnL.index, columns=[f'Signed Laplacian Portfolio Performance in {year}'], data=PnL_SL_5.values)\n",
    "portfolio_cumulative_SPONGE_5 = pd.DataFrame(index=sp500_PnL.index, columns=[f'SPONGE Portfolio Performance in {year}'], data=PnL_SPONGE_5.values)\n",
    "portfolio_cumulative_SPONGE_1 = pd.DataFrame(index=sp500_PnL.index, columns=[f'SPONGE Portfolio Performance in {year}'], data=PnL_SPONGE_1.values)\n",
    "portfolio_cumulative_SPONGE_sym_5 = pd.DataFrame(index=sp500_PnL.index, columns=[f'Symmetric SPONGE Portfolio Performance in {year}'], data=PnL_SPONGE_sym_5.values)\n",
    "\n",
    "# portfolio_cumulative_SL_5_2 = pd.DataFrame(index=sp500_PnL.iloc[:-1].index, columns=[f'Signed Laplacian Portfolio Performance in {year}'], data=PnL_SL_5_2.values)\n",
    "# portfolio_cumulative_SPONGE_5_2 = pd.DataFrame(index=sp500_PnL.iloc[:-1].index, columns=[f'SPONGE Portfolio Performance in {year}'], data=PnL_SPONGE_5_2.values)\n",
    "# portfolio_cumulative_SPONGE_sym_5_2 = pd.DataFrame(index=sp500_PnL.iloc[:-1].index, columns=[f'Symmetric SPONGE Portfolio Performance in {year}'], data=PnL_SPONGE_sym_5_2.values)\n",
    "\n",
    "# Combine dataframes for seaborn plotting\n",
    "## combined_df = pd.concat([portfolio_cumulative_SL_5, portfolio_cumulative_SL_5_2, portfolio_cumulative_SPONGE_5, portfolio_cumulative_SPONGE_5_2, portfolio_cumulative_SPONGE_sym_5, portfolio_cumulative_SPONGE_sym_5_2, sp500_PnL.iloc[:-1].cumsum()], axis=1)\n",
    "combined_df = pd.concat([portfolio_cumulative_SL_5, portfolio_cumulative_SPONGE_5, portfolio_cumulative_SPONGE_sym_5, sp500_PnL.iloc[:-2].cumsum()], axis=1)\n",
    "combined_df.columns = [f'Signed Laplacian - Evaluation window {evaluation_window}', f'SPONGE - Evaluation window {evaluation_window}', f'SPONGE_sym - Evaluation window {evaluation_window}', f'S&P 500 Index in {year}']\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Plot the relative performance using Seaborn\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(data=combined_df, palette=['blue', 'red', 'orange', 'green'], alpha=0.7)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulated Profits and Losses')\n",
    "plt.title(f'Portfolio Relative Performance vs S&P 500 in {year}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Outputs\\2013-2019\\PnL_2013-2019_signed_laplacian_shortselling_eval_window=5_eta=0.02.csv\")\n",
    "PnL=df.squeeze()\n",
    "def calculate_performance_metrics(PnL_Serie,duree_strat):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for a series of end-of-day portfolio values.\n",
    "\n",
    "    Parameters:\n",
    "    PnL_Serie (pd.Series): Series of end-of-day values for the portfolio\n",
    "\n",
    "    Returns:\n",
    "    dict: Performance metrics including daily returns, volatility, total return,\n",
    "          annualized return, Sharpe ratio, and Sortino ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the input is a pandas Series\n",
    "    if not isinstance(PnL_Serie, pd.Series):\n",
    "        raise TypeError(\"The end-of-day values input must be a pandas Series.\")\n",
    "\n",
    "    # Calculate daily returns\n",
    "    daily_returns = PnL_Serie.pct_change()\n",
    "\n",
    "    # Calculate the daily mean return and daily volatility\n",
    "    daily_mean_return = daily_returns.mean()\n",
    "    daily_volatility = daily_returns.std()\n",
    "\n",
    "    # Calculate total and annualized returns\n",
    "    total_return = PnL_Serie.iloc[-1]  - 1\n",
    "    annualized_return = (1 + total_return) ** (252 / duree_strat) - 1\n",
    "\n",
    "    # Calculate annualized daily mean return and volatility\n",
    "    annualized_daily_mean_return = daily_mean_return * 252\n",
    "    annualized_volatility = daily_volatility * np.sqrt(252)\n",
    "\n",
    "    # Sharpe ratio (using annualized figures)\n",
    "    sharpe_ratio = annualized_daily_mean_return / annualized_volatility\n",
    "\n",
    "    # Calculate Sortino ratio\n",
    "    # First, get the negative returns only\n",
    "    negative_returns = daily_returns[daily_returns < 0]\n",
    "    annualized_downside_volatility = negative_returns.std() * np.sqrt(252)\n",
    "\n",
    "    # Sortino ratio uses the same mean return as Sharpe but only the downside volatility\n",
    "    sortino_ratio = annualized_daily_mean_return / annualized_downside_volatility\n",
    "\n",
    "    # Compile results in a dictionary\n",
    "    metrics = {\n",
    "        'Daily Returns': daily_returns,\n",
    "        'Volatility': daily_volatility,\n",
    "        'Annualized Volatility': annualized_volatility,\n",
    "        'Total Return': total_return,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Sortino Ratio': sortino_ratio\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "calculate_performance_metrics(PnL,duree_strat=1510)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
