{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import process\n",
    "import numpy as np \n",
    "# Jerome path : r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\DataBase.csv'\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv'\n",
    "df = pd.read_csv(r'/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv')\n",
    "\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns.str[1:], format='%Y%m%d').strftime('%d/%m/%Y')\n",
    "\n",
    "df_cleaned = df.fillna(0) # Utilisez la méthode fillna(0) pour remplacer les NaN par 0\n",
    "\n",
    "df_cleaned = df_cleaned.transpose() ## WE WANT COLUMNS TO BE VECTOR OF RETURN FOR A GIVEN TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "######################### 1. We start by randomizing the auxiliary observation matrix  ̃X from Equation (5) along the time axis #########################\n",
    "def auxilary_matrix(days, beta, df_cleaned):\n",
    "\n",
    "    ## 1. We extract the data corresponding to the returns of our assets (columns) during these d days (lines)\n",
    "    X = df_cleaned.iloc[0:days,:]\n",
    "\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  # Compute the weight matrix\n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X))\n",
    "\n",
    "    ## 3. We randomize the auxiliary matrix of observations according to the time axis\n",
    "    Randomized_X = X_tilde.transpose().sample(frac=1, axis=1, random_state=42) ## we transpose X as we want to have daily observations of the whole dataset !\n",
    "\n",
    "    return Randomized_X\n",
    "\n",
    "######################### 2. We then split the (randomized) auxiliary observations into K non-overlapping folds of equal size #########################\n",
    "def shuffle_split(array, K):\n",
    "    # Initialize ShuffleSplit\n",
    "    shuffle_split = ShuffleSplit(n_splits=K, test_size=0.2, random_state=42) \n",
    "    # test_size=0.2 : 20% des données pour l'ensemble de test, 80% pour l'ensemble d'entraînement.\n",
    "\n",
    "    # Create empty list to store splits\n",
    "    splits = []\n",
    "\n",
    "    # Perform shuffling and splitting\n",
    "    for train_index, test_index in shuffle_split.split(array):\n",
    "        train_fold = [array[i] for i in train_index]\n",
    "        test_fold = [array[i] for i in test_index]\n",
    "        splits.append((train_fold, test_fold)) ## attention à cette structure\n",
    "\n",
    "    return splits\n",
    "\n",
    "######################### 3. For each K fold configuration, we estimate the sample eigenvectors from the training set #########################\n",
    "def eigen_sample(data, train_fold): ## we train the data on this test fold\n",
    "\n",
    "    train_data = data.loc[:, train_fold]\n",
    "\n",
    "    # Calculer la moyenne de l'ensemble d'entraînement\n",
    "    mean_train = np.mean(train_data, axis=1)\n",
    "\n",
    "    # Centrer les données d'entraînement\n",
    "    centered_train_data = train_data.sub(mean_train, axis=0)\n",
    "\n",
    "    # Calculer la matrice de covariance des données d'entraînement\n",
    "    cov_matrix_train = np.cov(centered_train_data) ## size number of assets * number of assets\n",
    "\n",
    "    # Calculer les vecteurs et valeurs propres de la matrice de covariance\n",
    "    _, eigenvectors_train = np.linalg.eig(cov_matrix_train)\n",
    "\n",
    "    return eigenvectors_train\n",
    "\n",
    "def intra_fold_loss(data, test_fold, sample_eigenvector_i, beta): ## we test the data on this test fold\n",
    "\n",
    "    ## 1. get the fold cardinality \n",
    "    fold_cardinality = len(test_fold)\n",
    "\n",
    "    ## 2. sample vector of the auxiliary observation matrix from the test fold (inspired from the code above)\n",
    "\n",
    "    days = len(test_fold)\n",
    "    X = data.loc[test_fold,:]\n",
    "\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  # Compute the weight matrix\n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X))\n",
    "\n",
    "    res = (np.dot(sample_eigenvector_i, X_tilde) ** 2) / fold_cardinality\n",
    "    result = np.sum(res, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "def average_loss_i(data, splits, index, beta):\n",
    "\n",
    "    res = 0 ## to stock the overall loss\n",
    "\n",
    "    for (train_fold, test_fold) in splits:\n",
    "\n",
    "        ## sur chaque fold, on calcule les sample eigenvectors à partir du training fold correspondant\n",
    "\n",
    "        sample_eigenvector_i = eigen_sample(data=data, train_fold=train_fold)[index] ## on ne garde que l'eigenvector correspondant au bon index\n",
    "\n",
    "        ## sur chaque fold, on calcule la perte au sein du fold à partir de l'échantillon de test\n",
    "\n",
    "        res = res + intra_fold_loss(data=data, test_fold=test_fold, sample_eigenvector_i=sample_eigenvector_i, beta=beta)\n",
    "\n",
    "    res = res / len(splits) ## we average by the number of folds (which corresponds to the lengths of the splits)\n",
    "\n",
    "    return res\n",
    "\n",
    "def eigenvalue_estimator(data, splits, beta):\n",
    "\n",
    "    number_of_stocks = data.shape[0]\n",
    "\n",
    "    xi = np.array([average_loss_i(data, splits, i, beta) for i in range(number_of_stocks)])\n",
    "                  \n",
    "    return xi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "X_tilde = auxilary_matrix(days=250, beta=0.99, df_cleaned=df_cleaned)\n",
    "array = X_tilde.columns\n",
    "K = 10 \n",
    "splits = shuffle_split(array=array, K=K)\n",
    "train, test = splits[0][0], splits[0][1]\n",
    "eigenvectors_train = eigen_sample(data=X_tilde, train_fold=train)\n",
    "x1 = eigenvectors_train[0]\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695, 695)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = X_tilde.loc[:, train]\n",
    "cov = np.cov(train_data)\n",
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
