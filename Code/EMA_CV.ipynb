{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import process\n",
    "import numpy as np \n",
    "# Jerome path : r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\DataBase.csv'\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv'\n",
    "df = pd.read_csv(r'/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv')\n",
    "\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns.str[1:], format='%Y%m%d').strftime('%d/%m/%Y')\n",
    "\n",
    "df_cleaned = df.fillna(0) # Utilisez la méthode fillna(0) pour remplacer les NaN par 0\n",
    "\n",
    "df_cleaned = df_cleaned.transpose() ## WE WANT COLUMNS TO BE VECTOR OF RETURN FOR A GIVEN TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul en cours: 100%|██████████| 695/695 [34:17<00:00,  2.96s/itération]\n"
     ]
    }
   ],
   "source": [
    "from EMA_CV import EMA_CV\n",
    "EMA_CV = EMA_CV(historical_data=df_cleaned, lookback_window=[0, 250], beta=0.99, number_of_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>AA</th>\n",
       "      <th>ABM</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADX</th>\n",
       "      <th>AEE</th>\n",
       "      <th>AEG</th>\n",
       "      <th>AEM</th>\n",
       "      <th>AEP</th>\n",
       "      <th>...</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRX</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZTR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AA</th>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>5.817404e-05</td>\n",
       "      <td>1.011430e-05</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>4.842459e-06</td>\n",
       "      <td>2.806044e-05</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABM</th>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.460662e-05</td>\n",
       "      <td>1.528040e-05</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>-1.213952e-07</td>\n",
       "      <td>2.248528e-05</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABT</th>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.492889e-05</td>\n",
       "      <td>1.252639e-05</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>2.067278e-05</td>\n",
       "      <td>5.368901e-07</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADI</th>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>2.372049e-04</td>\n",
       "      <td>-2.144467e-05</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>-3.514583e-05</td>\n",
       "      <td>7.077112e-05</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADM</th>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>6.921312e-07</td>\n",
       "      <td>9.551530e-06</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>2.210010e-06</td>\n",
       "      <td>9.259835e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>6.987230e-05</td>\n",
       "      <td>9.543554e-07</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>-1.146381e-06</td>\n",
       "      <td>2.781116e-05</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-4.423178e-06</td>\n",
       "      <td>8.064010e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>8.098977e-06</td>\n",
       "      <td>1.623261e-06</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XRX</th>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>7.539650e-05</td>\n",
       "      <td>-1.247252e-05</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>-1.802317e-05</td>\n",
       "      <td>2.468418e-05</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YUM</th>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>7.048836e-05</td>\n",
       "      <td>3.874212e-06</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>8.419086e-06</td>\n",
       "      <td>2.509178e-05</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTR</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.943936e-05</td>\n",
       "      <td>-2.566563e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-2.930502e-06</td>\n",
       "      <td>6.302990e-06</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>695 rows × 695 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker        AA       ABM       ABT       ADI       ADM           ADX  \\\n",
       "ticker                                                                   \n",
       "AA      0.000428  0.000059 -0.000006  0.000220  0.000046  5.817404e-05   \n",
       "ABM     0.000059  0.000343 -0.000017  0.000269  0.000011  4.460662e-05   \n",
       "ABT    -0.000006 -0.000017  0.000339 -0.000100 -0.000001 -1.492889e-05   \n",
       "ADI     0.000220  0.000269 -0.000100  0.001654  0.000109  2.372049e-04   \n",
       "ADM     0.000046  0.000011 -0.000001  0.000109  0.000294  6.921312e-07   \n",
       "...          ...       ...       ...       ...       ...           ...   \n",
       "XLY     0.000092  0.000074 -0.000019  0.000338  0.000045  6.987230e-05   \n",
       "XOM     0.000003 -0.000004  0.000014 -0.000069 -0.000007 -4.423178e-06   \n",
       "XRX     0.000083  0.000060 -0.000028  0.000381  0.000024  7.539650e-05   \n",
       "YUM     0.000083  0.000063  0.000006  0.000299  0.000051  7.048836e-05   \n",
       "ZTR     0.000018  0.000017 -0.000004  0.000081  0.000004  1.943936e-05   \n",
       "\n",
       "ticker           AEE       AEG       AEM       AEP  ...       XLI       XLK  \\\n",
       "ticker                                              ...                       \n",
       "AA      1.011430e-05  0.000033 -0.000062  0.000013  ...  0.000082  0.000137   \n",
       "ABM     1.528040e-05  0.000050 -0.000048  0.000018  ...  0.000068  0.000134   \n",
       "ABT     1.252639e-05 -0.000004  0.000020  0.000024  ... -0.000018 -0.000061   \n",
       "ADI    -2.144467e-05  0.000114 -0.000280 -0.000030  ...  0.000306  0.000708   \n",
       "ADM     9.551530e-06  0.000014 -0.000026  0.000011  ...  0.000040  0.000060   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "XLY     9.543554e-07  0.000044 -0.000071  0.000006  ...  0.000102  0.000208   \n",
       "XOM     8.064010e-06 -0.000004  0.000019  0.000006  ... -0.000014 -0.000034   \n",
       "XRX    -1.247252e-05  0.000028 -0.000064 -0.000019  ...  0.000076  0.000202   \n",
       "YUM     3.874212e-06  0.000040 -0.000062  0.000011  ...  0.000092  0.000174   \n",
       "ZTR    -2.566563e-06  0.000012 -0.000018 -0.000002  ...  0.000022  0.000048   \n",
       "\n",
       "ticker           XLP           XLU       XLV       XLY       XOM       XRX  \\\n",
       "ticker                                                                       \n",
       "AA      4.842459e-06  2.806044e-05  0.000073  0.000092  0.000003  0.000083   \n",
       "ABM    -1.213952e-07  2.248528e-05  0.000065  0.000074 -0.000004  0.000060   \n",
       "ABT     2.067278e-05  5.368901e-07 -0.000020 -0.000019  0.000014 -0.000028   \n",
       "ADI    -3.514583e-05  7.077112e-05  0.000304  0.000338 -0.000069  0.000381   \n",
       "ADM     2.210010e-06  9.259835e-06  0.000029  0.000045 -0.000007  0.000024   \n",
       "...              ...           ...       ...       ...       ...       ...   \n",
       "XLY    -1.146381e-06  2.781116e-05  0.000102  0.000343 -0.000017  0.000089   \n",
       "XOM     8.098977e-06  1.623261e-06 -0.000011 -0.000017  0.000262  0.000001   \n",
       "XRX    -1.802317e-05  2.468418e-05  0.000079  0.000089  0.000001  0.000710   \n",
       "YUM     8.419086e-06  2.509178e-05  0.000095  0.000107 -0.000006  0.000083   \n",
       "ZTR    -2.930502e-06  6.302990e-06  0.000023  0.000025 -0.000007  0.000029   \n",
       "\n",
       "ticker       YUM       ZTR  \n",
       "ticker                      \n",
       "AA      0.000083  0.000018  \n",
       "ABM     0.000063  0.000017  \n",
       "ABT     0.000006 -0.000004  \n",
       "ADI     0.000299  0.000081  \n",
       "ADM     0.000051  0.000004  \n",
       "...          ...       ...  \n",
       "XLY     0.000107  0.000025  \n",
       "XOM    -0.000006 -0.000007  \n",
       "XRX     0.000083  0.000029  \n",
       "YUM     0.000441  0.000025  \n",
       "ZTR     0.000025  0.000224  \n",
       "\n",
       "[695 rows x 695 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMA_CV.EMA_CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote by $\\bm{X}$ the $d \\times n$ matrix of observations \\textit{i.e.}\n",
    "\n",
    "$$\n",
    "\\bm{X} = \n",
    "\\begin{bmatrix}\n",
    "    r_{1}^{(1)} & r_{1}^{(2)} & \\ldots & r_{1}^{(n)} \\\\\n",
    "    r_{2}^{(1)} & r_{2}^{(2)} & \\ldots & r_{2}^{(n)} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    r_{d}^{(1)} & r_{d}^{(2)} & \\ldots & r_{d}^{(n)} \\\\\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix} \\mathbf{r}^{(1)} | ... |\\mathbf{r}^{(n)}\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{d\\times n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $d$ corresponds to the number of days\n",
    "- $n$ corresponds to the number of stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a standard sample covariance can be generalized to include some arbitrary weight profile assigned along the time dimension. In particular, it is expressed in the following form\n",
    "\n",
    "\\begin{equation}\n",
    "\\bm{S_W} := \\frac{1}{d} \\bm{X}'W\\bm{X} \\in \\mathbb{R}^{n\\times n}\n",
    "\\end{equation}\\\\\n",
    "\n",
    "The EWA-SC as defined can be written as a weighted sample covariance matrix if we define the matrix of weighted $W$ to be: \n",
    "\n",
    "\\begin{align*}\n",
    "    W_{t,k} =\n",
    "    \\begin{cases}\n",
    "        d\\frac{1 - \\beta}{1-\\beta^d} \\beta^{d-t} & \\text{if } t = k \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "If we define the auxiliary observation matrix: \n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\bm{{X}}} := \\bm{W}^\\frac{1}{2} \\bm{X}\n",
    "\\end{align}\n",
    "\n",
    "then we can see that the EWA-SC can be expressed in a similar form as the standard uniformly weighted sample covariance. The advantage of recasting the EWA-SC in this way is that many of the refinements for the standard sample covariance that have been developed over the years are at our disposal; including shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### 1. We start by randomizing the auxiliary observation matrix  ̃X from Equation (5) along the time axis #########################\n",
    "def auxilary_matrix(lookback_window, beta, df_cleaned):\n",
    "\n",
    "    ## 1. We extract the data corresponding to the returns of our assets (columns) during these d days (lines)\n",
    "    X = df_cleaned.iloc[lookback_window[0]:lookback_window[1],:] ## shape days * number of stocks\n",
    "    days = len(lookback_window)\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    # Compute the weight matrix : shape (days, days) (if days = 250, shape (250, 250))\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(lookback_window[0], lookback_window[1])[::-1]) / (1 - beta**days)))  \n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X))\n",
    "\n",
    "    ## 3. We randomize the auxiliary matrix of observations according to the time axis\n",
    "    # Randomized_X = X_tilde.transpose().sample(frac=1, axis=1, random_state=42) ## we transpose X as we want to have daily observations of the whole dataset !\n",
    "\n",
    "    return X_tilde ## shape (days, 695)\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "\n",
    "lookback_window = [0, 250]\n",
    "beta = 0.999\n",
    "X_tilde = auxilary_matrix(lookback_window=lookback_window, beta=beta, df_cleaned=df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the (randomized) auxiliary observations into $K$ non-overlapping folds **of equal size** represented as $\\{\\mathcal{I}_k | \\mathcal{I}_k \\subset \\{1, ..., K\\}\\}_{k=1}^K$. Each set indexed by $\\mathcal{I}_k$ works as a **\"test\" fold**, while the remaining observations' indices constitute a **\"training\" fold**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "######################### 2. We then split the (randomized) auxiliary observations into K non-overlapping folds of equal size #########################\n",
    "def shuffle_split(data, K):\n",
    "    # Initialize ShuffleSplit\n",
    "    shuffle_split = ShuffleSplit(n_splits=K, test_size=0.2, random_state=42) \n",
    "    # test_size=0.2 : 20% des données pour l'ensemble de test, 80% pour l'ensemble d'entraînement.\n",
    "\n",
    "    # Create empty list to store splits\n",
    "    splits = []\n",
    "\n",
    "    # Perform shuffling and splitting\n",
    "    for train_index, test_index in shuffle_split.split(data.index):\n",
    "        train_fold = [data.index[i] for i in train_index]\n",
    "        test_fold = [data.index[i] for i in test_index]\n",
    "        splits.append((train_fold, test_fold)) ## attention à cette structure\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.05, -0.02],\n",
       "       [-0.57,  0.31, -0.  , ..., -0.01, -0.  , -0.02],\n",
       "       [-0.36, -0.23, -0.07, ...,  0.01,  0.05, -0.01],\n",
       "       ...,\n",
       "       [ 0.01, -0.  ,  0.03, ...,  0.02, -0.01, -0.04],\n",
       "       [ 0.03,  0.03, -0.  , ..., -0.02,  0.04, -0.04],\n",
       "       [-0.  , -0.01,  0.02, ...,  0.01, -0.  , -0.  ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### 3. For each K fold configuration, we estimate the sample eigenvectors from the training set #########################\n",
    "\n",
    "def eigen_sample(data, beta, train_fold):\n",
    "    ## 1. We extract the data corresponding to the returns of our assets (columns) during these d days (lines)\n",
    "    X = data.loc[train_fold] ## shape days * number of stocks\n",
    "    days = len(train_fold) \n",
    "\n",
    "    # Compute the weight matrix : shape (days, days) (if days = 250, shape (250, 250))\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  \n",
    "\n",
    "    # We compute the auxiliary matrix\n",
    "    X_tilde = np.dot(W, X)\n",
    "\n",
    "    # We compute the training sample exponential moving average \n",
    "    sample_expo_cov = np.dot(X_tilde.T, X_tilde)\n",
    "\n",
    "    # Calculer les vecteurs et valeurs propres de la matrice de covariance\n",
    "    _, eigenvectors_train = np.linalg.eigh(sample_expo_cov) ## .eigh and not .eig so that the eigenvalues are real \n",
    "\n",
    "    return eigenvectors_train\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "lookback_window = [0, 250]\n",
    "beta = 0.999\n",
    "X_tilde = auxilary_matrix(lookback_window=lookback_window, beta=beta, df_cleaned=df_cleaned)\n",
    "splits = shuffle_split(data=X_tilde, K=5)\n",
    "eigenvectors_train = eigen_sample(data=df_cleaned, beta=0.99, train_fold=splits[0][0]) ## input : df_cleaned\n",
    "eigenvectors_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a fixed exponential decay rate $\\beta \\in (0, 1)$ an its associated EWA-SC $\\bm{E}$. Remember we denoted $\\bm{\\Sigma}$ the \"true\" and unobserved covariance matrix. Both these matrices are symmetric and thus admit the following spectral decomposition: \n",
    "\n",
    "\\begin{equation}\n",
    "\\bm{E} = \\sum_{i=1}^{n} \\hat{\\lambda}_i \\hat{u}_i \\hat{u}_i', \\quad \\text{and} \\quad \\bm{\\Sigma} = \\sum_{i=1}^{n} \\lambda_i u_i u_i',\n",
    "\\end{equation}\n",
    "\n",
    "where $(\\hat{\\lambda}_1, \\ldots, \\hat{\\lambda}_n; \\hat{u}_1, \\ldots, \\hat{u}_n)$  denotes a system of sample eigenvalues and eigenvectors of $\\bm{E}$, and $(\\lambda_1, \\ldots, \\lambda_n; u_1, \\ldots, u_n)$ denotes a system of eigenvalues and eigenvectors of the \"true\" covariance $\\bm{\\Sigma}$. The eigenvalues are assumed to be sorted in ascending order.\n",
    "\n",
    "To correct the bias previously mentioned, we consider a specific framework where the sample eigenvalues should be corrected while retaining the sample eigenvectors of the original matrix. This is mathematically tantamount to write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\bm{\\Sigma}} = \\sum_{i=1}^{n} \\xi_i \\hat{u}_i \\hat{u}_i',\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bm{\\xi} = (\\xi_i)_{i=1,...,n}$  is an $n$-dimensional vector that we have to obtain. This framework is somewhat reasonable as, in absence of any **a priori** knowledge about the structure of the covariance matrix, the most natural guess that we have about the population eigenvectors is the sample eigenvectors that we observe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each $K$ fold configuration, we estimate the sample eigenvectors from the training set and then estimate an N-dimensional vector of out-of-sample variances using the test set and the sample eigenvectors. \n",
    "\n",
    "- Finally, we average the out-of-sample variance estimates over $K$ to give us the bias-corrected eigenvalue of the ith sample eigenvector portfolio denoted as $\\xi^{\\dagger}_i$ for all $i$.\n",
    "\n",
    "These two last steps are equivalent to introducing the $K$-fold cross-validation estimator:\n",
    "\n",
    "$$\n",
    "\\xi^{\\dagger}_i := \\frac{1}{K} \\sum_{k=1}^K \\sum_{t \\in \\mathcal{I}_k}  \\frac{1}{\\lvert \\mathcal{I}_k \\rvert} \\left(\\hat{u}_i[k]'\\tilde{x}_t \\right)^2, \\quad \\text{for } i = 1, \\ldots, n,\n",
    "$$\n",
    "\n",
    "where: \n",
    "- $\\lvert \\mathcal{I}_k \\rvert$ denotes the cardinality of the kth test set such that each of them is approximately equal in size, that is, $K \\lvert \\mathcal{I}_k \\rvert \\approx d$\n",
    "- Here, $\\hat{u}_i[k]$ is the $i$-th sample eigenvector of a sample covariance matrix that is obtained from the training fold, and $\\tilde{x}$ is a sample vector of the auxiliary observation matrix from the test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004855949478495614"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intra_fold_loss(data, test_fold, sample_eigenvector_i, beta): ## we test the data on this test fold\n",
    "\n",
    "    ## 1. get the fold cardinality \n",
    "    fold_cardinality = len(test_fold) ## 20% of the observations\n",
    "\n",
    "    ## 2. sample vector of the auxiliary observation matrix from the test fold (inspired from the code above)\n",
    "\n",
    "    days = len(test_fold)\n",
    "    X = data.loc[test_fold] ## shape (days, 695)\n",
    "\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  # shape (days, days)\n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X)) # shape (days, 695)\n",
    "\n",
    "    res = (np.dot(sample_eigenvector_i, X_tilde.T) ** 2)  # shape (, 695) * (695, days) = (, days)\n",
    "    result = np.sum(res) / fold_cardinality ## sum over days / size of the test sample\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "beta = 0.99\n",
    "data = X_tilde\n",
    "sample_eigenvector_i = eigenvectors_train[:, 0]\n",
    "test_fold = splits[0][1]\n",
    "intra_loss = intra_fold_loss(data=data, test_fold=test_fold, sample_eigenvector_i=sample_eigenvector_i, beta=beta)\n",
    "intra_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00046520410756772034"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_loss(data, splits, index, beta):\n",
    "\n",
    "    res = 0 ## to stock the overall loss\n",
    "\n",
    "    for (train_fold, test_fold) in splits:\n",
    "\n",
    "        ## sur chaque fold, on calcule les sample eigenvectors à partir du training fold correspondant\n",
    "\n",
    "        sample_eigenvector_i = eigen_sample(data=data, beta=beta, train_fold=train_fold)[:, index] ## on ne garde que l'eigenvector correspondant au bon index\n",
    "\n",
    "        ## sur chaque fold, on calcule la perte au sein du fold à partir de l'échantillon de test\n",
    "\n",
    "        res = res + intra_fold_loss(data=data, test_fold=test_fold, sample_eigenvector_i=sample_eigenvector_i, beta=beta)\n",
    "\n",
    "    res = res / len(splits) ## we average by the number of folds (which corresponds to the lengths of the splits)\n",
    "\n",
    "    return res\n",
    "\n",
    "# ---------------------------------------------------------------- TEST ----------------------------------------------------------------\n",
    "average_loss_0 = average_loss(data=df_cleaned, splits=splits, index=0, beta=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul en cours: 100%|██████████| 695/695 [08:34<00:00,  1.35itération/s]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=10)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eigenvalue_estimator(data, splits, beta):\n",
    "\n",
    "    number_of_stocks = len(data.columns) ## COLUMNS HAVE TO BE COMPOSED OF THE STOCKS TICKERS\n",
    "\n",
    "    xi = np.zeros(number_of_stocks)  # initialisation de x\n",
    "\n",
    "    for i in tqdm(range(number_of_stocks), desc='Calcul en cours', unit='itération'):\n",
    "        xi[i] = average_loss(data=data, splits=splits, index=i, beta=beta)   \n",
    "                       \n",
    "    return xi\n",
    "\n",
    "# ---------------------------------------------------------------- TEST ----------------------------------------------------------------\n",
    "eigenvalue_estimator = eigenvalue_estimator(data=df_cleaned, splits=splits, beta=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     Sigma \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(index\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mcolumns, columns\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mcolumns, data\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mreal(Sigma))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Sigma\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m EMA_CV(data\u001b[39m=\u001b[39;49mdf_cleaned, beta\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m, lookback_window\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m, \u001b[39m250\u001b[39;49m], number_of_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m X_tilde \u001b[39m=\u001b[39m auxilary_matrix(lookback_window\u001b[39m=\u001b[39mlookback_window, beta\u001b[39m=\u001b[39mbeta, df_cleaned\u001b[39m=\u001b[39mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m splits \u001b[39m=\u001b[39m shuffle_split(data\u001b[39m=\u001b[39mX_tilde, K\u001b[39m=\u001b[39mnumber_of_folds)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m eigenvalue_est \u001b[39m=\u001b[39m eigenvalue_estimator(data\u001b[39m=\u001b[39;49mdata, splits\u001b[39m=\u001b[39;49msplits, beta\u001b[39m=\u001b[39;49mbeta)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Initialisation de Sigma avec des zéros\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m Sigma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((S\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], S\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mcomplex128)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "def EMA_CV(data, beta, lookback_window, number_of_folds):\n",
    "\n",
    "    days = len(lookback_window)\n",
    "    ## compute the sample exponential moving average correlation matrix\n",
    "    X = data.iloc[lookback_window[0]:lookback_window[1],:]\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(lookback_window[0], lookback_window[1])[::-1]) / (1 - beta**days)))  \n",
    "    X_tilde = np.dot(W, X)  # Produit matriciel de X' et W\n",
    "    S = np.dot(X_tilde.T, X_tilde)\n",
    "\n",
    "    ## compute the eigenvectors of S\n",
    "    _, eigenvectors = np.linalg.eigh(S)\n",
    "\n",
    "    ## computes the estimator \n",
    "    X_tilde = auxilary_matrix(lookback_window=lookback_window, beta=beta, df_cleaned=data)\n",
    "    splits = shuffle_split(data=X_tilde, K=number_of_folds)\n",
    "    eigenvalue_est = eigenvalue_estimator(data=data, splits=splits, beta=beta)\n",
    "\n",
    "    # Initialisation de Sigma avec des zéros\n",
    "    Sigma = np.zeros((S.shape[0], S.shape[1]), dtype=np.complex128)\n",
    "\n",
    "    # Parcourir chaque vecteur propre et valeur propre\n",
    "    for i in range(len(data.columns)):\n",
    "        xi_dagger = eigenvalue_est[i]  # Conjugue de xi\n",
    "        ui = eigenvectors[:, i]  # i-ème vecteur propre\n",
    "\n",
    "        # Calcul du produit externe xi^† * ui * ui^† et addition à Sigma\n",
    "        Sigma += xi_dagger * np.outer(ui, ui) \n",
    "\n",
    "    # Sigma est maintenant la somme des produits xi^† * ui * ui^†\n",
    "    Sigma = pd.DataFrame(index=data.columns, columns=data.columns, data=np.real(Sigma))\n",
    "\n",
    "    return Sigma\n",
    "\n",
    "EMA_CV(data=df_cleaned, beta=0.99, lookback_window=[0, 250], number_of_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9974842727, 0.          ],\n",
       "       [0.          , 1.0025094142]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 0.99\n",
    "days = len([0, 250])\n",
    "W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
