{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import process\n",
    "import numpy as np \n",
    "# Jerome path : r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\DataBase.csv'\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv'\n",
    "df = pd.read_csv(r'/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv')\n",
    "\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns.str[1:], format='%Y%m%d').strftime('%d/%m/%Y')\n",
    "\n",
    "df_cleaned = df.fillna(0) # Utilisez la méthode fillna(0) pour remplacer les NaN par 0\n",
    "\n",
    "df_cleaned = df_cleaned.transpose() ## WE WANT COLUMNS TO BE VECTOR OF RETURN FOR A GIVEN TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EMA_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4524 - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul en cours:   3%|▎         | 20/695 [00:15<08:31,  1.32itération/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m EMA_CV\u001b[39m.\u001b[39;49mEMA_CV(data\u001b[39m=\u001b[39;49mdf_cleaned, beta\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m, lookback_window\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m, \u001b[39m250\u001b[39;49m], number_of_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.py:151\u001b[0m, in \u001b[0;36mEMA_CV\u001b[0;34m(data, beta, lookback_window, number_of_folds)\u001b[0m\n\u001b[1;32m    149\u001b[0m X_tilde \u001b[39m=\u001b[39m auxilary_matrix(lookback_window\u001b[39m=\u001b[39mlookback_window, beta\u001b[39m=\u001b[39mbeta, df_cleaned\u001b[39m=\u001b[39mdata)\n\u001b[1;32m    150\u001b[0m splits \u001b[39m=\u001b[39m shuffle_split(data\u001b[39m=\u001b[39mX_tilde, K\u001b[39m=\u001b[39mnumber_of_folds)\n\u001b[0;32m--> 151\u001b[0m eigenvalue_est \u001b[39m=\u001b[39m eigenvalue_estimator(data\u001b[39m=\u001b[39;49mdata, splits\u001b[39m=\u001b[39;49msplits, beta\u001b[39m=\u001b[39;49mbeta)\n\u001b[1;32m    153\u001b[0m \u001b[39m# Initialisation de Sigma avec des zéros\u001b[39;00m\n\u001b[1;32m    154\u001b[0m Sigma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((S\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], S\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mcomplex128)\n",
      "File \u001b[0;32m~/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.py:132\u001b[0m, in \u001b[0;36meigenvalue_estimator\u001b[0;34m(data, splits, beta)\u001b[0m\n\u001b[1;32m    129\u001b[0m xi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(number_of_stocks)  \u001b[39m# initialisation de x\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(number_of_stocks), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCalcul en cours\u001b[39m\u001b[39m'\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mitération\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 132\u001b[0m     xi[i] \u001b[39m=\u001b[39m average_loss(data\u001b[39m=\u001b[39;49mdata, splits\u001b[39m=\u001b[39;49msplits, index\u001b[39m=\u001b[39;49mi, beta\u001b[39m=\u001b[39;49mbeta)   \n\u001b[1;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m xi\n",
      "File \u001b[0;32m~/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.py:115\u001b[0m, in \u001b[0;36maverage_loss\u001b[0;34m(data, splits, index, beta)\u001b[0m\n\u001b[1;32m    109\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m## to stock the overall loss\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m (train_fold, test_fold) \u001b[39min\u001b[39;00m splits:\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m     \u001b[39m## sur chaque fold, on calcule les sample eigenvectors à partir du training fold correspondant\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     sample_eigenvector_i \u001b[39m=\u001b[39m eigen_sample(data\u001b[39m=\u001b[39;49mdata, beta\u001b[39m=\u001b[39;49mbeta, train_fold\u001b[39m=\u001b[39;49mtrain_fold)[:, index] \u001b[39m## on ne garde que l'eigenvector correspondant au bon index\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m## sur chaque fold, on calcule la perte au sein du fold à partir de l'échantillon de test\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     res \u001b[39m=\u001b[39m res \u001b[39m+\u001b[39m intra_fold_loss(data\u001b[39m=\u001b[39mdata, test_fold\u001b[39m=\u001b[39mtest_fold, sample_eigenvector_i\u001b[39m=\u001b[39msample_eigenvector_i, beta\u001b[39m=\u001b[39mbeta)\n",
      "File \u001b[0;32m~/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.py:83\u001b[0m, in \u001b[0;36meigen_sample\u001b[0;34m(data, beta, train_fold)\u001b[0m\n\u001b[1;32m     80\u001b[0m sample_expo_cov \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X_tilde\u001b[39m.\u001b[39mT, X_tilde)\n\u001b[1;32m     82\u001b[0m \u001b[39m# Calculer les vecteurs et valeurs propres de la matrice de covariance\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m _, eigenvectors_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meigh(sample_expo_cov) \u001b[39m## .eigh and not .eig so that the eigenvalues are real \u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m eigenvectors_train\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/linalg/linalg.py:1487\u001b[0m, in \u001b[0;36meigh\u001b[0;34m(a, UPLO)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     gufunc \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39meigh_up\n\u001b[1;32m   1486\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->dD\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->dd\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1487\u001b[0m w, vt \u001b[39m=\u001b[39m gufunc(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m   1488\u001b[0m w \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mastype(_realType(result_t), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1489\u001b[0m vt \u001b[39m=\u001b[39m vt\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## year 2018-2019 ==> 4524:4774\n",
    "\n",
    "beta = 0.99\n",
    "lookback_window = [4474, 4524] ## lookback window of length 50\n",
    "evaluation_window = 5\n",
    "\n",
    "\n",
    "EMA_CV.EMA_CV(data=df_cleaned, beta=0.99, lookback_window=[0, 250], number_of_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote by $\\bm{X}$ the $d \\times n$ matrix of observations \\textit{i.e.}\n",
    "\n",
    "$$\n",
    "\\bm{X} = \n",
    "\\begin{bmatrix}\n",
    "    r_{1}^{(1)} & r_{1}^{(2)} & \\ldots & r_{1}^{(n)} \\\\\n",
    "    r_{2}^{(1)} & r_{2}^{(2)} & \\ldots & r_{2}^{(n)} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    r_{d}^{(1)} & r_{d}^{(2)} & \\ldots & r_{d}^{(n)} \\\\\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix} \\mathbf{r}^{(1)} | ... |\\mathbf{r}^{(n)}\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{d\\times n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $d$ corresponds to the number of days\n",
    "- $n$ corresponds to the number of stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a standard sample covariance can be generalized to include some arbitrary weight profile assigned along the time dimension. In particular, it is expressed in the following form\n",
    "\n",
    "\\begin{equation}\n",
    "\\bm{S_W} := \\frac{1}{d} \\bm{X}'W\\bm{X} \\in \\mathbb{R}^{n\\times n}\n",
    "\\end{equation}\\\\\n",
    "\n",
    "The EWA-SC as defined can be written as a weighted sample covariance matrix if we define the matrix of weighted $W$ to be: \n",
    "\n",
    "\\begin{align*}\n",
    "    W_{t,k} =\n",
    "    \\begin{cases}\n",
    "        d\\frac{1 - \\beta}{1-\\beta^d} \\beta^{d-t} & \\text{if } t = k \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "If we define the auxiliary observation matrix: \n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\bm{{X}}} := \\bm{W}^\\frac{1}{2} \\bm{X}\n",
    "\\end{align}\n",
    "\n",
    "then we can see that the EWA-SC can be expressed in a similar form as the standard uniformly weighted sample covariance. The advantage of recasting the EWA-SC in this way is that many of the refinements for the standard sample covariance that have been developed over the years are at our disposal; including shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### 1. We start by randomizing the auxiliary observation matrix  ̃X from Equation (5) along the time axis #########################\n",
    "def auxilary_matrix(lookback_window, beta, df_cleaned):\n",
    "\n",
    "    ## 1. We extract the data corresponding to the returns of our assets (columns) during these d days (lines)\n",
    "    X = df_cleaned.iloc[lookback_window[0]:lookback_window[1],:] ## shape days * number of stocks\n",
    "    n_days = len(lookback_window)\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    # Compute the weight matrix : shape (days, days) (if days = 250, shape (250, 250))\n",
    "    W = np.sqrt(np.diag(n_days * (1 - beta) * beta**(np.arange(lookback_window[0], lookback_window[1])[::-1]) / (1 - beta**n_days)))  \n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X))\n",
    "\n",
    "    ## 3. We randomize the auxiliary matrix of observations according to the time axis\n",
    "    # Randomized_X = X_tilde.transpose().sample(frac=1, axis=1, random_state=42) ## we transpose X as we want to have daily observations of the whole dataset !\n",
    "\n",
    "    return X_tilde ## shape (days, 695)\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "\n",
    "lookback_window = [0, 250]\n",
    "beta = 0.999\n",
    "X_tilde = auxilary_matrix(lookback_window=lookback_window, beta=beta, df_cleaned=df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the (randomized) auxiliary observations into $K$ non-overlapping folds **of equal size** represented as $\\{\\mathcal{I}_k | \\mathcal{I}_k \\subset \\{1, ..., K\\}\\}_{k=1}^K$. Each set indexed by $\\mathcal{I}_k$ works as a **\"test\" fold**, while the remaining observations' indices constitute a **\"training\" fold**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "######################### 2. We then split the (randomized) auxiliary observations into K non-overlapping folds of equal size #########################\n",
    "def shuffle_split(data, K):\n",
    "    # Initialize ShuffleSplit\n",
    "    shuffle_split = ShuffleSplit(n_splits=K, test_size=0.2, random_state=42) \n",
    "    # test_size=0.2 : 20% des données pour l'ensemble de test, 80% pour l'ensemble d'entraînement.\n",
    "\n",
    "    # Create empty list to store splits\n",
    "    splits = []\n",
    "\n",
    "    # Perform shuffling and splitting\n",
    "    for train_index, test_index in shuffle_split.split(data.index):\n",
    "        train_fold = [data.index[i] for i in train_index]\n",
    "        test_fold = [data.index[i] for i in test_index]\n",
    "        splits.append((train_fold, test_fold)) ## attention à cette structure\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.05, -0.02],\n",
       "       [-0.57,  0.31, -0.  , ..., -0.01, -0.  , -0.02],\n",
       "       [-0.36, -0.23, -0.07, ...,  0.01,  0.05, -0.01],\n",
       "       ...,\n",
       "       [ 0.01, -0.  ,  0.03, ...,  0.02, -0.01, -0.04],\n",
       "       [ 0.03,  0.03, -0.  , ..., -0.02,  0.04, -0.04],\n",
       "       [-0.  , -0.01,  0.02, ...,  0.01, -0.  , -0.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### 3. For each K fold configuration, we estimate the sample eigenvectors from the training set #########################\n",
    "\n",
    "def eigen_sample(data, beta, train_fold):\n",
    "    ## 1. We extract the data corresponding to the returns of our assets (columns) during these d days (lines)\n",
    "    X = data.loc[train_fold] ## shape days * number of stocks\n",
    "    days = len(train_fold) \n",
    "\n",
    "    # Compute the weight matrix : shape (days, days) (if days = 250, shape (250, 250))\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  \n",
    "\n",
    "    # We compute the auxiliary matrix\n",
    "    X_tilde = np.dot(W, X)\n",
    "\n",
    "    # We compute the training sample exponential moving average \n",
    "    sample_expo_cov = np.dot(X_tilde.T, X_tilde)\n",
    "\n",
    "    # Calculer les vecteurs et valeurs propres de la matrice de covariance\n",
    "    _, eigenvectors_train = np.linalg.eigh(sample_expo_cov) ## .eigh and not .eig so that the eigenvalues are real \n",
    "\n",
    "    return eigenvectors_train\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "lookback_window = [0, 250]\n",
    "beta = 0.999\n",
    "X_tilde = auxilary_matrix(lookback_window=lookback_window, beta=beta, df_cleaned=df_cleaned)\n",
    "splits = shuffle_split(data=X_tilde, K=5)\n",
    "eigenvectors_train = eigen_sample(data=df_cleaned, beta=0.99, train_fold=splits[0][0]) ## input : df_cleaned\n",
    "eigenvectors_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a fixed exponential decay rate $\\beta \\in (0, 1)$ an its associated EWA-SC $\\bm{E}$. Remember we denoted $\\bm{\\Sigma}$ the \"true\" and unobserved covariance matrix. Both these matrices are symmetric and thus admit the following spectral decomposition: \n",
    "\n",
    "\\begin{equation}\n",
    "\\bm{E} = \\sum_{i=1}^{n} \\hat{\\lambda}_i \\hat{u}_i \\hat{u}_i', \\quad \\text{and} \\quad \\bm{\\Sigma} = \\sum_{i=1}^{n} \\lambda_i u_i u_i',\n",
    "\\end{equation}\n",
    "\n",
    "where $(\\hat{\\lambda}_1, \\ldots, \\hat{\\lambda}_n; \\hat{u}_1, \\ldots, \\hat{u}_n)$  denotes a system of sample eigenvalues and eigenvectors of $\\bm{E}$, and $(\\lambda_1, \\ldots, \\lambda_n; u_1, \\ldots, u_n)$ denotes a system of eigenvalues and eigenvectors of the \"true\" covariance $\\bm{\\Sigma}$. The eigenvalues are assumed to be sorted in ascending order.\n",
    "\n",
    "To correct the bias previously mentioned, we consider a specific framework where the sample eigenvalues should be corrected while retaining the sample eigenvectors of the original matrix. This is mathematically tantamount to write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\bm{\\Sigma}} = \\sum_{i=1}^{n} \\xi_i \\hat{u}_i \\hat{u}_i',\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bm{\\xi} = (\\xi_i)_{i=1,...,n}$  is an $n$-dimensional vector that we have to obtain. This framework is somewhat reasonable as, in absence of any **a priori** knowledge about the structure of the covariance matrix, the most natural guess that we have about the population eigenvectors is the sample eigenvectors that we observe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each $K$ fold configuration, we estimate the sample eigenvectors from the training set and then estimate an N-dimensional vector of out-of-sample variances using the test set and the sample eigenvectors. \n",
    "\n",
    "- Finally, we average the out-of-sample variance estimates over $K$ to give us the bias-corrected eigenvalue of the ith sample eigenvector portfolio denoted as $\\xi^{\\dagger}_i$ for all $i$.\n",
    "\n",
    "These two last steps are equivalent to introducing the $K$-fold cross-validation estimator:\n",
    "\n",
    "$$\n",
    "\\xi^{\\dagger}_i := \\frac{1}{K} \\sum_{k=1}^K \\sum_{t \\in \\mathcal{I}_k}  \\frac{1}{\\lvert \\mathcal{I}_k \\rvert} \\left(\\hat{u}_i[k]'\\tilde{x}_t \\right)^2, \\quad \\text{for } i = 1, \\ldots, n,\n",
    "$$\n",
    "\n",
    "where: \n",
    "- $\\lvert \\mathcal{I}_k \\rvert$ denotes the cardinality of the kth test set such that each of them is approximately equal in size, that is, $K \\lvert \\mathcal{I}_k \\rvert \\approx d$\n",
    "- Here, $\\hat{u}_i[k]$ is the $i$-th sample eigenvector of a sample covariance matrix that is obtained from the training fold, and $\\tilde{x}$ is a sample vector of the auxiliary observation matrix from the test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00043027390620140266"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intra_fold_loss(data, test_fold, sample_eigenvector_i, beta): ## we test the data on this test fold\n",
    "\n",
    "    ## 1. get the fold cardinality \n",
    "    fold_cardinality = len(test_fold) ## 20% of the observations\n",
    "\n",
    "    ## 2. sample vector of the auxiliary observation matrix from the test fold (inspired from the code above)\n",
    "\n",
    "    days = len(test_fold)\n",
    "    X = data.loc[test_fold] ## shape (days, 695)\n",
    "\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  # shape (days, days)\n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X)) # shape (days, 695)\n",
    "\n",
    "    res = (np.dot(sample_eigenvector_i, X_tilde.T) ** 2)  # shape (, 695) * (695, days) = (, days)\n",
    "    result = np.sum(res) / fold_cardinality ## sum over days / size of the test sample\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "beta = 0.99\n",
    "data = X_tilde\n",
    "sample_eigenvector_i = eigenvectors_train[:, 0]\n",
    "test_fold = splits[0][1]\n",
    "intra_loss = intra_fold_loss(data=data, test_fold=test_fold, sample_eigenvector_i=sample_eigenvector_i, beta=beta)\n",
    "intra_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_loss(data, splits, index, beta):\n",
    "\n",
    "    res = 0 ## to stock the overall loss\n",
    "\n",
    "    for (train_fold, test_fold) in splits:\n",
    "\n",
    "        ## sur chaque fold, on calcule les sample eigenvectors à partir du training fold correspondant\n",
    "\n",
    "        sample_eigenvector_i = eigen_sample(data=data, beta=beta, train_fold=train_fold)[:, index] ## on ne garde que l'eigenvector correspondant au bon index\n",
    "\n",
    "        ## sur chaque fold, on calcule la perte au sein du fold à partir de l'échantillon de test\n",
    "\n",
    "        res = res + intra_fold_loss(data=data, test_fold=test_fold, sample_eigenvector_i=sample_eigenvector_i, beta=beta)\n",
    "\n",
    "    res = res / len(splits) ## we average by the number of folds (which corresponds to the lengths of the splits)\n",
    "\n",
    "    return res\n",
    "\n",
    "# ---------------------------------------------------------------- TEST ----------------------------------------------------------------\n",
    "average_loss_0 = average_loss(data=df_cleaned, splits=splits, index=0, beta=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=10)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eigenvalue_estimator(data, splits, beta):\n",
    "\n",
    "    number_of_stocks = len(data.columns) ## COLUMNS HAVE TO BE COMPOSED OF THE STOCKS TICKERS\n",
    "\n",
    "    xi = np.zeros(number_of_stocks)  # initialisation de x\n",
    "\n",
    "    for i in tqdm(range(number_of_stocks), desc='Calcul en cours', unit='itération'):\n",
    "        xi[i] = average_loss(data=data, splits=splits, index=i, beta=beta)   \n",
    "                       \n",
    "    return xi\n",
    "\n",
    "# ---------------------------------------------------------------- TEST ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul en cours:   4%|▍         | 29/695 [00:21<08:22,  1.33itération/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     Sigma \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(index\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mcolumns, columns\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mcolumns, data\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mreal(Sigma))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Sigma\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m EMA_CV(data\u001b[39m=\u001b[39;49mdf_cleaned, beta\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m, lookback_window\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m, \u001b[39m250\u001b[39;49m], number_of_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m X_tilde \u001b[39m=\u001b[39m auxilary_matrix(lookback_window\u001b[39m=\u001b[39mlookback_window, beta\u001b[39m=\u001b[39mbeta, df_cleaned\u001b[39m=\u001b[39mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m splits \u001b[39m=\u001b[39m shuffle_split(data\u001b[39m=\u001b[39mX_tilde, K\u001b[39m=\u001b[39mnumber_of_folds)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m eigenvalue_est \u001b[39m=\u001b[39m eigenvalue_estimator(data\u001b[39m=\u001b[39;49mdata, splits\u001b[39m=\u001b[39;49msplits, beta\u001b[39m=\u001b[39;49mbeta)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Initialisation de Sigma avec des zéros\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m Sigma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((S\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], S\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mcomplex128)\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m xi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(number_of_stocks)  \u001b[39m# initialisation de x\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(number_of_stocks), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCalcul en cours\u001b[39m\u001b[39m'\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mitération\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     xi[i] \u001b[39m=\u001b[39m average_loss(data\u001b[39m=\u001b[39;49mdata, splits\u001b[39m=\u001b[39;49msplits, index\u001b[39m=\u001b[39;49mi, beta\u001b[39m=\u001b[39;49mbeta)   \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m xi\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m## to stock the overall loss\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m (train_fold, test_fold) \u001b[39min\u001b[39;00m splits:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m## sur chaque fold, on calcule les sample eigenvectors à partir du training fold correspondant\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     sample_eigenvector_i \u001b[39m=\u001b[39m eigen_sample(data\u001b[39m=\u001b[39;49mdata, beta\u001b[39m=\u001b[39;49mbeta, train_fold\u001b[39m=\u001b[39;49mtrain_fold)[:, index] \u001b[39m## on ne garde que l'eigenvector correspondant au bon index\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m## sur chaque fold, on calcule la perte au sein du fold à partir de l'échantillon de test\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     res \u001b[39m=\u001b[39m res \u001b[39m+\u001b[39m intra_fold_loss(data\u001b[39m=\u001b[39mdata, test_fold\u001b[39m=\u001b[39mtest_fold, sample_eigenvector_i\u001b[39m=\u001b[39msample_eigenvector_i, beta\u001b[39m=\u001b[39mbeta)\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m sample_expo_cov \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X_tilde\u001b[39m.\u001b[39mT, X_tilde)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Calculer les vecteurs et valeurs propres de la matrice de covariance\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m _, eigenvectors_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meigh(sample_expo_cov) \u001b[39m## .eigh and not .eig so that the eigenvalues are real \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m eigenvectors_train\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/linalg/linalg.py:1487\u001b[0m, in \u001b[0;36meigh\u001b[0;34m(a, UPLO)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     gufunc \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39meigh_up\n\u001b[1;32m   1486\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->dD\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->dd\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1487\u001b[0m w, vt \u001b[39m=\u001b[39m gufunc(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m   1488\u001b[0m w \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mastype(_realType(result_t), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1489\u001b[0m vt \u001b[39m=\u001b[39m vt\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def EMA_CV(data, beta, lookback_window, number_of_folds):\n",
    "\n",
    "    days = len(lookback_window)\n",
    "    ## compute the sample exponential moving average correlation matrix\n",
    "    X = data.iloc[lookback_window[0]:lookback_window[1],:]\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(lookback_window[0], lookback_window[1])[::-1]) / (1 - beta**days)))  \n",
    "    X_tilde = np.dot(W, X)  # Produit matriciel de X' et W\n",
    "    S = np.dot(X_tilde.T, X_tilde)\n",
    "\n",
    "    ## compute the eigenvectors of S\n",
    "    _, eigenvectors = np.linalg.eigh(S)\n",
    "\n",
    "    ## computes the estimator \n",
    "    X_tilde = auxilary_matrix(lookback_window=lookback_window, beta=beta, df_cleaned=data)\n",
    "    splits = shuffle_split(data=X_tilde, K=number_of_folds)\n",
    "    eigenvalue_est = eigenvalue_estimator(data=data, splits=splits, beta=beta)\n",
    "\n",
    "    # Initialisation de Sigma avec des zéros\n",
    "    Sigma = np.zeros((S.shape[0], S.shape[1]), dtype=np.complex128)\n",
    "\n",
    "    # Parcourir chaque vecteur propre et valeur propre\n",
    "    for i in range(len(data.columns)):\n",
    "        xi_dagger = eigenvalue_est[i]  # Conjugue de xi\n",
    "        ui = eigenvectors[:, i]  # i-ème vecteur propre\n",
    "\n",
    "        # Calcul du produit externe xi^† * ui * ui^† et addition à Sigma\n",
    "        Sigma += xi_dagger * np.outer(ui, ui) \n",
    "\n",
    "    # Sigma est maintenant la somme des produits xi^† * ui * ui^†\n",
    "    Sigma = pd.DataFrame(index=data.columns, columns=data.columns, data=np.real(Sigma))\n",
    "\n",
    "    return Sigma\n",
    "\n",
    "EMA_CV(data=df_cleaned, beta=0.99, lookback_window=[0, 250], number_of_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
