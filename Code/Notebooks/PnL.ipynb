{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "ckCTxIv0LB19",
        "outputId": "0a631acc-2354-4a50-c419-4b59e6018e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pykeops in /usr/local/lib/python3.11/dist-packages (2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pykeops) (2.0.2)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.11/dist-packages (from pykeops) (2.13.6)\n",
            "Requirement already satisfied: keopscore==2.3 in /usr/local/lib/python3.11/dist-packages (from pykeops) (2.3)\n",
            "Using device: cuda\n",
            "Data loaded and cleaned. Sample (first 5 rows/cols):\n",
            "ticker            AA       ABM       ABT       ADI       ADM\n",
            "2000-01-03 -0.013042 -0.009188 -0.007117 -0.036071  0.000000\n",
            "2000-01-04  0.010043  0.012346 -0.012786 -0.044261  0.005277\n",
            "2000-01-05  0.047628 -0.006192  0.011111  0.014493 -0.015915\n",
            "2000-01-06 -0.011713  0.000000  0.032553 -0.027719  0.010695\n",
            "2000-01-07 -0.016118  0.003091  0.028573  0.033654  0.005249\n",
            "Shape of the cleaned data: (5279, 143)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VAR Order Grid Search:   0%|          | 0/3 [00:11<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d0ae1739e3a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;31m# Removed 'if historical_data_tensor.numel() > 0:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar_order_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_orders_to_test_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VAR Order Grid Search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     results = run_sliding_window_var_evaluation_vmap(\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0masset_returns_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistorical_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Will error if not defined or empty and used inappropriately by functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0minitial_lookback_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_lookback_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d0ae1739e3a7>\u001b[0m in \u001b[0;36mrun_sliding_window_var_evaluation_vmap\u001b[0;34m(asset_returns_tensor, initial_lookback_len, eval_len, n_clusters_config, cluster_method, var_order, sigma_intra_cluster, num_windows, device, store_sample_forecasts, run_naive_var_comparison)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mall_pnl_c_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_forecast_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_actual_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0mall_pnl_n_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_forecast_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_actual_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m     processed_window_indices = vmapped_processor(\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0mfinal_batched_lookback_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mfinal_batched_eval_data_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         return vmap_impl(\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# If chunk_size is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     return _flat_vmap(\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         )\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mbatched_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unwrap_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d0ae1739e3a7>\u001b[0m in \u001b[0;36msingle_window_iteration_processor_refactored\u001b[0;34m(lookback_asset_returns_item, eval_asset_returns_item_padded, current_eval_len_for_window, window_idx_batch, initial_n_clusters_config_static, cluster_method_static, var_order_static, sigma_for_weights_static, max_eval_len_static, run_naive_var_comparison_static, num_assets_overall_static, device_static)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mactual_eval_asset_returns_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_asset_returns_item_padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     cluster_defs, cluster_names_ord, actual_n_clust_formed, _ = _define_clusters_and_centroids_functional(\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mlookback_asset_returns_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_n_clusters_config_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_method_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_static\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-10-d0ae1739e3a7>\u001b[0m in \u001b[0;36m_define_clusters_and_centroids_functional\u001b[0;34m(asset_returns_lookback_tensor, initial_n_clusters_config, cluster_method, device_param)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# Removed num_assets == 0 check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_apply_clustering_algorithm_functional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masset_corr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_n_clusters_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderived_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;31m# def handmade_unique(x):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m#   sorted_x, _ = torch.sort(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d0ae1739e3a7>\u001b[0m in \u001b[0;36m_apply_clustering_algorithm_functional\u001b[0;34m(correlation_matrix_tensor, num_clusters_to_form, cluster_method, device_param)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mpos_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation_matrix_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mneg_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation_matrix_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectral_clustering_laplacian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_n_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsupported cluster_method: {cluster_method}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d0ae1739e3a7>\u001b[0m in \u001b[0;36mspectral_clustering_laplacian\u001b[0;34m(p, n, k)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# # normalize the rows to have unit 2-norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# t = u / u.norm(dim=1, keepdim=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mSPONGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d0ae1739e3a7>\u001b[0m in \u001b[0;36mKMeans\u001b[0;34m(x, K, Niter, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# .squeeze() converts shape (1,) to 0-dim scalar tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mfirst_centroid_idx_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_centroid_idx_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mhas_been_chosen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_centroid_idx_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report."
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Naive strats.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/18ITgq52qINQNnAxk3gWXMr5SrrSWLx1k\n",
        "\n",
        "# **Cluster Return Prediction with VAR and Evaluation**\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# !pip install pykeops # Assuming pykeops is installed if KMeans part is used elsewhere\n",
        "from pykeops.torch import LazyTensor # Not used in the refactored VAR part\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "# import warnings # warnings.warn calls will be removed\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Global variable for KMeans, as in the original snippet (though KMeans itself is not used in this VAR logic)\n",
        "# use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# import torch # Redundant\n",
        "import torch.func as func # For vmap\n",
        "# from tqdm import tqdm # Redundant\n",
        "\n",
        "# Device configuration (use the one from the original script)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Redundant\n",
        "\n",
        "# import torch # Redundant\n",
        "import torch.nn.functional as F\n",
        "# from torch import vmap # PyTorch native vmap # Using functorch.vmap\n",
        "# import functorch as func # Redundant, already imported as func\n",
        "\n",
        "def KMeans(x, K, Niter=300, verbose=True):\n",
        "    \"\"\"\n",
        "    Implements Lloyd's algorithm with K-Means++ initialization for the Euclidean metric.\n",
        "    This version avoids using .item().\n",
        "    \"\"\"\n",
        "\n",
        "    start_total_time = time.time()\n",
        "    N, D = x.shape  # Number of samples, dimension of the ambient space\n",
        "    use_cuda = x.is_cuda\n",
        "    device = x.device\n",
        "\n",
        "    # K-Means++ initialization\n",
        "    if verbose:\n",
        "        print(\"Starting K-Means++ initialization...\")\n",
        "    start_init_time = time.time()\n",
        "\n",
        "    centroids = torch.empty(K, D, dtype=x.dtype, device=device)\n",
        "\n",
        "    # Boolean mask to track chosen centroids\n",
        "    has_been_chosen = torch.zeros(N, dtype=torch.bool, device=device)\n",
        "\n",
        "    # 1. Choose the first centroid uniformly at random from the data points\n",
        "    # Ensure reproducibility if desired by setting torch.manual_seed before calling KMeans\n",
        "    # .squeeze() converts shape (1,) to 0-dim scalar tensor\n",
        "    first_centroid_idx_tensor = torch.randint(0, N, (1,), device=device).squeeze()\n",
        "    centroids[0] = x[first_centroid_idx_tensor]\n",
        "    has_been_chosen[first_centroid_idx_tensor] = True\n",
        "\n",
        "    # 2. For each subsequent centroid (from 1 to K-1)\n",
        "    for k_idx in range(1, K):\n",
        "        current_chosen_centroids = centroids[:k_idx, :]\n",
        "\n",
        "        dist_sq_to_chosen = torch.cdist(x, current_chosen_centroids)**2\n",
        "        min_dist_sq = dist_sq_to_chosen.min(dim=1).values # Shape: (N,)\n",
        "\n",
        "        # Check if all distances are zero (e.g., all points are identical to chosen centroids)\n",
        "        if torch.isclose(min_dist_sq.sum(), torch.tensor(0.0, device=device, dtype=x.dtype)):\n",
        "            # Fallback: pick a random point not yet chosen, or if all chosen, allow re-pick.\n",
        "            if verbose:\n",
        "                print(f\"K-Means++: min_dist_sq sum is ~0 at k_idx={k_idx}. Fallback to random unchosen point.\")\n",
        "\n",
        "            available_indices = torch.where(~has_been_chosen)[0]\n",
        "            if available_indices.numel() == 0: # All points marked as chosen or no unchosen points left\n",
        "                if verbose: print(f\"Warning: No unchosen points left for K-Means++ fallback. Picking randomly from all points.\")\n",
        "                # Pick randomly from all N points\n",
        "                next_centroid_idx_tensor = torch.randint(0, N, (1,), device=device).squeeze()\n",
        "            else:\n",
        "                rand_idx_in_available = torch.randint(0, available_indices.numel(), (1,), device=device).squeeze()\n",
        "                next_centroid_idx_tensor = available_indices[rand_idx_in_available]\n",
        "        else:\n",
        "            # Choose the next centroid with probability proportional to min_dist_sq (D(x)^2)\n",
        "            # .squeeze() converts shape (1,) to 0-dim scalar tensor\n",
        "            next_centroid_idx_tensor = torch.multinomial(min_dist_sq, 1).squeeze()\n",
        "\n",
        "        centroids[k_idx] = x[next_centroid_idx_tensor]\n",
        "        has_been_chosen[next_centroid_idx_tensor] = True # Mark as chosen\n",
        "\n",
        "    c = centroids.clone()  # Initial centroids from K-Means++\n",
        "\n",
        "    if verbose:\n",
        "        end_init_time = time.time()\n",
        "        print(f\"K-Means++ initialization finished in {end_init_time - start_init_time:.5f}s.\")\n",
        "\n",
        "    # --- K-means loop (Lloyd's algorithm) ---\n",
        "    x_i = LazyTensor(x.view(N, 1, D))  # (N, 1, D) samples\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Starting Lloyd's iterations...\")\n",
        "    start_lloyd_time = time.time()\n",
        "\n",
        "    actual_iters = Niter # Default to Niter\n",
        "    for i in range(Niter):\n",
        "        c_j = LazyTensor(c.view(1, K, D))  # (1, K, D) centroids\n",
        "\n",
        "        # E step: assign points to the closest cluster -------------------------\n",
        "        # ((x_i - c_j) ** 2) results in a LazyTensor.\n",
        "        # .sum(-1) on this LazyTensor calls our dummy LazyTensor.sum(), returning a torch.Tensor.\n",
        "        D_ij_summed = ((x_i - c_j) ** 2).sum(-1)  # (N, K) symbolic squared distances, now a torch.Tensor\n",
        "        cl = D_ij_summed.argmin(dim=1).long().view(-1)  # Points -> Nearest cluster\n",
        "\n",
        "        # M step: update the centroids to the normalized cluster average: ------\n",
        "        c_old = c.clone()\n",
        "\n",
        "        c.zero_()\n",
        "        # scatter_add_ expects indices as long tensors. cl is already .long().\n",
        "        c.scatter_add_(0, cl[:, None].repeat(1, D), x)\n",
        "\n",
        "        Ncl = torch.bincount(cl, minlength=K).type_as(c).view(K, 1)\n",
        "\n",
        "        # Handle empty clusters\n",
        "        for k_loop_idx in range(K): # k_idx is already used for K-Means++ init loop\n",
        "            if Ncl[k_loop_idx] > 0:\n",
        "                c[k_loop_idx] /= Ncl[k_loop_idx]\n",
        "            else:\n",
        "                c[k_loop_idx] = c_old[k_loop_idx]\n",
        "                if verbose and i % 10 == 0 :\n",
        "                    print(f\"Iteration {i}: Cluster {k_loop_idx} is empty. Retaining old centroid.\")\n",
        "\n",
        "        # Optional: convergence check\n",
        "        if torch.allclose(c, c_old, atol=1e-6, rtol=0):\n",
        "           if verbose: print(f\"Converged at iteration {i+1}.\")\n",
        "           actual_iters = i + 1\n",
        "           break\n",
        "    else: # If loop finished without break\n",
        "        actual_iters = Niter\n",
        "\n",
        "\n",
        "    if verbose:  # Fancy display -----------------------------------------------\n",
        "        if use_cuda:\n",
        "            torch.cuda.synchronize()\n",
        "        end_lloyd_time = time.time()\n",
        "        end_total_time = time.time()\n",
        "\n",
        "        print(\n",
        "            f\"\\nK-means++ (no .item()) for the Euclidean metric with {N:,} points in dimension {D:,}, K = {K:,}:\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Initialization time: {end_init_time - start_init_time:.5f}s\"\n",
        "        )\n",
        "        print(\n",
        "            \"Lloyd's iterations ({}) : {:.5f}s = {:.5f}s/iter\".format(\n",
        "                actual_iters,\n",
        "                end_lloyd_time - start_lloyd_time,\n",
        "                (end_lloyd_time - start_lloyd_time) / actual_iters if actual_iters > 0 else 0\n",
        "            )\n",
        "        )\n",
        "        print(f\"Total time: {end_total_time - start_total_time:.5f}s\")\n",
        "\n",
        "    return cl # Return cluster assignments and final centroids\n",
        "\n",
        "\n",
        "def sqrtinvdiag(M):\n",
        "    return torch.diag(1.0 / torch.max(torch.sqrt(torch.diag(M)), torch.tensor(1e-10)))\n",
        "\n",
        "def spectral_clustering_laplacian(p, n, k):\n",
        "    D_p = torch.diag(p.sum(axis=0))\n",
        "    D_n = torch.diag(n.sum(axis=0))\n",
        "    d = sqrtinvdiag(D_p)\n",
        "    matrix = d @ p @ d\n",
        "    d = sqrtinvdiag(D_n)\n",
        "    matrix = matrix - (d @ n @ d)\n",
        "    matrix = torch.eye(matrix.shape[0], device=device) - matrix\n",
        "    w, v = torch.linalg.eigh(matrix)\n",
        "    # u = v[:, :k]\n",
        "    # # normalize the rows to have unit 2-norm\n",
        "    # t = u / u.norm(dim=1, keepdim=True)\n",
        "    return KMeans(v/w, k, verbose=False)\n",
        "\n",
        "def SPONGE(p, n, k, tau):\n",
        "    D_p = torch.diag(p.sum(axis=0))\n",
        "    D_n = torch.diag(n.sum(axis=0))\n",
        "\n",
        "def calculate_pnl_torch(forecast_tensor, actual_tensor):\n",
        "    # Removed empty tensor checks and inconsistent shape checks\n",
        "    min_len = min(forecast_tensor.shape[0], actual_tensor.shape[0])\n",
        "    common_series = min(forecast_tensor.shape[1], actual_tensor.shape[1])\n",
        "\n",
        "    forecast_aligned = forecast_tensor[:min_len, :common_series]\n",
        "    actual_aligned = actual_tensor[:min_len, :common_series]\n",
        "\n",
        "    positions = torch.sign(forecast_aligned)\n",
        "    pnl_per_period = positions * actual_aligned\n",
        "    return pnl_per_period.sum(dim=0)\n",
        "\n",
        "def _calculate_correlation_matrix_functional(asset_returns_lookback_tensor, device_param=None):\n",
        "    # derived_device = device_param if device_param is not None else asset_returns_lookback_tensor.device\n",
        "    # derived_dtype = asset_returns_lookback_tensor.dtype\n",
        "\n",
        "    # Removed empty/small tensor check\n",
        "    corr_matrix = torch.corrcoef(asset_returns_lookback_tensor.T)\n",
        "    # Removed torch.nan_to_num\n",
        "    return corr_matrix\n",
        "\n",
        "def _apply_clustering_algorithm_functional(correlation_matrix_tensor, num_clusters_to_form, cluster_method, device_param=None):\n",
        "    derived_device = device_param if device_param is not None else correlation_matrix_tensor.device\n",
        "    num_assets = correlation_matrix_tensor.shape[0]\n",
        "\n",
        "    # Removed num_assets == 0 check\n",
        "    effective_n_clusters = min(num_clusters_to_form, num_assets)\n",
        "    # Removed effective_n_clusters <= 0 check (it will use original num_clusters_to_form if < min result or proceed if min result is <=0)\n",
        "\n",
        "\n",
        "    if cluster_method == 'spectral_clustering':\n",
        "        pos_corr = torch.clamp(correlation_matrix_tensor, min=0)\n",
        "        neg_corr = torch.abs(torch.clamp(correlation_matrix_tensor, max=0))\n",
        "        labels = spectral_clustering_laplacian(pos_corr, neg_corr, effective_n_clusters)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported cluster_method: {cluster_method}\")\n",
        "    return labels.to(derived_device)\n",
        "\n",
        "def _define_clusters_and_centroids_functional(asset_returns_lookback_tensor, initial_n_clusters_config, cluster_method, device_param=None):\n",
        "    derived_device = device_param if device_param is not None else asset_returns_lookback_tensor.device\n",
        "\n",
        "    asset_corr_matrix = _calculate_correlation_matrix_functional(asset_returns_lookback_tensor, derived_device)\n",
        "    # num_assets = asset_corr_matrix.shape[0] # Not strictly needed after removing checks\n",
        "\n",
        "    cluster_definitions = {}\n",
        "    cluster_names_ordered = []\n",
        "    # actual_n_clusters_formed = 0 # Calculated later\n",
        "\n",
        "    output_corr_matrix = asset_corr_matrix # Default, may be overwritten\n",
        "\n",
        "    # Removed num_assets == 0 check\n",
        "\n",
        "    labels = _apply_clustering_algorithm_functional(asset_corr_matrix, initial_n_clusters_config, cluster_method, derived_device)\n",
        "    # def handmade_unique(x):\n",
        "    #   sorted_x, _ = torch.sort(x)\n",
        "    #   mask = (sorted_x[1:] - sorted_x[:-1])>0\n",
        "    #   return sorted_x[1:]*mask\n",
        "    # unique_labels = handmade_unique(labels).nonzero_static(size = )[0]\n",
        "    # unique_labels = torch.nonzero(unique_labels, as_tuple=False).squeeze()\n",
        "    # print(unique_labels)\n",
        "\n",
        "    centroids_to_cat = []\n",
        "    cluster_names = []\n",
        "\n",
        "    for label_val_tensor in torch.arange(initial_n_clusters_config):\n",
        "        cluster_name = label_val_tensor + 1\n",
        "        cluster_names.append(cluster_name)\n",
        "        asset_indices_in_cluster = (labels == label_val_tensor).nonzero_static(size = labels.shape[0])[0]\n",
        "\n",
        "        centroid_ts = asset_returns_lookback_tensor[:, asset_indices_in_cluster].mean(dim=1)\n",
        "        cluster_definitions[cluster_name] = {\n",
        "            'asset_indices': asset_indices_in_cluster,\n",
        "            'centroid_ts': centroid_ts\n",
        "        }\n",
        "        centroids_to_cat.append(centroid_ts.unsqueeze(0))\n",
        "\n",
        "    # Removed 'if centroids_to_cat:' check\n",
        "    all_centroids_stacked = torch.cat(centroids_to_cat, dim=0)\n",
        "    # Removed shape check for all_centroids_stacked before corrcoef\n",
        "    output_corr_matrix = torch.corrcoef(all_centroids_stacked)\n",
        "    # Removed torch.nan_to_num\n",
        "\n",
        "    return cluster_definitions, cluster_names, initial_n_clusters_config, output_corr_matrix\n",
        "\n",
        "# --- REFACTORED FUNCTION for calculating weighted cluster returns ---\n",
        "def _calculate_weighted_cluster_returns_functional(\n",
        "    data_slice_for_returns_calc,\n",
        "    data_slice_for_centroid_weights_calc,\n",
        "    cluster_definitions,\n",
        "    cluster_names_ordered,\n",
        "    sigma_for_weights\n",
        "):\n",
        "    device = data_slice_for_returns_calc.device\n",
        "    dtype_returns = data_slice_for_returns_calc.dtype\n",
        "    dtype_weights_data = data_slice_for_centroid_weights_calc.dtype\n",
        "\n",
        "    # Removed 'if not cluster_definitions:' and 'if not cluster_names_ordered:' checks\n",
        "\n",
        "    T_current_period = data_slice_for_returns_calc.shape[0]\n",
        "    cluster_returns_list = []\n",
        "\n",
        "    for cluster_name in cluster_names_ordered:\n",
        "        info = cluster_definitions[cluster_name] # Direct access, removed .get and None check\n",
        "\n",
        "        asset_indices_in_cluster = info['asset_indices']\n",
        "        centroid_lookback_ts = info['centroid_ts']\n",
        "\n",
        "        current_cluster_period_returns = torch.zeros(T_current_period, device=device, dtype=dtype_returns) # Default\n",
        "\n",
        "        # Removed 'if len(asset_indices_in_cluster) > 0:'\n",
        "        asset_gaussian_weights = torch.zeros(len(asset_indices_in_cluster), device=device, dtype=dtype_weights_data) # Default\n",
        "\n",
        "        # Removed 'if len(centroid_lookback_ts) > 0' part, kept alignment check\n",
        "        if data_slice_for_centroid_weights_calc.shape[0] == len(centroid_lookback_ts):\n",
        "            assets_in_cluster_returns_centroid_def_period = data_slice_for_centroid_weights_calc.index_select(1, asset_indices_in_cluster)\n",
        "            centroid_lookback_ts_casted = centroid_lookback_ts.to(dtype_weights_data)\n",
        "\n",
        "            squared_distances = torch.sum((assets_in_cluster_returns_centroid_def_period - centroid_lookback_ts_casted.unsqueeze(1))**2, dim=0)\n",
        "            weights = torch.exp(-squared_distances / (2 * (sigma_for_weights**2))) # Removed + 1e-12\n",
        "            asset_gaussian_weights = weights\n",
        "\n",
        "        total_gaussian_weight_sum = asset_gaussian_weights.sum()\n",
        "        assets_in_cluster_current_period = data_slice_for_returns_calc.index_select(1, asset_indices_in_cluster)\n",
        "\n",
        "        normalized_weights = asset_gaussian_weights / total_gaussian_weight_sum # May produce NaN/inf if sum is 0\n",
        "        current_cluster_period_returns = (assets_in_cluster_current_period * normalized_weights.unsqueeze(0)).sum(dim=1)\n",
        "\n",
        "        # Removed T_current_period == 0 check for specific return shape\n",
        "\n",
        "        cluster_returns_list.append(current_cluster_period_returns)\n",
        "\n",
        "    # Removed 'if not cluster_returns_list:' check\n",
        "    return torch.stack(cluster_returns_list, dim=1)\n",
        "\n",
        "def _fit_var_and_forecast_functional(\n",
        "    lookback_series_tensor,\n",
        "    n_series_for_var_model,\n",
        "    var_order, forecast_horizon, device_param=None\n",
        "):\n",
        "    # derived_device = device_param if device_param is not None else lookback_series_tensor.device # Not used\n",
        "    model = VAR(var_order)\n",
        "\n",
        "    model.fit(lookback_series_tensor) # Use the passed var_order\n",
        "\n",
        "    # Convert forecast back to tensor and move to original device\n",
        "    forecast_tensor_batched = model.forecast(lookback_series_tensor[-var_order:], forecast_horizon)\n",
        "\n",
        "    return forecast_tensor_batched # (Horizon, N_series_var_model)\n",
        "\n",
        "def _pad_tensor_cols_functional(tensor, target_cols, current_rows_for_empty_case):\n",
        "    output_dtype = tensor.dtype\n",
        "    # Removed tensor.numel() == 0 check and its specific return for (num_rows, target_cols)\n",
        "\n",
        "    current_cols = tensor.shape[1]\n",
        "    if current_cols == target_cols:\n",
        "        return tensor\n",
        "    elif current_cols < target_cols:\n",
        "        padding_shape = (tensor.shape[0], target_cols - current_cols)\n",
        "        # Removed handling for padding_shape[0] == 0\n",
        "        padding = torch.zeros(padding_shape, device=tensor.device, dtype=output_dtype)\n",
        "        return torch.cat([tensor, padding], dim=1)\n",
        "    else: # current_cols > target_cols (truncate)\n",
        "        return tensor[:, :target_cols]\n",
        "\n",
        "# --- REFACTORED Main processing function for a single window ---\n",
        "def single_window_iteration_processor_refactored(\n",
        "    lookback_asset_returns_item,\n",
        "    eval_asset_returns_item_padded,\n",
        "    current_eval_len_for_window,\n",
        "    window_idx_batch,\n",
        "    initial_n_clusters_config_static,\n",
        "    cluster_method_static,\n",
        "    var_order_static,\n",
        "    sigma_for_weights_static,\n",
        "    max_eval_len_static,\n",
        "    run_naive_var_comparison_static,\n",
        "    num_assets_overall_static,\n",
        "    device_static\n",
        "):\n",
        "    actual_eval_asset_returns_item = eval_asset_returns_item_padded\n",
        "\n",
        "    cluster_defs, cluster_names_ord, actual_n_clust_formed, _ = _define_clusters_and_centroids_functional(\n",
        "        lookback_asset_returns_item, initial_n_clusters_config_static, cluster_method_static, device_static\n",
        "    )\n",
        "\n",
        "    # Removed 'if cluster_names_ord:'\n",
        "    lookback_cluster_returns = _calculate_weighted_cluster_returns_functional(\n",
        "        data_slice_for_returns_calc=lookback_asset_returns_item,\n",
        "        data_slice_for_centroid_weights_calc=lookback_asset_returns_item,\n",
        "        cluster_definitions=cluster_defs,\n",
        "        cluster_names_ordered=cluster_names_ord,\n",
        "        sigma_for_weights=sigma_for_weights_static\n",
        "    )\n",
        "\n",
        "    # print(lookback_cluster_returns)\n",
        "\n",
        "    # Removed 'if lookback_cluster_returns.numel() > 0 and lookback_cluster_returns.shape[1] > 0:'\n",
        "    forecasted_cluster_returns_raw = _fit_var_and_forecast_functional(\n",
        "        lookback_cluster_returns, actual_n_clust_formed, var_order_static,\n",
        "        current_eval_len_for_window, device_static\n",
        "    )\n",
        "\n",
        "\n",
        "    # Removed 'if current_eval_len_for_window > 0:'\n",
        "    true_eval_cluster_returns_raw = _calculate_weighted_cluster_returns_functional(\n",
        "        data_slice_for_returns_calc=actual_eval_asset_returns_item,\n",
        "        data_slice_for_centroid_weights_calc=lookback_asset_returns_item,\n",
        "        cluster_definitions=cluster_defs,\n",
        "        cluster_names_ordered=cluster_names_ord,\n",
        "        sigma_for_weights=sigma_for_weights_static\n",
        "    )\n",
        "\n",
        "    pnl_c_tensor = calculate_pnl_torch(forecasted_cluster_returns_raw, true_eval_cluster_returns_raw)\n",
        "    # Removed 'if pnl_c_tensor.numel() > 0:'\n",
        "    pnl_c_val = pnl_c_tensor.mean().to(torch.float32)\n",
        "    # Removed 'if torch.isnan(pnl_c_val):'\n",
        "\n",
        "    pnl_n_val = torch.tensor(0.0, dtype=torch.float32, device=device_static)\n",
        "    forecasted_naive_returns_raw = torch.tensor(0.0, dtype=torch.float32, device=device_static)\n",
        "    true_eval_naive_returns_raw = torch.tensor(0.0, dtype=torch.float32, device=device_static)\n",
        "\n",
        "    if run_naive_var_comparison_static:\n",
        "        # Removed 'if lookback_asset_returns_item.numel() > 0 and lookback_asset_returns_item.shape[1] > 0 :'\n",
        "        forecasted_naive_returns_raw = _fit_var_and_forecast_functional(\n",
        "            lookback_asset_returns_item, num_assets_overall_static, var_order_static,\n",
        "            current_eval_len_for_window, device_static\n",
        "        )\n",
        "\n",
        "        # Removed 'if current_eval_len_for_window > 0:'\n",
        "        true_eval_naive_returns_raw = actual_eval_asset_returns_item\n",
        "\n",
        "        pnl_n_tensor = calculate_pnl_torch(forecasted_naive_returns_raw, forecasted_naive_returns_raw)\n",
        "        # Removed 'if pnl_n_tensor.numel() > 0:'\n",
        "        pnl_n_val = pnl_n_tensor.mean().to(torch.float32)\n",
        "        # Removed 'if torch.isnan(pnl_n_val):'\n",
        "\n",
        "    return (pnl_c_val, forecasted_cluster_returns_raw, true_eval_cluster_returns_raw,\n",
        "            pnl_n_val, forecasted_naive_returns_raw, true_eval_naive_returns_raw,\n",
        "            window_idx_batch\n",
        "           )\n",
        "\n",
        "# --- REFACTORED Main sliding window function ---\n",
        "def run_sliding_window_var_evaluation_vmap(\n",
        "    asset_returns_tensor, initial_lookback_len, eval_len, n_clusters_config,\n",
        "    cluster_method, var_order, sigma_intra_cluster, num_windows,\n",
        "    device=None, store_sample_forecasts=True, run_naive_var_comparison=True\n",
        "):\n",
        "    # Removed 'if device is None:' device will be asset_returns_tensor.device if not passed\n",
        "    effective_device = device if device is not None else asset_returns_tensor.device\n",
        "\n",
        "    num_assets_overall = asset_returns_tensor.shape[1]\n",
        "    tensor_total_len = asset_returns_tensor.shape[0]\n",
        "\n",
        "    base_lookback_start_indices = torch.arange(num_windows, device=effective_device) * eval_len\n",
        "    base_lookback_end_indices = base_lookback_start_indices + initial_lookback_len\n",
        "\n",
        "    valid_mask_initial = (base_lookback_end_indices <= tensor_total_len) & \\\n",
        "                         (base_lookback_end_indices < tensor_total_len)\n",
        "\n",
        "    filtered_lb_start_indices = base_lookback_start_indices[valid_mask_initial]\n",
        "    filtered_lb_end_indices = base_lookback_end_indices[valid_mask_initial]\n",
        "    original_window_indices_filtered = torch.arange(num_windows, device=effective_device)[valid_mask_initial]\n",
        "\n",
        "    # Removed 'if len(filtered_lb_start_indices) == 0:' and early return\n",
        "\n",
        "    num_potentially_valid_windows = len(filtered_lb_start_indices)\n",
        "    current_eval_lens_for_filtered = torch.full((num_potentially_valid_windows,), eval_len, dtype=torch.long, device=effective_device)\n",
        "    for i in range(num_potentially_valid_windows): # This loop might be empty if num_potentially_valid_windows is 0\n",
        "        max_possible_eval_len = tensor_total_len - filtered_lb_end_indices[i]\n",
        "        current_eval_lens_for_filtered[i] = min(eval_len, max_possible_eval_len)\n",
        "\n",
        "    valid_mask_eval_len = current_eval_lens_for_filtered > 0\n",
        "\n",
        "    final_lookback_start_indices = filtered_lb_start_indices[valid_mask_eval_len]\n",
        "    final_lookback_end_indices = filtered_lb_end_indices[valid_mask_eval_len]\n",
        "    final_current_eval_lens = current_eval_lens_for_filtered[valid_mask_eval_len]\n",
        "    final_original_window_indices_batch = original_window_indices_filtered[valid_mask_eval_len]\n",
        "\n",
        "    # Removed 'if len(final_lookback_start_indices) == 0:' and early return\n",
        "\n",
        "    actual_num_windows_to_process = len(final_lookback_start_indices)\n",
        "\n",
        "    batched_lookback_slices = []\n",
        "    batched_eval_slices_padded = []\n",
        "\n",
        "    # for _ in repeat_for_robustness:\n",
        "    #   for p in lag_orders_to_test:\n",
        "    #       for k in num_clusters_to_test:\n",
        "    for s in range(actual_num_windows_to_process): # This loop might be empty\n",
        "        start = final_lookback_start_indices[s]\n",
        "        end = final_lookback_end_indices[s]\n",
        "\n",
        "        lb_slice = asset_returns_tensor[start:end, :]\n",
        "        batched_lookback_slices.append(lb_slice)\n",
        "\n",
        "        eval_s = end\n",
        "        current_L = final_current_eval_lens[s].item()\n",
        "        eval_e = eval_s + current_L\n",
        "        ev_slice_actual = asset_returns_tensor[eval_s:eval_e, :]\n",
        "\n",
        "        # # Padding logic kept for vmap compatibility\n",
        "        # if current_L < eval_len:\n",
        "        #     padding = torch.zeros((eval_len - current_L, num_assets_overall),\n",
        "        #                           dtype=asset_returns_tensor.dtype, device=effective_device)\n",
        "        #     ev_slice_padded = torch.cat([ev_slice_actual, padding], dim=0)\n",
        "        # elif current_L == eval_len:\n",
        "        #     ev_slice_padded = ev_slice_actual\n",
        "        # else:\n",
        "        #     ev_slice_padded = ev_slice_actual[:eval_len,:]\n",
        "\n",
        "        batched_eval_slices_padded.append(ev_slice_actual)\n",
        "\n",
        "    # These stacks will error if their respective lists are empty (e.g., actual_num_windows_to_process is 0)\n",
        "    final_batched_lookback_data = torch.stack(batched_lookback_slices)\n",
        "    final_batched_eval_data_padded = torch.stack(batched_eval_slices_padded)\n",
        "\n",
        "\n",
        "    vmapped_processor = func.vmap(\n",
        "        single_window_iteration_processor_refactored,\n",
        "        in_dims=(0, 0, None, 0,\n",
        "                   None, None, None, None, None, None, None, None),\n",
        "        out_dims=0, randomness='different'\n",
        "    )\n",
        "\n",
        "    all_pnl_c_vals, all_forecast_c, all_actual_c, \\\n",
        "    all_pnl_n_vals, all_forecast_n, all_actual_n, \\\n",
        "    processed_window_indices = vmapped_processor(\n",
        "        final_batched_lookback_data,\n",
        "        final_batched_eval_data_padded,\n",
        "        eval_len,\n",
        "        final_original_window_indices_batch,\n",
        "        n_clusters_config,\n",
        "        cluster_method,\n",
        "        var_order,\n",
        "        sigma_intra_cluster,\n",
        "        eval_len,\n",
        "        run_naive_var_comparison,\n",
        "        num_assets_overall,\n",
        "        effective_device\n",
        "    )\n",
        "\n",
        "    all_window_pnl_cluster_dicts = []\n",
        "    all_window_pnl_naive_dicts = []\n",
        "\n",
        "    for i in range(actual_num_windows_to_process): # This loop might be empty\n",
        "        win_id = processed_window_indices[i].item()\n",
        "\n",
        "        # Removed filter 'if all_pnl_c_vals[i].item() != 0.0 or all_forecast_c[i].abs().sum() > 1e-9:'\n",
        "        all_window_pnl_cluster_dicts.append({\n",
        "             'Window_ID': win_id, 'Avg_Window_PNL': all_pnl_c_vals[i].item(),\n",
        "             'VAR_Order': var_order, 'Method': 'Clustered VAR'\n",
        "         })\n",
        "\n",
        "        if run_naive_var_comparison:\n",
        "            # Removed filter for naive pnl/forecast values\n",
        "            all_window_pnl_naive_dicts.append({\n",
        "                'Window_ID': win_id, 'Avg_Window_PNL': all_pnl_n_vals[i].item(),\n",
        "                'VAR_Order': var_order, 'Method': 'Naive VAR'\n",
        "            })\n",
        "\n",
        "    sample_forecast_c, sample_actual_c, sample_win_idx = None, None, -1\n",
        "    # Removed 'and actual_num_windows_to_process > 0' from condition\n",
        "    if store_sample_forecasts:\n",
        "        sample_idx_in_batch = 0 # Will error if actual_num_windows_to_process is 0\n",
        "        actual_eval_len_for_sample = final_current_eval_lens[sample_idx_in_batch].item()\n",
        "\n",
        "        sample_forecast_c = all_forecast_c[sample_idx_in_batch, :actual_eval_len_for_sample, :].clone()\n",
        "        sample_actual_c = all_actual_c[sample_idx_in_batch, :actual_eval_len_for_sample, :].clone()\n",
        "        sample_win_idx = processed_window_indices[sample_idx_in_batch].item()\n",
        "\n",
        "    return {\n",
        "        'cluster_avg_pnl_list': all_window_pnl_cluster_dicts,\n",
        "        'naive_avg_pnl_list': all_window_pnl_naive_dicts,\n",
        "        'sample_forecast_cluster': sample_forecast_c,\n",
        "        'sample_actual_cluster': sample_actual_c,\n",
        "        'sample_window_idx_cluster': sample_win_idx,\n",
        "        'var_order_for_sample': var_order\n",
        "    }\n",
        "\n",
        "\"\"\"## 1. Load and Clean Data\"\"\"\n",
        "\n",
        "# file_path = r'OPCL_20000103_20201231.csv' # Path to your data file\n",
        "file_path = r'/content/OPCL_20000103_20201231.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Removed 'if not df.empty:' checks. Code will error if df is empty and operations are called.\n",
        "df.set_index('ticker', inplace=True)\n",
        "df.columns = pd.to_datetime(df.columns.str.lstrip('X'), format='%Y%m%d').strftime('%Y-%m-%d')\n",
        "df_cleaned = df.dropna().transpose()\n",
        "df_cleaned.index = pd.to_datetime(df_cleaned.index)\n",
        "print(\"Data loaded and cleaned. Sample (first 5 rows/cols):\")\n",
        "# This print will error if df_cleaned is too small or empty.\n",
        "print(df_cleaned.iloc[0:5,0:5])\n",
        "\n",
        "print(f\"Shape of the cleaned data: {df_cleaned.shape}\")\n",
        "\n",
        "numpy_array = df_cleaned.astype(float).values\n",
        "historical_data_tensor = torch.from_numpy(numpy_array).float()\n",
        "historical_data_tensor = historical_data_tensor.to(device)\n",
        "# historical_data_tensor = torch.log1p(historical_data_tensor)\n",
        "\"\"\"## 2. VAR Lag Grid Search and Evaluation\"\"\"\n",
        "\n",
        "##################################################################### PARAMETERS #####################################################################\n",
        "initial_lookback_len = 252\n",
        "evaluation_len = 5\n",
        "num_clusters_config = 20\n",
        "cluster_method_config = 'spectral_clustering'\n",
        "sigma_config = 0.5\n",
        "num_windows_config = 25\n",
        "var_orders_to_test_config = range(5,11,2)\n",
        "####################################################################################################################################################\n",
        "\n",
        "all_lags_combined_pnl_data = []\n",
        "final_sample_details = {}\n",
        "\n",
        "# Removed dummy data definition block for testing as it's a form of edge case / setup handling.\n",
        "# The script now assumes 'historical_data_tensor' and 'device' are correctly defined.\n",
        "# if 'historical_data_tensor' not in globals() or 'device' not in globals():\n",
        "#    ...\n",
        "\n",
        "# Removed 'if historical_data_tensor.numel() > 0:'\n",
        "for var_order_val in tqdm(var_orders_to_test_config, desc=\"VAR Order Grid Search\"):\n",
        "    results = run_sliding_window_var_evaluation_vmap(\n",
        "        asset_returns_tensor=historical_data_tensor, # Will error if not defined or empty and used inappropriately by functions\n",
        "        initial_lookback_len=initial_lookback_len,\n",
        "        eval_len=evaluation_len,\n",
        "        n_clusters_config=num_clusters_config,\n",
        "        cluster_method=cluster_method_config,\n",
        "        var_order=var_order_val,\n",
        "        sigma_intra_cluster=sigma_config,\n",
        "        num_windows=num_windows_config,\n",
        "        device=device,\n",
        "        store_sample_forecasts=(var_order_val == var_orders_to_test_config[-1]),\n",
        "        run_naive_var_comparison=False\n",
        "    )\n",
        "    all_lags_combined_pnl_data.extend(results['cluster_avg_pnl_list'])\n",
        "    all_lags_combined_pnl_data.extend(results['naive_avg_pnl_list'])\n",
        "\n",
        "    # Removed 'and results.get('sample_forecast_cluster') is not None'\n",
        "    if (var_order_val == var_orders_to_test_config[-1]):\n",
        "        final_sample_details = results # Will store even if sample is None\n",
        "\n",
        "print(\"\\n--- Grid Search Completed ---\")\n",
        "# Removed 'if all_lags_combined_pnl_data:'\n",
        "agg_data = {}\n",
        "methods_seen = set()\n",
        "for record in all_lags_combined_pnl_data: # Will error if all_lags_combined_pnl_data is empty but accessed.\n",
        "    methods_seen.add(record['Method'])\n",
        "    key = (record['VAR_Order'], record['Method'])\n",
        "    agg_data.setdefault(key, []).append(record['Avg_Window_PNL'])\n",
        "\n",
        "sorted_methods = sorted(list(methods_seen))\n",
        "# Removed 'if not sorted_methods:'\n",
        "header = f\"{'VAR_Order':<10} | \" + \" | \".join([f\"{m:<15}\" for m in sorted_methods])\n",
        "print(\"\\nAverage Window PNL per Lag Order and Method:\")\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "\n",
        "for var_order in sorted(list(set(r['VAR_Order'] for r in all_lags_combined_pnl_data))):\n",
        "    row_str = f\"{var_order:<10} | \"\n",
        "    for method in sorted_methods:\n",
        "        pnl_list = agg_data.get((var_order, method), []) # Keep .get for safety or it could KeyError\n",
        "        # Removed 'if pnl_list:' for calculating avg_pnl. Division by zero if len is 0.\n",
        "        avg_pnl = sum(pnl_list) / len(pnl_list) if pnl_list else float('nan') # Retained minimal check to avoid div by zero for print\n",
        "        row_str += f\"{avg_pnl:<15.6f} | \"\n",
        "    print(row_str.strip().rsplit('|', 1)[0].strip())\n",
        "\n",
        "\n",
        "# 3. Visualization\n",
        "# Removed 'if all_lags_combined_pnl_data:'\n",
        "# Removed 'try-except' block for plotting\n",
        "# Filter for valid numeric PNLs for plotting to prevent seaborn/matplotlib errors. This is a practical necessity for plotting.\n",
        "plot_data_list = [d for d in all_lags_combined_pnl_data if isinstance(d.get('Avg_Window_PNL'), (int, float)) and not np.isnan(d.get('Avg_Window_PNL'))]\n",
        "\n",
        "# Removed 'if plot_data_list:'\n",
        "plot_df_data = {\n",
        "    'VAR_Order': [d['VAR_Order'] for d in plot_data_list],\n",
        "    'Avg_Window_PNL': [d['Avg_Window_PNL'] for d in plot_data_list],\n",
        "    'Method': [d['Method'] for d in plot_data_list]\n",
        "}\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='VAR_Order', y='Avg_Window_PNL', hue='Method', data=plot_df_data) # Will error if plot_df_data is empty/malformed\n",
        "plt.title('Distribution of Avg Window PNL by VAR Lag & Method (PyTorch with vmap)')\n",
        "plt.xlabel('VAR Lag Order'); plt.ylabel('Avg Window PNL')\n",
        "plt.legend(title='Forecast Method'); plt.grid(True, axis='y', alpha=0.7)\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "# Removed 'if final_sample_details and final_sample_details.get('sample_forecast_cluster') is not None:'\n",
        "# Direct access, will error if keys don't exist or values are None and methods are called.\n",
        "fc = final_sample_details['sample_forecast_cluster'].cpu().numpy()\n",
        "ac = final_sample_details['sample_actual_cluster'].cpu().numpy()\n",
        "win_idx = final_sample_details['sample_window_idx_cluster']\n",
        "var_ord_sample = final_sample_details['var_order_for_sample']\n",
        "\n",
        "print(f\"\\n--- Plotting Sample: Window {win_idx + 1 if win_idx != -1 else 'N/A'}, Clustered VAR Lag {var_ord_sample} ---\")\n",
        "num_series_to_plot = min(3, fc.shape[1]) # fc.shape[1] will error if fc is None\n",
        "# Removed 'if num_series_to_plot == 0:'\n",
        "\n",
        "for i in range(num_series_to_plot):\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    # Removed data/length validation before plotting. Plotting functions might error.\n",
        "    sns.lineplot(x=range(len(ac[:, i])), y=ac[:, i], label=f'Actual Cluster {i+1}', marker='o')\n",
        "    sns.lineplot(x=range(len(fc[:, i])), y=fc[:, i], label=f'Forecast Cluster {i+1}', marker='x', linestyle='--')\n",
        "    plt.title(f'Prediction vs Actual for Cluster {i+1} (Win {win_idx+1 if win_idx != -1 else \"N/A\"}, VAR {var_ord_sample}, PyTorch with vmap)')\n",
        "    plt.xlabel('Forecast Step'); plt.ylabel('Cluster Return')\n",
        "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6EXp2zzh73-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class VAR:\n",
        "    \"\"\"\n",
        "    Vector Autoregression (VAR) model estimated using OLS on Yule-Walker equations.\n",
        "    This implementation does not use nn.Module.\n",
        "\n",
        "    The VAR(p) model is defined as:\n",
        "    y_t = c + A_1 y_{t-1} + ... + A_p y_{t-p} + u_t\n",
        "    where y_t is a k-dimensional vector of variables,\n",
        "    c is a k-dimensional intercept vector,\n",
        "    A_i are k x k coefficient matrices,\n",
        "    u_t is a k-dimensional white noise vector with covariance matrix Sigma_u.\n",
        "\n",
        "    Estimation follows the Yule-Walker method:\n",
        "    1. Estimate mean and autocovariance matrices (Gamma_j) from data.\n",
        "    2. Solve the Yule-Walker equations for A_i coefficients.\n",
        "       The system solved is effectively:\n",
        "       Gamma_cap @ [A_1, ..., A_p].T = [Gamma_1, ..., Gamma_p].T (transposed)\n",
        "       where Gamma_cap is a block matrix of autocovariances, and the solution\n",
        "       for [A_1.T, ..., A_p.T].T is found using least squares (lstsq).\n",
        "    3. Estimate intercept c.\n",
        "    4. Estimate residual covariance Sigma_u.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lag_order: int, include_intercept: bool = True):\n",
        "        if lag_order < 1:\n",
        "            raise ValueError(\"Lag order must be at least 1.\")\n",
        "        self.lag_order = lag_order\n",
        "        self.include_intercept = include_intercept\n",
        "\n",
        "        self.k_ = None  # Number of variables\n",
        "        self.intercept_ = None  # Intercept vector c (1 x k)\n",
        "        self.lag_coef_ = None  # Stacked A_i' matrices (kp x k): [A_1.T, A_2.T, ..., A_p.T].T\n",
        "                               # So, A_i = self.lag_coef_[(i-1)*k : i*k, :].T\n",
        "        self.sigma_u_ = None  # Residual covariance matrix (k x k)\n",
        "        self.fitted_ = False\n",
        "        self.mu_ = None # Mean of the series used for fitting\n",
        "\n",
        "    def _compute_autocov(self, X_processed: torch.Tensor, lag: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes sample autocovariance matrix Gamma_lag.\n",
        "        X_processed: (T x k) tensor, already mean-centered if intercept is included.\n",
        "        lag: integer lag.\n",
        "        Returns: (k x k) covariance matrix.\n",
        "        Uses divisor T. For YW, sample autocovariances are often E[(y_t-mu)(y_{t-h}-mu)'].\n",
        "        The divisor T is common for Gamma_0 in some contexts, and T-h for Gamma_h.\n",
        "        Using T for all for consistency in constructing the YW system is one approach.\n",
        "        Statsmodels VAR uses T-p for OLS and T for YW Gamma matrix elements.\n",
        "        Here, we use T for simplicity, recognizing it's one common definition.\n",
        "        \"\"\"\n",
        "        T, k = X_processed.shape\n",
        "        if T <= lag:\n",
        "             # This can happen if T is very small relative to p.\n",
        "             # The calling fit method should check T > p.\n",
        "             # For autocov computation specifically, if lag >= T, result is ill-defined or zero.\n",
        "            return torch.zeros((k,k), device=X_processed.device, dtype=X_processed.dtype)\n",
        "\n",
        "\n",
        "        if lag == 0:\n",
        "            # Gamma_0 = (1/T) * sum_{t=1 to T} (x_t - mu)(x_t - mu)'\n",
        "            return (X_processed.T @ X_processed) / T\n",
        "        else:\n",
        "            # Gamma_lag = (1/T) * sum_{t=lag+1 to T} (x_t - mu)(x_{t-lag} - mu)'\n",
        "            # X_processed[lag:] corresponds to x_t\n",
        "            # X_processed[:-lag] corresponds to x_{t-lag}\n",
        "            # For (T x k) matrices, (A.T @ B) / N gives sum of (a_t' b_t) / N if a_t,b_t are columns\n",
        "            # Here, X_processed[lag:] are rows for y_t, X_processed[:-lag] are rows for y_{t-lag}\n",
        "            # We need sum (y_t - mu)' (y_{t-lag} - mu) if y are columns\n",
        "            # (X_processed[lag:].T @ X_processed[:-lag]) gives sum_t (y_t-mu)(y_{t-lag}-mu)'\n",
        "            # where (y_t-mu) is a column vector (k x 1)\n",
        "            # No, it's sum_t (y_t-mu) (y_{t-lag}-mu)' where (y_t-mu) is taken as a column vector.\n",
        "            # X_processed is (N_eff x k). X_processed[lag:].T is (k x (T-lag))\n",
        "            # X_processed[:-lag] is ((T-lag) x k)\n",
        "            # So (X_processed[lag:].T @ X_processed[:-lag]) is (k x k)\n",
        "            return (X_processed[lag:].T @ X_processed[:-lag]) / T\n",
        "\n",
        "\n",
        "    def fit(self, data: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Fits the VAR(p) model to the data using Yule-Walker equations.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): Time series data of shape (T x k),\n",
        "                                 where T is the number of observations and\n",
        "                                 k is the number of variables.\n",
        "        \"\"\"\n",
        "        T, k = data.shape\n",
        "        if T <= self.lag_order:\n",
        "            raise ValueError(f\"Number of observations (T={T}) must be greater than lag order (p={self.lag_order}).\")\n",
        "\n",
        "        self.k_ = k\n",
        "        p = self.lag_order\n",
        "\n",
        "        device = data.device\n",
        "        dtype = data.dtype\n",
        "\n",
        "        if self.include_intercept:\n",
        "            self.mu_ = data.mean(dim=0, keepdim=True)  # Shape (1 x k)\n",
        "            X_processed = data - self.mu_\n",
        "        else:\n",
        "            self.mu_ = torch.zeros((1, k), device=device, dtype=dtype) # Assume mean zero\n",
        "            X_processed = data # Data is used as is\n",
        "\n",
        "        # 1. Estimate autocovariance matrices Gamma_j\n",
        "        # We need Gamma_0, Gamma_1, ..., Gamma_p\n",
        "        gammas = [self._compute_autocov(X_processed, j) for j in range(p + 1)]\n",
        "\n",
        "        # 2. Solve Yule-Walker equations for lag coefficients (A_i)\n",
        "        # System based on Ltkepohl (2005), \"New Introduction to Multiple Time Series Analysis\", p.70 eq (3.2.2)\n",
        "        # [A_1, ..., A_p] = [Gamma_1, ..., Gamma_p] @ inv(Gamma_cap)\n",
        "        # where Gamma_cap (kp x kp) is block matrix: Gamma_cap[r,c] = Gamma_{r-c} (using Gamma_{-s} = Gamma_s.T)\n",
        "        # Let A_coeffs_stacked = [A_1.T, ..., A_p.T].T (kp x k)\n",
        "        # Then (Gamma_cap).T @ A_coeffs_stacked = [Gamma_1.T, ..., Gamma_p.T].T\n",
        "\n",
        "        # Construct Gamma_cap (Ltkepohl's Gamma, often denoted Psi or Big_Gamma in other texts)\n",
        "        # Gamma_cap[block_row_idx, block_col_idx] = Gamma_{block_row_idx - block_col_idx}\n",
        "\n",
        "        block_rows_list = []\n",
        "        for r_idx in range(p):  # For each block row (0 to p-1)\n",
        "            current_block_row_elements = []\n",
        "            for c_idx in range(p):  # For each block in that row (0 to p-1)\n",
        "                lag_val = r_idx - c_idx\n",
        "                if lag_val == 0:\n",
        "                    block_to_add = gammas[0]                            # Gamma_0\n",
        "                elif lag_val > 0:\n",
        "                    block_to_add = gammas[lag_val]                      # Gamma_{lag_val}\n",
        "                else:  # lag_val < 0\n",
        "                    block_to_add = gammas[abs(lag_val)].T               # Gamma_{-abs(lag_val)} = Gamma_{abs(lag_val)}.T\n",
        "                current_block_row_elements.append(block_to_add)\n",
        "\n",
        "            # Concatenate blocks horizontally for this block row\n",
        "            # Each element is k x k, so result is k x (p*k)\n",
        "            block_row = torch.cat(current_block_row_elements, dim=1)\n",
        "            block_rows_list.append(block_row)\n",
        "\n",
        "        # Concatenate block rows vertically\n",
        "        # Each block_row is k x (p*k), stacking p of them gives (p*k) x (p*k)\n",
        "        gamma_cap = torch.cat(block_rows_list, dim=0)\n",
        "\n",
        "        # Construct RHS vector: [Gamma_1.T, ..., Gamma_p.T].T (stacked vertically)\n",
        "        # Each Gamma_j.T is (k x k). Stacking p of them vertically results in (pk x k)\n",
        "        rhs_stacked_T_list = [gammas[j+1].T for j in range(p)] # gammas[1].T, ..., gammas[p].T\n",
        "        rhs_stacked_T = torch.cat(rhs_stacked_T_list, dim=0)\n",
        "\n",
        "\n",
        "        # Solve (Gamma_cap).T @ A_coeffs_stacked = rhs_stacked_T for A_coeffs_stacked\n",
        "        # A_coeffs_stacked is self.lag_coef_\n",
        "        try:\n",
        "            # Using lstsq aligns with \"OLS on Yule-Walker\" by finding the least squares solution.\n",
        "            self.lag_coef_ = torch.linalg.lstsq(gamma_cap.T, rhs_stacked_T).solution\n",
        "        except torch.linalg.LinAlgError as e:\n",
        "            raise RuntimeError(f\"Failed to solve Yule-Walker system using lstsq. Matrix may be singular. Original error: {e}\")\n",
        "\n",
        "\n",
        "        # 3. Estimate intercept c\n",
        "        if self.include_intercept:\n",
        "            # c = (I - A_1 - ... - A_p) @ mu\n",
        "            A_sum = torch.zeros((k, k), device=device, dtype=dtype)\n",
        "            if p > 0:\n",
        "                for i in range(p):\n",
        "                  A_i_plus_1_T = self.lag_coef_[i * k:(i + 1) * k, :] # This is A_{i+1}.T\n",
        "                  A_sum = A_sum + A_i_plus_1_T.T # This is A_{i+1}\n",
        "\n",
        "            identity_k = torch.eye(k, device=device, dtype=dtype)\n",
        "            self.intercept_ = ((identity_k - A_sum) @ self.mu_.T).T # Shape (1 x k)\n",
        "        else:\n",
        "            self.intercept_ = torch.zeros((1, k), device=device, dtype=dtype)\n",
        "\n",
        "        # 4. Estimate residual covariance Sigma_u\n",
        "        # Sigma_u = Gamma_0 - Sum_{j=1 to p} A_j @ Gamma_j.T (Ltkepohl eq 2.1.19)\n",
        "        # Note: Ltkepohl has Gamma_j in his formula, meaning E[y_t y_{t-j}'].\n",
        "        # Our Gamma_j means E[y_t y_{t-j}']. So Gamma_j.T is E[y_{t-j} y_t'].\n",
        "        # Ltkepohl (3.2.5) for Sigma_u: Gamma_0 - [A_1 ... A_p] @ [Gamma_1 ... Gamma_p].T\n",
        "        # Which is Gamma_0 - sum A_i Gamma_i.T\n",
        "        # This is equivalent to Gamma_0 - sum A_i Gamma_{-i} (using Gamma_{-i} = Gamma_i.T)\n",
        "\n",
        "        self.sigma_u_ = gammas[0].clone()\n",
        "        if p > 0:\n",
        "            for i in range(p): # i from 0 to p-1\n",
        "                # A_{i+1} corresponds to the i-th block in lag_coef_\n",
        "                A_i_plus_1_T = self.lag_coef_[i * k:(i + 1) * k, :] # A_{i+1}.T\n",
        "                A_i_plus_1 = A_i_plus_1_T.T # A_{i+1}\n",
        "\n",
        "                # We need A_{i+1} @ Gamma_{i+1}.T\n",
        "                self.sigma_u_ -= A_i_plus_1 @ gammas[i+1].T\n",
        "\n",
        "        self.fitted_ = True\n",
        "\n",
        "    def _check_fitted(self):\n",
        "        if not self.fitted_:\n",
        "            raise RuntimeError(\"Model has not been fitted yet. Call fit() first.\")\n",
        "\n",
        "    def predict(self, y_init_lags, n_ahead:int):\n",
        "        \"\"\"\n",
        "        Predicts future values of the time series.\n",
        "\n",
        "        Args:\n",
        "            y_init_lags (torch.Tensor): Initial lag values of shape (L x k) where L >= p.\n",
        "                                      These are the most recent p observations, ordered\n",
        "                                      chronologically (oldest to newest).\n",
        "                                      Example: If p=2, y_init_lags should be [y_{t-p}; ... ; y_{t-1}].\n",
        "            n_ahead (int): Number of steps to predict ahead.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted values of shape (n_ahead x k).\n",
        "        \"\"\"\n",
        "        self._check_fitted()\n",
        "        if y_init_lags.shape[1] != self.k_:\n",
        "            raise ValueError(\n",
        "                f\"Initial lags have {y_init_lags.shape[1]} variables, \"\n",
        "                f\"but model was fitted on {self.k_} variables.\"\n",
        "            )\n",
        "\n",
        "        p = self.lag_order\n",
        "        k = self.k_\n",
        "\n",
        "        # At this point, p > 0 and n_ahead > 0.\n",
        "        if y_init_lags.shape[0] < p:\n",
        "            raise ValueError(\n",
        "                f\"Need at least {self.lag_order} initial lags for prediction, got {y_init_lags.shape[0]}.\"\n",
        "            )\n",
        "\n",
        "        current_lags = y_init_lags[-p:].clone() # Shape (p x k), [y_{t-(p-1)}, ..., y_{t}] (oldest to newest)\n",
        "                                                # Or, using t as 'now': [y_{t-p+1}, ..., y_t]\n",
        "                                                # Or, to match Ltkepohl: [y_{t-p}, ..., y_{t-1}] used to predict y_t\n",
        "                                                # The indexing current_lags[-(j+1)] handles this correctly.\n",
        "\n",
        "        predictions_list = []\n",
        "\n",
        "        # Use _n_ahead_val in the range function\n",
        "        for _ in range(n_ahead):\n",
        "            pred_row = self.intercept_.clone()\n",
        "\n",
        "            for j in range(p):\n",
        "                lagged_y_val_row_vec = current_lags[-(j + 1), :].reshape(1, k)\n",
        "                A_j_plus_1_T = self.lag_coef_[j * k:(j + 1) * k, :]\n",
        "                A_j_plus_1 = A_j_plus_1_T.T\n",
        "                pred_row += lagged_y_val_row_vec @ A_j_plus_1\n",
        "\n",
        "            predictions_list.append(pred_row)\n",
        "\n",
        "            if p > 1:\n",
        "                current_lags = torch.roll(current_lags, shifts=-1, dims=0)\n",
        "            current_lags[-1, :] = pred_row\n",
        "\n",
        "        return torch.cat(predictions_list, dim=0)\n",
        "\n",
        "    def forecast(self, y_init_lags: torch.Tensor, steps: int) -> torch.Tensor:\n",
        "        return self.predict(y_init_lags, steps)\n",
        "\n",
        "    def get_params(self) -> dict:\n",
        "        \"\"\"Returns the estimated parameters.\"\"\"\n",
        "        self._check_fitted()\n",
        "        return {\n",
        "            \"intercept\": self.intercept_.clone() if self.intercept_ is not None else None,\n",
        "            \"lag_coefficients_stacked_T\": self.lag_coef_.clone() if self.lag_coef_ is not None else None, # Stacked A_i.T (kp x k)\n",
        "            \"sigma_u\": self.sigma_u_.clone() if self.sigma_u_ is not None else None\n",
        "        }\n",
        "\n",
        "    def get_individual_A_matrices(self) -> list:\n",
        "        \"\"\"Returns a list of A_i matrices [A_1, A_2, ..., A_p].\"\"\"\n",
        "        self._check_fitted()\n",
        "        if self.lag_order == 0:\n",
        "            return []\n",
        "\n",
        "        A_matrices = []\n",
        "        for i in range(self.lag_order):\n",
        "            # self.lag_coef_ stores A_{idx+1}.T at block idx\n",
        "            A_i_plus_1_T = self.lag_coef_[i * self.k_ : (i + 1) * self.k_, :]\n",
        "            A_matrices.append(A_i_plus_1_T.T.clone()) # A_i = (A_i.T).T\n",
        "        return A_matrices"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
