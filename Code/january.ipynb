{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ÉTAPE 0** : préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no NaN values in the dataframe\n"
     ]
    }
   ],
   "source": [
    "import process \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DATA_Statapp.csv'\n",
    "# Jerome path : 'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\DATA_Statapp.csv'\n",
    "# Mohamed path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DATA_Statapp.csv'\n",
    "df = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DATA_Statapp.csv')\n",
    "\n",
    "# Apply conversion function to 'open' and 'close' columns\n",
    "df['open'] = df['open'].apply(process.safe_literal_eval)\n",
    "df['close'] = df['close'].apply(process.safe_literal_eval)\n",
    "\n",
    "# Calculate returns for each line\n",
    "df['return'] = df.apply(lambda row: [(close - open) / open for open, close in zip(row['open'], row['close'])], axis=1)\n",
    "\n",
    "new_df = df[['ticker', 'return']] # create a new data frame with the column ticker and return \n",
    "\n",
    "# Créons le DataFrame à partir des listes dans 'return'\n",
    "# On suppose ici que 'new_df' est déjà défini et contient la colonne 'return'\n",
    "\n",
    "# Convertir chaque liste dans la colonne 'return' en plusieurs colonnes dans le nouveau DataFrame\n",
    "returns_df = pd.DataFrame(new_df['return'].tolist())\n",
    "\n",
    "# Ajouter la colonne 'ticker' du 'new_df' au début de 'returns_df'\n",
    "returns_df.insert(0, 'ticker', new_df['ticker'])\n",
    "\n",
    "# Renommer les colonnes pour refléter qu'elles sont des rendements\n",
    "returns_df.columns = ['ticker'] + [f'return_{i}' for i in range(len(returns_df.columns) - 1)]\n",
    "\n",
    "df_cleaned = process.remove_rows_with_nan(returns_df)\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "process.check_nan_inf(df_cleaned)\n",
    "\n",
    "df_cleaned.set_index('ticker', inplace=True) ## ces deux lignes sont fondamentales, ne pas les retirer !!\n",
    "df.set_index('ticker', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/process.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  A_pos = mat.applymap(lambda x: x if x >= 0 else 0)\n",
      "/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/process.py:50: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  A_neg = mat.applymap(lambda x: abs(x) if x < 0 else 0)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import process \n",
    "lookback_window = [5000, 5250]\n",
    "corr_matrix = process.correlation_matrix(lookback_window=lookback_window, df_cleaned=df_cleaned)\n",
    "cluster_composition_and_centroid = process.cluster_composition_and_centroid(df_cleaned=df_cleaned, correlation_matrix=corr_matrix, number_of_clusters=20, lookback_window=lookback_window)\n",
    "constituent_weights = process.constituent_weights(\n",
    "    df_cleaned=df_cleaned, \n",
    "    cluster_composition=cluster_composition_and_centroid, \n",
    "    sigma=10, \n",
    "    lookback_window=lookback_window\n",
    ")\n",
    "cluster_return = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, df=df, lookback_window=lookback_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected_returns is not a series, list or array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/january.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/january.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m markowitz_weights \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39;49mmarkowitz_weights(cluster_return_res\u001b[39m=\u001b[39;49mcluster_return, constituent_weights\u001b[39m=\u001b[39;49mconstituent_weights, df_cleaned\u001b[39m=\u001b[39;49mdf_cleaned, df\u001b[39m=\u001b[39;49mdf, lookback_window\u001b[39m=\u001b[39;49mlookback_window, eta\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/january.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m final_weights \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mfinal_weights(markowitz_weights\u001b[39m=\u001b[39mmarkowitz_weights, constituent_weights\u001b[39m=\u001b[39mconstituent_weights)\n",
      "File \u001b[0;32m~/Documents/GitHub/Portfolio_clustering_project/Code/process.py:448\u001b[0m, in \u001b[0;36mmarkowitz_weights\u001b[0;34m(cluster_return_res, constituent_weights, df_cleaned, df, lookback_window, eta)\u001b[0m\n\u001b[1;32m    444\u001b[0m cluster_target_return \u001b[39m=\u001b[39m cluster_return(constituent_weights\u001b[39m=\u001b[39mconstituent_weights, df_cleaned\u001b[39m=\u001b[39mdf_cleaned, df\u001b[39m=\u001b[39mdf, lookback_window\u001b[39m=\u001b[39m[lookback_window[\u001b[39m1\u001b[39m], lookback_window[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    446\u001b[0m expected_returns \u001b[39m=\u001b[39m noised_array(y\u001b[39m=\u001b[39mcluster_target_return, eta\u001b[39m=\u001b[39meta)\n\u001b[0;32m--> 448\u001b[0m ef \u001b[39m=\u001b[39m EfficientFrontier(expected_returns\u001b[39m=\u001b[39;49mexpected_returns, cov_matrix\u001b[39m=\u001b[39;49mcov_matrix)\n\u001b[1;32m    449\u001b[0m ef\u001b[39m.\u001b[39mmax_sharpe()\n\u001b[1;32m    451\u001b[0m markowitz_weights \u001b[39m=\u001b[39m ef\u001b[39m.\u001b[39mclean_weights()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pypfopt/efficient_frontier/efficient_frontier.py:86\u001b[0m, in \u001b[0;36mEfficientFrontier.__init__\u001b[0;34m(self, expected_returns, cov_matrix, weight_bounds, solver, verbose, solver_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m# Inputs\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_cov_matrix(cov_matrix)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_expected_returns(expected_returns)\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_return_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_market_neutral \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pypfopt/efficient_frontier/efficient_frontier.py:127\u001b[0m, in \u001b[0;36mEfficientFrontier._validate_expected_returns\u001b[0;34m(expected_returns)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m expected_returns\u001b[39m.\u001b[39mravel()\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected_returns is not a series, list or array\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected_returns is not a series, list or array"
     ]
    }
   ],
   "source": [
    "markowitz_weights = process.markowitz_weights(cluster_return_res=cluster_return, constituent_weights=constituent_weights, df_cleaned=df_cleaned, df=df, lookback_window=lookback_window, eta=0.01)\n",
    "final_weights = process.final_weights(markowitz_weights=markowitz_weights, constituent_weights=constituent_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_day = lookback_window[1] \n",
    "cluster_target_return = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, df=df, lookback_window=[target_day, target_day+1])\n",
    "cluster_target_return_noised = cluster_target_return\n",
    "\n",
    "def noisy(y):\n",
    "\n",
    "    x = y.copy()\n",
    "\n",
    "    noise = np.random.normal(0, 1, len(y))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        x.iloc[i, 0] = y.iloc[i, 0] + noise[i]\n",
    "\n",
    "    return x\n",
    "\n",
    "def noised_array(y, eta):\n",
    "\n",
    "    '''\n",
    "    ----------------------------------------------------------------\n",
    "    GENERAL IDEA : given an array y and a target correlation eta, \n",
    "                   compute the array with the noise  \n",
    "    ----------------------------------------------------------------\n",
    "\n",
    "    ----------------------------------------------------------------\n",
    "    PARAMS : \n",
    "\n",
    "    - y : numpy ndarray that we want to perturb\n",
    "\n",
    "    - eta : target correlation that we want to create between y and \n",
    "            its perturbated version\n",
    "\n",
    "    ----------------------------------------------------------------\n",
    "\n",
    "    ----------------------------------------------------------------\n",
    "    OUTPUT : noised version of y that satisfies the targeted level \n",
    "             of correlation\n",
    "    ----------------------------------------------------------------\n",
    "    '''\n",
    "\n",
    "    # We compute with a small noise \n",
    "    epsilon_std_dev = 0.1\n",
    "\n",
    "    # Calculer la corrélation initiale\n",
    "    correlation = 0\n",
    "\n",
    "    x = y.copy()\n",
    "\n",
    "    z = y.to_numpy()\n",
    "    z = np.array([item for sublist in z for item in sublist])\n",
    "\n",
    "    \n",
    "    # Boucle pour ajuster l'écart-type du bruit jusqu'à ce que la corrélation atteigne eta\n",
    "    while correlation < eta:\n",
    "        # Generate a vector of Gaussian noise\n",
    "        noise = np.random.normal(0, epsilon_std_dev, len(y))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            x.iloc[i, 0] = y.iloc[i, 0] + noise[i]\n",
    "\n",
    "        w = x.to_numpy()\n",
    "        w = np.array([item for sublist in w for item in sublist])\n",
    "        # Calculate the new correlation\n",
    "        correlation = np.corrcoef(w, z)[0, 1]\n",
    "\n",
    "        # Adjust the standard deviation of the noise\n",
    "        epsilon_std_dev += 0.01  \n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.056840264845198676, -0.2112179366109751, -0.06488163057753042,\n",
       "        -0.012652525480657146, -0.07830942789099936,\n",
       "        -0.039278694872045855, -0.07927509800164466, 0.12067597864982475,\n",
       "        0.1273950336263377, 0.13034234520455215, -0.04311737185345518,\n",
       "        0.06350207535460001, -0.12474307688360413, 0.07546203613925469,\n",
       "        0.05831169009062068, -0.15464664928864388, 0.004178344894214596,\n",
       "        -0.013689128443031998, -0.09465640209347785]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = noised_array(y=cluster_target_return, eta=0.01).transpose().to_numpy()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>portfolio return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>return_5250</th>\n",
       "      <td>0.017398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5251</th>\n",
       "      <td>-0.012297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5252</th>\n",
       "      <td>0.020837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5253</th>\n",
       "      <td>0.022024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5254</th>\n",
       "      <td>-0.003849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5255</th>\n",
       "      <td>0.003957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5256</th>\n",
       "      <td>-0.022711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5257</th>\n",
       "      <td>-0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5258</th>\n",
       "      <td>-0.006371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5259</th>\n",
       "      <td>-0.006529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             portfolio return\n",
       "return_5250          0.017398\n",
       "return_5251         -0.012297\n",
       "return_5252          0.020837\n",
       "return_5253          0.022024\n",
       "return_5254         -0.003849\n",
       "return_5255          0.003957\n",
       "return_5256         -0.022711\n",
       "return_5257         -0.005000\n",
       "return_5258         -0.006371\n",
       "return_5259         -0.006529"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_return = process.portfolio_returns(evaluation_window=10, df_cleaned=df_cleaned, lookback_window=lookback_window, consolidated_W=consolidated_weights)\n",
    "portfolio_return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
