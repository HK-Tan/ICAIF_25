{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ÉTAPE 0** : préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no NaN values in the dataframe\n"
     ]
    }
   ],
   "source": [
    "import process \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DATA_Statapp.csv'\n",
    "# Jerome path : 'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\DATA_Statapp.csv'\n",
    "# Mohamed path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DATA_Statapp.csv'\n",
    "df = pd.read_csv('/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DATA_Statapp.csv')\n",
    "\n",
    "# Apply conversion function to 'open' and 'close' columns\n",
    "df['open'] = df['open'].apply(process.safe_literal_eval)\n",
    "df['close'] = df['close'].apply(process.safe_literal_eval)\n",
    "\n",
    "# Calculate returns for each line\n",
    "df['return'] = df.apply(lambda row: [(close - open) / open for open, close in zip(row['open'], row['close'])], axis=1)\n",
    "\n",
    "new_df = df[['ticker', 'return']] # create a new data frame with the column ticker and return \n",
    "\n",
    "# Créons le DataFrame à partir des listes dans 'return'\n",
    "# On suppose ici que 'new_df' est déjà défini et contient la colonne 'return'\n",
    "\n",
    "# Convertir chaque liste dans la colonne 'return' en plusieurs colonnes dans le nouveau DataFrame\n",
    "returns_df = pd.DataFrame(new_df['return'].tolist())\n",
    "\n",
    "# Ajouter la colonne 'ticker' du 'new_df' au début de 'returns_df'\n",
    "returns_df.insert(0, 'ticker', new_df['ticker'])\n",
    "\n",
    "# Renommer les colonnes pour refléter qu'elles sont des rendements\n",
    "returns_df.columns = ['ticker'] + [f'return_{i}' for i in range(len(returns_df.columns) - 1)]\n",
    "\n",
    "df_cleaned = process.remove_rows_with_nan(returns_df)\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "process.check_nan_inf(df_cleaned)\n",
    "\n",
    "df_cleaned.set_index('ticker', inplace=True) ## ces deux lignes sont fondamentales, ne pas les retirer !!\n",
    "df.set_index('ticker', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/process.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  A_pos = mat.applymap(lambda x: x if x >= 0 else 0)\n",
      "/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/process.py:50: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  A_neg = mat.applymap(lambda x: abs(x) if x < 0 else 0)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import process \n",
    "lookback_window = [5000, 5250]\n",
    "corr_matrix = process.correlation_matrix(lookback_window=lookback_window, df_cleaned=df_cleaned)\n",
    "cluster_composition_and_centroid = process.cluster_composition_and_centroid(df_cleaned=df_cleaned, correlation_matrix=corr_matrix, number_of_clusters=20, lookback_window=lookback_window)\n",
    "constituent_weights = process.constituent_weights(\n",
    "    df_cleaned=df_cleaned, \n",
    "    cluster_composition=cluster_composition_and_centroid, \n",
    "    sigma=10, \n",
    "    lookback_window=lookback_window\n",
    ")\n",
    "cluster_return = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, df=df, lookback_window=lookback_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "cluster_target_return = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, df=df, lookback_window=[lookback_window[1], lookback_window[1]+1])\n",
    "cov_matrix = cluster_return.transpose().cov()\n",
    "expected_returns = process.noised_array(y=cluster_target_return, eta=0.01).iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "ef = EfficientFrontier(expected_returns=expected_returns, cov_matrix=cov_matrix)\n",
    "ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "markowitz_weights = process.markowitz_weights(cluster_return_res=cluster_return, constituent_weights=constituent_weights, df_cleaned=df_cleaned, df=df, lookback_window=lookback_window, eta=0.01)\n",
    "final_weights = process.final_weights(markowitz_weights=markowitz_weights, constituent_weights=constituent_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_W = process.consolidated_W(number_of_repetitions=30, lookback_window=lookback_window, df_cleaned=df_cleaned, number_of_clusters=20, sigma=10e-2, df=df, eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.123051285305439e-06"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_window = 10\n",
    "\n",
    "\n",
    "## we first get the open and close values for each stock \n",
    "open = pd.DataFrame(index = df_cleaned.index, columns=df_cleaned.columns[lookback_window[1]:lookback_window[1] + evaluation_window])\n",
    "close = pd.DataFrame(index = df_cleaned.index, columns=df_cleaned.columns[lookback_window[1]:lookback_window[1] + evaluation_window])\n",
    "\n",
    "for stock in open.index:\n",
    "    open.loc[stock, :] = df.loc[stock, 'open'][lookback_window[1]:lookback_window[1] + evaluation_window]\n",
    "    close.loc[stock, :] = df.loc[stock, 'close'][lookback_window[1]:lookback_window[1] + evaluation_window]\n",
    "\n",
    "portfolio_returns = pd.DataFrame(index=open.columns, columns=['portfolio return'], data=np.zeros(len(open.columns)))\n",
    "stock = 'AA'\n",
    "returns = 'return_5250'\n",
    "open.loc[stock, returns]\n",
    "consolidated_W.loc[stock, 'weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_day = lookback_window[1] \n",
    "cluster_target_return = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, df=df, lookback_window=[target_day, target_day+1])\n",
    "cluster_target_return_noised = cluster_target_return\n",
    "\n",
    "def noisy(y):\n",
    "\n",
    "    x = y.copy()\n",
    "\n",
    "    noise = np.random.normal(0, 1, len(y))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        x.iloc[i, 0] = y.iloc[i, 0] + noise[i]\n",
    "\n",
    "    return x\n",
    "\n",
    "def noised_array(y, eta):\n",
    "\n",
    "    '''\n",
    "    ----------------------------------------------------------------\n",
    "    GENERAL IDEA : given an array y and a target correlation eta, \n",
    "                   compute the array with the noise  \n",
    "    ----------------------------------------------------------------\n",
    "\n",
    "    ----------------------------------------------------------------\n",
    "    PARAMS : \n",
    "\n",
    "    - y : numpy ndarray that we want to perturb\n",
    "\n",
    "    - eta : target correlation that we want to create between y and \n",
    "            its perturbated version\n",
    "\n",
    "    ----------------------------------------------------------------\n",
    "\n",
    "    ----------------------------------------------------------------\n",
    "    OUTPUT : noised version of y that satisfies the targeted level \n",
    "             of correlation\n",
    "    ----------------------------------------------------------------\n",
    "    '''\n",
    "\n",
    "    # We compute with a small noise \n",
    "    epsilon_std_dev = 0.1\n",
    "\n",
    "    # Calculer la corrélation initiale\n",
    "    correlation = 0\n",
    "\n",
    "    x = y.copy()\n",
    "\n",
    "    z = y.to_numpy()\n",
    "    z = np.array([item for sublist in z for item in sublist])\n",
    "\n",
    "    \n",
    "    # Boucle pour ajuster l'écart-type du bruit jusqu'à ce que la corrélation atteigne eta\n",
    "    while correlation < eta:\n",
    "        # Generate a vector of Gaussian noise\n",
    "        noise = np.random.normal(0, epsilon_std_dev, len(y))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            x.iloc[i, 0] = y.iloc[i, 0] + noise[i]\n",
    "\n",
    "        w = x.to_numpy()\n",
    "        w = np.array([item for sublist in w for item in sublist])\n",
    "        # Calculate the new correlation\n",
    "        correlation = np.corrcoef(w, z)[0, 1]\n",
    "\n",
    "        # Adjust the standard deviation of the noise\n",
    "        epsilon_std_dev += 0.01  \n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08953295407306291, 0.014161012997184013, 0.0505755780228499,\n",
       "       -0.12672986341034503, -0.061263847822916766, -0.1415154927540358,\n",
       "       0.0034097653190171095, -0.060570312485364736, 0.09970722864783374,\n",
       "       0.15708617228963706, 0.04981438045306754, -0.21257699338229225,\n",
       "       -0.030810652909558107, -0.006230094221206882, 0.047434122269770965,\n",
       "       -0.17107143087501828, -0.06456472581336359, -0.15647180510013664,\n",
       "       -0.006940247623107859], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = noised_array(y=cluster_target_return, eta=0.01).iloc[:, 0].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>portfolio return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>return_5250</th>\n",
       "      <td>0.017398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5251</th>\n",
       "      <td>-0.012297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5252</th>\n",
       "      <td>0.020837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5253</th>\n",
       "      <td>0.022024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5254</th>\n",
       "      <td>-0.003849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5255</th>\n",
       "      <td>0.003957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5256</th>\n",
       "      <td>-0.022711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5257</th>\n",
       "      <td>-0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5258</th>\n",
       "      <td>-0.006371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_5259</th>\n",
       "      <td>-0.006529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             portfolio return\n",
       "return_5250          0.017398\n",
       "return_5251         -0.012297\n",
       "return_5252          0.020837\n",
       "return_5253          0.022024\n",
       "return_5254         -0.003849\n",
       "return_5255          0.003957\n",
       "return_5256         -0.022711\n",
       "return_5257         -0.005000\n",
       "return_5258         -0.006371\n",
       "return_5259         -0.006529"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_return = process.portfolio_returns(evaluation_window=10, df_cleaned=df_cleaned, lookback_window=lookback_window, consolidated_W=consolidated_weights)\n",
    "portfolio_return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
