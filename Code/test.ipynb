{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import process\n",
    "import numpy as np \n",
    "# Jerome path : r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\DataBase.csv'\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv'\n",
    "df = pd.read_csv(r'/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv')\n",
    "\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns.str[1:], format='%Y%m%d').strftime('%d/%m/%Y')\n",
    "\n",
    "df_cleaned = df.fillna(0) # Utilisez la méthode fillna(0) pour remplacer les NaN par 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 6 (2352418720.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[94], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    result = dict(zip(list(self.correlation_matrix.columns), self.apply_SPONGE())) ## composition\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 6\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "lookback_window = [0, 50]\n",
    "number_of_clusters = 38\n",
    "\n",
    "def cluster_composition_and_centroid(self):\n",
    "## cluster composition and centroids\n",
    "\n",
    "result = dict(zip(list(self.correlation_matrix.columns), self.apply_SPONGE())) ## composition\n",
    "\n",
    "df_cleaned['Cluster'] = df_cleaned.index.map(result)\n",
    "centroid_returns = df_cleaned.groupby('Cluster').mean() ## centroids \n",
    "\n",
    "df_cleaned = df_cleaned.transpose()\n",
    "centroid_returns = centroid_returns.transpose()\n",
    "\n",
    "## constituent_weights\n",
    "centred_returns = df_cleaned.copy()\n",
    "sigma = 0.01\n",
    "\n",
    "constituent_weights = pd.DataFrame(index=['Weight'], columns=centred_returns.columns)\n",
    "total_weight = pd.DataFrame(index=['Total weight'], columns=[i for i in range(number_of_clusters)], data=np.zeros((1, number_of_clusters)))\n",
    "\n",
    "## we first compute the difference between the cluster centroid return and the cluster ticker return\n",
    "for ticker in centred_returns.columns:\n",
    "    centred_returns[ticker][:-1] = centred_returns[ticker][:-1] - centroid_returns[int(centred_returns[ticker]['Cluster'])]\n",
    "\n",
    "## we use this difference to compute the distance between each asset and its cluster centroid return \n",
    "for ticker in centred_returns.columns:\n",
    "    constituent_weights[ticker] = np.exp(sigma*((np.linalg.norm(centred_returns[ticker][:-1]))**2)/2)\n",
    "    total_weight[int(centred_returns[ticker]['Cluster'])]['Total weight'] += np.exp(sigma*((np.linalg.norm(centred_returns[ticker][:-1]))**2)/2)\n",
    "\n",
    "## we normalize the weights\n",
    "for ticker in centred_returns.columns:\n",
    "    constituent_weights[ticker] = constituent_weights[ticker]['Weight']/total_weight[int(centred_returns[ticker]['Cluster'])]['Total weight']\n",
    "\n",
    "## check whether the weights equal to 0 within each cluster: \n",
    "# constituent_weights[[ticker for ticker in centred_returns.columns if centred_returns[ticker]['Cluster']  == 1.0]].sum(axis=1)\n",
    "    \n",
    "## cluster returns \n",
    "    \n",
    "cluster_returns = pd.DataFrame(index=df_cleaned.index[:-1], columns=np.arange(number_of_clusters), data=np.zeros((df_cleaned.shape[0] - 1, number_of_clusters)))\n",
    "\n",
    "for ticker in df_cleaned.columns:\n",
    "    cluster_returns[int(df_cleaned[ticker]['Cluster'])] = cluster_returns[int(df_cleaned[ticker]['Cluster'])] + constituent_weights[ticker]['Weight'] * df_cleaned[ticker][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "import process\n",
    "from PyFolioCC import PyFolioCC \n",
    "\n",
    "historical_data = df_cleaned\n",
    "lookback_window = [0, 50]\n",
    "evaluation_window = 1\n",
    "number_of_clusters = 38\n",
    "clustering_method = 'SPONGE'\n",
    "sigma = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "corr_matrix = process.correlation_matrix(lookback_window=lookback_window, df_cleaned=df_cleaned)\n",
    "cluster_comp = process.cluster_composition_and_centroid(df_cleaned=df_cleaned, correlation_matrix=corr_matrix, number_of_clusters=number_of_clusters, lookback_window=lookback_window, clustering_method=clustering_method)\n",
    "constituent_weights = process.constituent_weights(df_cleaned=df_cleaned, cluster_composition=cluster_comp, sigma=sigma, lookback_window=lookback_window)\n",
    "cluster_return_res = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, lookback_window=lookback_window)\n",
    "# markowitz_weights_res = process.markowitz_weights(cluster_return_res=cluster_return, constituent_weights=constituent_weights, df_cleaned=df_cleaned, lookback_window=lookback_window, evaluation_window=evaluation_window, eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = cluster_return_res.cov()\n",
    "\n",
    "cov_matrix.fillna(0.)\n",
    "\n",
    "## on construit le vecteur d'expected return du cluster (252 jours de trading par an, on passe de rendements journaliers à rendements annualisés)\n",
    "\n",
    "cluster_target_return = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, lookback_window=[lookback_window[1], lookback_window[1]+evaluation_window]).mean()\n",
    "\n",
    "expected_returns = process.noised_array(y=cluster_target_return, eta=eta)\n",
    "\n",
    "ef = EfficientFrontier(expected_returns=expected_returns, cov_matrix=cov_matrix, weight_bounds=(0, 1))\n",
    "\n",
    "ef.efficient_return(target_return=expected_returns.mean())\n",
    "\n",
    "markowitz_weights = ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('cluster 1', 0.0),\n",
       "             ('cluster 2', 0.07679),\n",
       "             ('cluster 3', 0.01276),\n",
       "             ('cluster 4', 0.0),\n",
       "             ('cluster 5', 0.0),\n",
       "             ('cluster 6', 0.0),\n",
       "             ('cluster 7', 0.00707),\n",
       "             ('cluster 8', 0.0),\n",
       "             ('cluster 9', 0.0),\n",
       "             ('cluster 10', 0.02374),\n",
       "             ('cluster 11', 0.66794),\n",
       "             ('cluster 12', 0.0),\n",
       "             ('cluster 13', 0.02627),\n",
       "             ('cluster 14', 0.03291),\n",
       "             ('cluster 15', 0.00318),\n",
       "             ('cluster 16', 0.0),\n",
       "             ('cluster 17', 0.0),\n",
       "             ('cluster 18', 0.01033),\n",
       "             ('cluster 19', 0.00849),\n",
       "             ('cluster 20', 0.01538),\n",
       "             ('cluster 21', 0.0),\n",
       "             ('cluster 22', 0.0),\n",
       "             ('cluster 23', 0.0003),\n",
       "             ('cluster 24', 0.03151),\n",
       "             ('cluster 25', 0.0),\n",
       "             ('cluster 26', 0.0),\n",
       "             ('cluster 27', 0.0),\n",
       "             ('cluster 28', 0.06241),\n",
       "             ('cluster 29', 0.00862),\n",
       "             ('cluster 30', 0.0),\n",
       "             ('cluster 31', 0.0),\n",
       "             ('cluster 32', 0.0),\n",
       "             ('cluster 33', 0.0),\n",
       "             ('cluster 34', 0.0),\n",
       "             ('cluster 35', 0.0028),\n",
       "             ('cluster 36', 0.0095),\n",
       "             ('cluster 37', 0.0),\n",
       "             ('cluster 38', 0.0)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markowitz_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_returns = pd.DataFrame(index = df_cleaned.columns[lookback_window[0]:lookback_window[1]], columns= [f'cluster {i}' for i in range(1, number_of_clusters + 1)], data = np.zeros((len(df_cleaned.columns[lookback_window[0]:lookback_window[1]]), len(constituent_weights) + 1)))\n",
    "\n",
    "for cluster in constituent_weights.keys():\n",
    "    for ticker, weight in constituent_weights[cluster].items():\n",
    "        cluster_returns[cluster] = cluster_returns[cluster] + df_cleaned.transpose()[ticker][lookback_window[0]:lookback_window[1]]*weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(695, 695) 37 37 37 37 692\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = process.correlation_matrix(lookback_window=lookback_window, df_cleaned=df_cleaned)\n",
    "cluster_comp = process.cluster_composition_and_centroid(df_cleaned=df_cleaned, correlation_matrix=corr_matrix, number_of_clusters=number_of_clusters, lookback_window=lookback_window, clustering_method=clustering_method)\n",
    "constituent_weights = process.constituent_weights(df_cleaned=df_cleaned, cluster_composition=cluster_comp, sigma=sigma, lookback_window=lookback_window)\n",
    "cluster_return = process.cluster_return(constituent_weights=constituent_weights, df_cleaned=df_cleaned, lookback_window=lookback_window)\n",
    "markowitz_weights_res = process.markowitz_weights(cluster_return_res=cluster_return, constituent_weights=constituent_weights, df_cleaned=df_cleaned, lookback_window=lookback_window, evaluation_window=evaluation_window, eta=eta)\n",
    "final_weights = process.final_weights(markowitz_weights=markowitz_weights_res, constituent_weights=constituent_weights)\n",
    "print(corr_matrix.shape, len(cluster_comp), len(constituent_weights), len(cluster_return), len(markowitz_weights_res), len(final_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVA 3.921716041425027e-05\n",
      "BKH 0.110883916406304\n",
      "BXS 0.2035512296849513\n",
      "DRQ 0.04082643926897566\n",
      "INCY 1.0230500389790201e-11\n",
      "LNT 0.1654632187161882\n",
      "MTZ 0.14365464588183466\n",
      "NI 0.05214005001251528\n",
      "OKE 0.2394986472578898\n",
      "PKI 0.01829458705579904\n",
      "RMD 0.02564804854489745\n"
     ]
    }
   ],
   "source": [
    "cluster_returns = pd.DataFrame(index = [f'cluster {i}' for i in range(1, len(constituent_weights) + 1)], columns = df_cleaned.columns[lookback_window[0]:lookback_window[1]])\n",
    "for keys, value in constituent_weights['cluster 1'].items():\n",
    "    print(keys, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
