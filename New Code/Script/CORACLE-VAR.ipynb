{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2a9b7b",
   "metadata": {},
   "source": [
    "## CORACLE-VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90181f3",
   "metadata": {},
   "source": [
    "### Data Processing Step for Confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c889cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\hktan\\OneDrive - University of California\\Codes\\ICAIF_25\\New Code\\Script\n",
      "Data loaded and cleaned. Sample (first 5 rows/cols):\n",
      "ticker            AA       ABM       ABT       ADI       ADM\n",
      "date                                                        \n",
      "2000-01-03 -0.013042 -0.009188 -0.007117 -0.036071  0.000000\n",
      "2000-01-04  0.010043  0.012346 -0.012786 -0.044261  0.005277\n",
      "2000-01-05  0.047628 -0.006192  0.011111  0.014493 -0.015915\n",
      "2000-01-06 -0.011713  0.000000  0.032553 -0.027719  0.010695\n",
      "2000-01-07 -0.016118  0.003091  0.028573  0.033654  0.005249\n",
      "Shape of the cleaned data: (5279, 663)\n",
      "Confounding Variables DataFrame constructed and cleaned. Sample (first 5 rows/cols):\n",
      "             DFF  T5YIE  USEPUINDXD  VIXCLS  DCOILWTICO  DTWEXBGS  DTWEXEMEGS\n",
      "date                                                                         \n",
      "2000-01-01  3.99    1.3       68.04   24.21       25.56  101.4155    100.9386\n",
      "2000-01-02  3.99    1.3      119.36   24.21       25.56  101.4155    100.9386\n",
      "2000-01-03  5.43    1.3       35.73   24.21       25.56  101.4155    100.9386\n",
      "2000-01-04  5.38    1.3      109.31   27.01       25.56  101.4155    100.9386\n",
      "2000-01-05  5.41    1.3      123.22   26.41       24.65  101.4155    100.9386\n",
      "Shape of the cleaned confounding variables data: (7671, 7)\n",
      "Filtered Confounding Variables DataFrame:\n",
      "             DFF  T5YIE  USEPUINDXD  VIXCLS  DCOILWTICO  DTWEXBGS  DTWEXEMEGS\n",
      "date                                                                         \n",
      "2000-01-03  5.43    1.3       35.73   24.21       25.56  101.4155    100.9386\n",
      "2000-01-04  5.38    1.3      109.31   27.01       25.56  101.4155    100.9386\n",
      "2000-01-05  5.41    1.3      123.22   26.41       24.65  101.4155    100.9386\n",
      "2000-01-06  5.54    1.3       53.61   25.73       24.79  101.4155    100.9386\n",
      "2000-01-07  5.61    1.3       42.03   21.72       24.79  101.4155    100.9386\n",
      "Shape of the filtered confounding variables data: (5279, 7)\n"
     ]
    }
   ],
   "source": [
    "## Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Helper imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal # For NYSE trading calendar\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "print(f\"Current Working Directory: {current_working_dir}\")\n",
    "project_root = os.path.dirname(current_working_dir)\n",
    "modules_path = os.path.join(project_root, 'Modules')\n",
    "if modules_path not in sys.path:\n",
    "    sys.path.append(modules_path)\n",
    "    print(f\"Added to sys.path for custom modules: {modules_path}\")\n",
    "\n",
    "####################################################################\n",
    "#### NYSE Daily Open-Close Returns\n",
    "####################################################################\n",
    "data_folder_path = os.path.join(project_root, 'Data')\n",
    "data_file_name = \"OPCL_20000103_20201231.csv\"   \n",
    "data_file_path = os.path.join(data_folder_path, data_file_name) # So that we get to the file itself and not the folder it is in\n",
    "returns_df = pd.read_csv(data_file_path) # Assumes file exists and is readable\n",
    "returns_df.set_index('ticker', inplace=True)\n",
    "returns_df.columns = pd.to_datetime(returns_df.columns.str.lstrip('X'), format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "returns_df_cleaned = returns_df.dropna().transpose() # Assumes dropna results in non-empty returns_df\n",
    "returns_df_cleaned.index = pd.to_datetime(returns_df_cleaned.index)\n",
    "returns_df_cleaned.index.name = 'date'\n",
    "print(\"Data loaded and cleaned. Sample (first 5 rows/cols):\")\n",
    "print(returns_df_cleaned.iloc[0:5,0:5])\n",
    "print(f\"Shape of the cleaned data: {returns_df_cleaned.shape}\")\n",
    "\n",
    "####################################################################\n",
    "#### Constructing the dataframe for the Confounding Variables\n",
    "####################################################################\n",
    "\n",
    "\"\"\"\n",
    "Data Source: Federal Reserve Economic Data (FRED)\n",
    "\n",
    "DFF -> Federal Funds Effective Rate\n",
    "T5YIE -> 5-Year Breakeven Inflation Rate (only from 2003)\n",
    "USEPUINDXD -> Economic Policy Uncertainty Index for United States; News-based, 7 days moving average\n",
    "VIX -> CBOE Volatility Index (VIX)\n",
    "DCOILWTICO -> West Texas Intermediate (WTI) Crude Oil Prices: Cushing, Oklahoma\n",
    "DTWEXBGS -> Broad U.S. Dollar Index: Trade Weighted Exchange Rate Index for Major Currencies (only from 2006)\n",
    "DTWEXEMEGS-> Broad U.S. Dollar Index: Trade Weighted Exchange Rate Index for Emerging Market Economies (only from 2006)\n",
    "\n",
    "Other Potential Data Sources:\n",
    "Gold Prices (Futures/Spot), etc.\n",
    "\"\"\"\n",
    "\n",
    "files = [\"DFF_20000103_20201231.csv\",\n",
    "         \"T5YIE_20030102_20201231.csv\",\n",
    "         \"USEPUINDXD_20000103_20201231.csv\",\n",
    "         \"VIX_20000103_20201231.csv\",\n",
    "         \"DCOILWTICO_20000103_20201231.csv\",\n",
    "         \"DTWEXBGS_20060102_20201231.csv\",\n",
    "         \"DTWEXEMEGS_20060102_20201231.csv\"\n",
    "         ]\n",
    "\n",
    "\n",
    "merged_confound_df = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=\"observation_date\", how=\"outer\"),\n",
    "    (pd.read_csv(os.path.join(data_folder_path, f), parse_dates=[0]) for f in files)\n",
    ")\n",
    "imputed_confound_df = merged_confound_df.interpolate(method='linear', limit_direction='both')\n",
    "imputed_confound_df.set_index('observation_date', inplace=True)\n",
    "imputed_confound_df.index.name = 'date'  # Renaming index to 'date'\n",
    "print(\"Confounding Variables DataFrame constructed and cleaned. Sample (first 5 rows/cols):\")\n",
    "print(imputed_confound_df.iloc[0:5,:])\n",
    "print(f\"Shape of the cleaned confounding variables data: {imputed_confound_df.shape}\")\n",
    "filtered_confound_df = imputed_confound_df[imputed_confound_df.index.isin(returns_df_cleaned.index)]\n",
    "print(\"Filtered Confounding Variables DataFrame:\")\n",
    "print(filtered_confound_df.head())\n",
    "print(f\"Shape of the filtered confounding variables data: {filtered_confound_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66985d",
   "metadata": {},
   "source": [
    "### Applying DML for orthogonalization\n",
    "\n",
    "Remarks on installing econml to run the code with the correct dependencies will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2678e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering packages etc\n",
    "import seaborn as sns\n",
    "from parallelized_runs import run_sliding_window_var_evaluation_vectorized\n",
    "import multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='statsmodels')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "try:\n",
    "    from signet.cluster import Cluster\n",
    "except ImportError:\n",
    "    print(\"Signet package not found. Attempting to install from GitHub...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/alan-turing-institute/SigNet.git\"]\n",
    "        )\n",
    "        # This part of the code should go first since importing parallelized_runs already requires the signet package\n",
    "        from signet.cluster import Cluster\n",
    "        print(\"Signet package installed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing Signet package: {e}\")\n",
    "        print(\"Please install it manually: pip install git+https://github.com/alan-turing-institute/SigNet.git\")\n",
    "\n",
    "\n",
    "from econml.dml import DML, LinearDML, SparseLinearDML\n",
    "from sklearn.linear_model import (Ridge, LassoCV, MultiTaskElasticNetCV)\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor \n",
    "from sklearn.base import clone\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbe1cf",
   "metadata": {},
   "source": [
    "### OR-VAR(p)\n",
    "\n",
    "We first look at a version of this without the ACLE (adaptive causal lag esimation) portion. This assumes that lag values are fixed, and we correct for confounding and fit for the relevant nuisance functions directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8d9f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2000-01-05', '2014-09-16'], dtype='datetime64[ns]', name='date', freq=None)\n",
      "DatetimeIndex(['2014-09-17', '2020-12-31'], dtype='datetime64[ns]', name='date', freq=None)\n",
      "Y_base_array shape: (5, 1, 20)\n",
      "Y_base shape after averaging: (1, 20)\n",
      "Theta shape: (20, 40)\n",
      "Y_hat_next shape: (1, 20)\n",
      "1‑step‑ahead forecast: [-0. -0. -0. -0. -0.] ...\n",
      "RMSE for 1-step-ahead point: 0.0110\n"
     ]
    }
   ],
   "source": [
    "p = 2                              # how many lags\n",
    "def make_lags(df, p):\n",
    "    return pd.concat([df.shift(k).add_suffix(f'_lag{k}') for k in range(1, p+1)], axis=1)\n",
    "\n",
    "#Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "T_df = make_lags(Y_df, p)\n",
    "W_df = make_lags(filtered_confound_df, p)\n",
    "#W_df = make_lags(filtered_confound_df,p)\n",
    "\n",
    "full = pd.concat([Y_df, T_df, W_df], axis=1).dropna()   # drop the first p rows with NaNs\n",
    "train_size = int(len(full)*0.7)\n",
    "\n",
    "train, test = full.iloc[:train_size], full.iloc[train_size:]\n",
    "print(train.index[[0,-1]])   # sanity‑check the split dates\n",
    "print(test.index[[0,-1]])\n",
    "\n",
    "\n",
    "# --- 2. align everything & turn into np.arrays ----------------------------\n",
    "Y_cols = Y_df.columns\n",
    "T_cols = T_df.columns\n",
    "W_cols = W_df.columns\n",
    "\n",
    "def to_arrays(df):\n",
    "    return df[Y_cols].values, df[T_cols].values, df[W_cols].values\n",
    "\n",
    "Y_tr, T_tr, W_tr = to_arrays(train)\n",
    "Y_te, T_te, W_te = to_arrays(test)\n",
    "\n",
    "# --- 3. fit Double ML ---------------------------------------------------\n",
    "tscv = TimeSeriesSplit(5)          # same blocked CV inside train only\n",
    "\n",
    "\n",
    "fast_tree = ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "est = LinearDML(model_y=fast_tree,\n",
    "                model_t=fast_tree,        \n",
    "                cv=tscv,\n",
    "                discrete_treatment=False,\n",
    "                random_state=0)\n",
    "\n",
    "est.fit(Y_tr, T_tr, X=None, W=W_tr)\n",
    "\n",
    "# --- 4. evaluate the model - CORRECTED --------------\n",
    "T_next = T_te[0, :].reshape(1, -1)\n",
    "W_next = W_te[0, :].reshape(1, -1)\n",
    "\n",
    "# The structure is: est.models_y[0] contains the 5 CV fold models\n",
    "Y_base_folds = []\n",
    "for model in est.models_y[0]:  # Note: iterate through est.models_y[0], not est.models_y\n",
    "    pred = model.predict(W_next)  # Shape: (1, 20)\n",
    "    Y_base_folds.append(pred)\n",
    "\n",
    "# Convert to array and average across folds\n",
    "Y_base_array = np.array(Y_base_folds)  # Shape: (5, 1, 20)\n",
    "print(f\"Y_base_array shape: {Y_base_array.shape}\")\n",
    "\n",
    "# Average across folds (axis=0)\n",
    "Y_base = np.mean(Y_base_array, axis=0)  # Shape: (1, 20)\n",
    "print(f\"Y_base shape after averaging: {Y_base.shape}\")\n",
    "\n",
    "# Get treatment effects\n",
    "theta = est.const_marginal_ate()  # Shape: (20, 60)\n",
    "print(f\"Theta shape: {theta.shape}\")\n",
    "\n",
    "# Compute prediction\n",
    "Y_hat_next = Y_base + T_next @ theta.T\n",
    "print(f\"Y_hat_next shape: {Y_hat_next.shape}\")\n",
    "print(\"1‑step‑ahead forecast:\", Y_hat_next.flatten()[:5], \"...\")\n",
    "\n",
    "# Compare with realized returns\n",
    "Y_true_next = Y_te[0, :]\n",
    "rmse_next = root_mean_squared_error(Y_true_next, Y_hat_next.flatten())\n",
    "print(f\"RMSE for 1-step-ahead point: {rmse_next:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ab90e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with 1 lags ===\n",
      "Y shape: 20, T shape: 20, W shape: 2\n",
      "Number of lags: 1\n",
      "T_cols sample: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Training shapes - Y: (3694, 20), T: (3694, 20), W: (3694, 2)\n",
      "Test shapes - Y: (1584, 20), T: (1584, 20), W: (1584, 2)\n",
      "Fitting DoubleML model...\n",
      "Treatment effects shape: (20, 20)\n",
      "Inside predict - theta shape: (20, 20)\n",
      "First prediction shapes:\n",
      "  Y_base: (1, 20)\n",
      "  treatment_effect: (1, 20)\n",
      "  Y_hat_t: (1, 20)\n",
      "RMSE with 1 lags: 0.018214\n",
      "\n",
      "=== Testing with 3 lags ===\n",
      "Y shape: 20, T shape: 60, W shape: 6\n",
      "Number of lags: 3\n",
      "T_cols sample: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Training shapes - Y: (3693, 20), T: (3693, 60), W: (3693, 6)\n",
      "Test shapes - Y: (1583, 20), T: (1583, 60), W: (1583, 6)\n",
      "Fitting DoubleML model...\n",
      "Treatment effects shape: (20, 60)\n",
      "Inside predict - theta shape: (20, 60)\n",
      "First prediction shapes:\n",
      "  Y_base: (1, 20)\n",
      "  treatment_effect: (1, 20)\n",
      "  Y_hat_t: (1, 20)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 370\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m5\u001b[39m]:\n\u001b[32m    369\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Testing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lags ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     results = \u001b[43mpredict_double_ml_full_with_lags\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_y_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mextra_trees\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_t_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mextra_trees\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRMSE with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lags: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mrmse_overall\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# Rolling window evaluation with lags\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mpredict_double_ml_full_with_lags\u001b[39m\u001b[34m(Y_df, W_df, p, model_y_name, model_t_name, model_y_params, model_t_params, cv_folds, train_ratio)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTreatment effects shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m predictions = \u001b[43mcorrected_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_te\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_te\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_te\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Calculate MSE\u001b[39;00m\n\u001b[32m    110\u001b[39m mse_per_asset = np.mean((Y_te - predictions) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 315\u001b[39m, in \u001b[36mcorrected_predict\u001b[39m\u001b[34m(est, T_te, W_te, Y_shape)\u001b[39m\n\u001b[32m    313\u001b[39m Y_base_folds = []\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m est.models_y[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_t\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Should be shape (1, 20)\u001b[39;00m\n\u001b[32m    316\u001b[39m     Y_base_folds.append(pred)\n\u001b[32m    318\u001b[39m Y_base = np.mean(Y_base_folds, axis=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (1, 20)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hktan\\miniconda3\\envs\\econml_env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:1076\u001b[39m, in \u001b[36mForestRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;66;03m# Parallel loop\u001b[39;00m\n\u001b[32m   1075\u001b[39m lock = threading.Lock()\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msharedmem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m y_hat /= \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_)\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hktan\\miniconda3\\envs\\econml_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hktan\\miniconda3\\envs\\econml_env\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hktan\\miniconda3\\envs\\econml_env\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hktan\\miniconda3\\envs\\econml_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m     config = {}\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hktan\\miniconda3\\envs\\econml_env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:731\u001b[39m, in \u001b[36m_accumulate_prediction\u001b[39m\u001b[34m(predict, X, out, lock)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[32m    725\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[33;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[32m    727\u001b[39m \n\u001b[32m    728\u001b[39m \u001b[33;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[32m    729\u001b[39m \u001b[33;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[32m    730\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     prediction = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m    733\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hktan\\miniconda3\\envs\\econml_env\\Lib\\site-packages\\sklearn\\tree\\_classes.py:530\u001b[39m, in \u001b[36mBaseDecisionTree.predict\u001b[39m\u001b[34m(self, X, check_input)\u001b[39m\n\u001b[32m    528\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    529\u001b[39m X = \u001b[38;5;28mself\u001b[39m._validate_X_predict(X, check_input)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m n_samples = X.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Classification\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from econml.dml import LinearDML\n",
    "\n",
    "def make_lags(df, p):\n",
    "    \"\"\"Create lagged variables for p periods\"\"\"\n",
    "    return pd.concat([df.shift(k).add_suffix(f'_lag{k}') for k in range(1, p+1)], axis=1)\n",
    "\n",
    "\n",
    "# --- 1. Wrapper for MultiOutput regressors ----------------------------\n",
    "def get_regressor(regressor_name, force_multioutput=True, **kwargs):\n",
    "    \"\"\"Factory function to create different regressors with MultiOutput wrapper\"\"\"\n",
    "    base_regressors = {\n",
    "        'extra_trees': ExtraTreesRegressor(\n",
    "            n_estimators=kwargs.get('n_estimators', 100),\n",
    "            max_depth=kwargs.get('max_depth', 5),\n",
    "            min_samples_split=kwargs.get('min_samples_split', 20),\n",
    "            random_state=kwargs.get('random_state', 0)\n",
    "        ),\n",
    "        'random_forest': RandomForestRegressor(\n",
    "            n_estimators=kwargs.get('n_estimators', 100),\n",
    "            max_depth=kwargs.get('max_depth', 5),\n",
    "            min_samples_split=kwargs.get('min_samples_split', 20),\n",
    "            random_state=kwargs.get('random_state', 0)\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    base_model = base_regressors[regressor_name]\n",
    "    \n",
    "    # For Y model (multiple outputs), we might need MultiOutputRegressor\n",
    "    if force_multioutput:\n",
    "        return MultiOutputRegressor(base_model)\n",
    "    else:\n",
    "        return base_model\n",
    "\n",
    "# --- 2. Corrected prediction function --------------------------------\n",
    "def predict_double_ml_full_with_lags(Y_df, W_df, \n",
    "                                    p=3,  # number of lags\n",
    "                                    model_y_name='extra_trees', \n",
    "                                    model_t_name='extra_trees',\n",
    "                                    model_y_params=None,\n",
    "                                    model_t_params=None,\n",
    "                                    cv_folds=5,\n",
    "                                    train_ratio=0.7):\n",
    "    \n",
    "    if model_y_params is None:\n",
    "        model_y_params = {}\n",
    "    if model_t_params is None:\n",
    "        model_t_params = {}\n",
    "    \n",
    "    # CREATE LAGGED TREATMENT VARIABLES\n",
    "    T_df = make_lags(Y_df, p)  # This creates lagged versions of Y as treatments\n",
    "    W_df = make_lags(W_df, p)  # Assuming W_df also needs lags\n",
    "    \n",
    "    # Create full dataset and drop NAs (first p rows will have NAs due to lags)\n",
    "    full = pd.concat([Y_df, T_df, W_df], axis=1).dropna()\n",
    "    train_size = int(len(full) * train_ratio)\n",
    "    \n",
    "    train, test = full.iloc[:train_size], full.iloc[train_size:]\n",
    "    \n",
    "    # Get column names\n",
    "    Y_cols = Y_df.columns  # 20 assets\n",
    "    T_cols = T_df.columns  # 20 assets × p lags\n",
    "    W_cols = W_df.columns  # confounders\n",
    "    \n",
    "    print(f\"Y shape: {len(Y_cols)}, T shape: {len(T_cols)}, W shape: {len(W_cols)}\")\n",
    "    print(f\"Number of lags: {p}\")\n",
    "    print(f\"T_cols sample: {T_cols[:5].tolist()}\")  # Show first 5 treatment columns\n",
    "    \n",
    "    def to_arrays(df):\n",
    "        return df[Y_cols].values, df[T_cols].values, df[W_cols].values\n",
    "    \n",
    "    Y_tr, T_tr, W_tr = to_arrays(train)\n",
    "    Y_te, T_te, W_te = to_arrays(test)\n",
    "    \n",
    "    print(f\"Training shapes - Y: {Y_tr.shape}, T: {T_tr.shape}, W: {W_tr.shape}\")\n",
    "    print(f\"Test shapes - Y: {Y_te.shape}, T: {T_te.shape}, W: {W_te.shape}\")\n",
    "    \n",
    "    # Create regressors\n",
    "    model_y = get_regressor(model_y_name, force_multioutput=False, **model_y_params)\n",
    "    model_t = get_regressor(model_t_name, force_multioutput=False, **model_t_params)\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    tscv = TimeSeriesSplit(cv_folds)\n",
    "    \n",
    "    # Fit DoubleML\n",
    "    est = LinearDML(\n",
    "        model_y=model_y,\n",
    "        model_t=model_t,\n",
    "        cv=tscv,\n",
    "        discrete_treatment=False,\n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    print(\"Fitting DoubleML model...\")\n",
    "    est.fit(Y_tr, T_tr, X=None, W=W_tr)\n",
    "    \n",
    "    # Get treatment effects and check dimensions\n",
    "    theta = est.const_marginal_ate()\n",
    "    print(f\"Treatment effects shape: {theta.shape}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = corrected_predict(est, T_te, W_te, Y_te.shape)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse_per_asset = np.mean((Y_te - predictions) ** 2, axis=0)\n",
    "    overall_rmse = np.sqrt(np.mean(mse_per_asset))\n",
    "    \n",
    "    results = {\n",
    "        'rmse_overall': overall_rmse,\n",
    "        'mse_per_asset': mse_per_asset,\n",
    "        'predictions': predictions,\n",
    "        'actuals': Y_te,\n",
    "        'model': est,\n",
    "        'theta_shape': theta.shape,\n",
    "        'asset_names': Y_cols,\n",
    "        'treatment_names': T_cols,\n",
    "        'n_lags': p\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Modified rolling window function with lags -------------------\n",
    "def rolling_window_evaluation_lookback_with_lags(Y_df, W_df,\n",
    "                                                p=3,  # number of lags\n",
    "                                                window_size=20,\n",
    "                                                lookback_days=252,\n",
    "                                                model_y_name='extra_trees',\n",
    "                                                model_t_name='extra_trees',\n",
    "                                                model_y_params=None,\n",
    "                                                model_t_params=None,\n",
    "                                                cv_folds=5,\n",
    "                                                train_ratio=0.7,\n",
    "                                                verbose=True):\n",
    "    \"\"\"\n",
    "    Rolling window evaluation with proper lagged treatment variables\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_y_params is None:\n",
    "        model_y_params = {}\n",
    "    if model_t_params is None:\n",
    "        model_t_params = {}\n",
    "    \n",
    "    # CREATE LAGGED TREATMENT VARIABLES\n",
    "    T_df = make_lags(Y_df, p)\n",
    "    \n",
    "    # Create full dataset and drop NAs\n",
    "    full = pd.concat([Y_df, T_df, W_df], axis=1).dropna()\n",
    "    \n",
    "    # Adjust for the fact that we lose p rows due to lagging\n",
    "    if train_ratio is None:\n",
    "        initial_train_size = lookback_days\n",
    "    else:\n",
    "        initial_train_size = int(max(train_ratio*full.shape[0], lookback_days))\n",
    "    \n",
    "    # Get column names\n",
    "    Y_cols = Y_df.columns\n",
    "    T_cols = T_df.columns\n",
    "    W_cols = W_df.columns\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Created {len(T_cols)} treatment variables from {p} lags of {len(Y_cols)} assets\")\n",
    "        print(f\"Sample treatment columns: {T_cols[:5].tolist()}\")\n",
    "    \n",
    "    # Storage for predictions and actuals\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    prediction_dates = []\n",
    "    training_times = []\n",
    "    \n",
    "    # Start from initial_train_size and go through each day\n",
    "    test_start = initial_train_size\n",
    "    test_end = len(full)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Starting rolling window evaluation with {p} lags...\")\n",
    "        print(f\"Lookback window: {lookback_days} days\")\n",
    "        print(f\"Initial training size: {initial_train_size}\")\n",
    "        print(f\"Test period: {test_end - test_start} days\")\n",
    "        print(f\"RMSE window size: {window_size} days\")\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for current_day in range(test_start, test_end):\n",
    "        if verbose and (current_day - test_start) % 10 == 0:\n",
    "            progress = (current_day - test_start) / (test_end - test_start) * 100\n",
    "            print(f\"Processing day {current_day - test_start + 1}/{test_end - test_start} ({progress:.1f}%)\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use only the last lookback_days for training\n",
    "        train_start_idx = max(0, current_day - lookback_days)\n",
    "        train_data = full.iloc[train_start_idx:current_day]\n",
    "        test_data = full.iloc[current_day:current_day+1]  # Just one day\n",
    "        \n",
    "        # Extract arrays\n",
    "        Y_tr = train_data[Y_cols].values\n",
    "        T_tr = train_data[T_cols].values\n",
    "        W_tr = train_data[W_cols].values\n",
    "        \n",
    "        Y_te = test_data[Y_cols].values[0]  # Get 1D array for single day\n",
    "        T_te = test_data[T_cols].values[0]\n",
    "        W_te = test_data[W_cols].values[0]\n",
    "        \n",
    "        # Create and fit model\n",
    "        model_y = get_regressor(model_y_name, force_multioutput=False, **model_y_params)\n",
    "        model_t = get_regressor(model_t_name, force_multioutput=False, **model_t_params)\n",
    "        \n",
    "        tscv = TimeSeriesSplit(cv_folds)\n",
    "        \n",
    "        est = LinearDML(\n",
    "            model_y=model_y,\n",
    "            model_t=model_t,\n",
    "            cv=tscv,\n",
    "            discrete_treatment=False,\n",
    "            random_state=0\n",
    "        )\n",
    "        \n",
    "        # Fit model\n",
    "        est.fit(Y_tr, T_tr, X=None, W=W_tr)\n",
    "        \n",
    "        # Make prediction for current day\n",
    "        prediction = corrected_predict_single_day(est, T_te, W_te)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.append(prediction)\n",
    "        all_actuals.append(Y_te)\n",
    "        prediction_dates.append(full.index[current_day])\n",
    "        \n",
    "        # Track timing\n",
    "        training_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actuals = np.array(all_actuals)\n",
    "    \n",
    "    # Calculate rolling window RMSE\n",
    "    window_rmses = []\n",
    "    window_dates = []\n",
    "    per_asset_window_rmses = []\n",
    "    \n",
    "    for i in range(len(all_predictions) - window_size + 1):\n",
    "        window_pred = all_predictions[i:i+window_size]\n",
    "        window_actual = all_actuals[i:i+window_size]\n",
    "        \n",
    "        # Calculate RMSE for this window\n",
    "        mse = np.mean((window_actual - window_pred) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Also calculate per-asset RMSE for this window\n",
    "        mse_per_asset = np.mean((window_actual - window_pred) ** 2, axis=0)\n",
    "        rmse_per_asset = np.sqrt(mse_per_asset)\n",
    "        \n",
    "        window_rmses.append(rmse)\n",
    "        per_asset_window_rmses.append(rmse_per_asset)\n",
    "        window_dates.append((prediction_dates[i], prediction_dates[i+window_size-1]))\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_mse_per_asset = np.mean((all_actuals - all_predictions) ** 2, axis=0)\n",
    "    overall_rmse_per_asset = np.sqrt(overall_mse_per_asset)\n",
    "    overall_mse = np.mean((all_actuals - all_predictions) ** 2)\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    \n",
    "    results = {\n",
    "        'predictions': all_predictions,\n",
    "        'actuals': all_actuals,\n",
    "        'prediction_dates': prediction_dates,\n",
    "        'window_rmses': np.array(window_rmses),\n",
    "        'per_asset_window_rmses': np.array(per_asset_window_rmses),\n",
    "        'window_dates': window_dates,\n",
    "        'rmse_per_asset': overall_rmse_per_asset,\n",
    "        'overall_rmse': overall_rmse,\n",
    "        'asset_names': Y_cols,\n",
    "        'treatment_names': T_cols,\n",
    "        'window_size': window_size,\n",
    "        'lookback_days': lookback_days,\n",
    "        'n_lags': p,\n",
    "        'avg_training_time': np.mean(training_times),\n",
    "        'total_training_time': np.sum(training_times)\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nEvaluation complete!\")\n",
    "        print(f\"Overall RMSE: {overall_rmse:.6f}\")\n",
    "        print(f\"Average window RMSE: {np.mean(window_rmses):.6f}\")\n",
    "        print(f\"Min window RMSE: {np.min(window_rmses):.6f}\")\n",
    "        print(f\"Max window RMSE: {np.max(window_rmses):.6f}\")\n",
    "        print(f\"Average training time per day: {np.mean(training_times):.2f} seconds\")\n",
    "        print(f\"Total training time: {np.sum(training_times)/60:.2f} minutes\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- 3. Corrected prediction logic -----------------------------------\n",
    "def corrected_predict(est, T_te, W_te, Y_shape):\n",
    "    \"\"\"Make predictions with proper dimension handling\"\"\"\n",
    "    n_test, n_assets = Y_shape\n",
    "    \n",
    "    predictions = np.zeros((n_test, n_assets))\n",
    "    \n",
    "    # Get treatment effects\n",
    "    theta = est.const_marginal_ate()\n",
    "    print(f\"Inside predict - theta shape: {theta.shape}\")\n",
    "    \n",
    "    for t in range(n_test):\n",
    "        T_t = T_te[t, :].reshape(1, -1)  # Shape: (1, 90)\n",
    "        W_t = W_te[t, :].reshape(1, -1)  # Shape: (1, n_confounders)\n",
    "        \n",
    "        # Get baseline prediction\n",
    "        Y_base_folds = []\n",
    "        for model in est.models_y[0]:\n",
    "            pred = model.predict(W_t)  # Should be shape (1, 20)\n",
    "            Y_base_folds.append(pred)\n",
    "        \n",
    "        Y_base = np.mean(Y_base_folds, axis=0)  # Shape: (1, 20)\n",
    "        \n",
    "        treatment_effect = T_t @ theta.T  # (1, 20p) @ (20p, 20) = (1, 20)\n",
    "        \n",
    "        # Compute final prediction\n",
    "        Y_hat_t = Y_base + treatment_effect  # Both should be (1, 20)\n",
    "        predictions[t, :] = Y_hat_t.flatten()\n",
    "        \n",
    "        if t == 0:  # Debug first prediction\n",
    "            print(f\"First prediction shapes:\")\n",
    "            print(f\"  Y_base: {Y_base.shape}\")\n",
    "            print(f\"  treatment_effect: {treatment_effect.shape}\")\n",
    "            print(f\"  Y_hat_t: {Y_hat_t.shape}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def corrected_predict_single_day(est, T_t, W_t):\n",
    "    \"\"\"Make prediction for a single day - more efficient version\"\"\"\n",
    "    # Ensure inputs are 2D\n",
    "    T_t = T_t.reshape(1, -1) if T_t.ndim == 1 else T_t  # Shape: (1, n_features)\n",
    "    W_t = W_t.reshape(1, -1) if W_t.ndim == 1 else W_t  # Shape: (1, n_confounders)\n",
    "    \n",
    "    # Get treatment effects\n",
    "    theta = est.const_marginal_ate()\n",
    "    \n",
    "    # Get baseline prediction by averaging across CV folds\n",
    "    Y_base_folds = []\n",
    "    for model in est.models_y[0]:\n",
    "        pred = model.predict(W_t)  # Shape: (1, n_assets)\n",
    "        Y_base_folds.append(pred)\n",
    "    \n",
    "    Y_base = np.mean(Y_base_folds, axis=0)  # Shape: (1, n_assets)\n",
    "    \n",
    "    # Calculate treatment effect\n",
    "    treatment_effect = T_t @ theta.T  # (1, n_features) @ (n_features, n_assets) = (1, n_assets)\n",
    "    \n",
    "    # Final prediction\n",
    "    Y_hat = Y_base + treatment_effect  # Both are (1, n_assets)\n",
    "    \n",
    "    return Y_hat.flatten()  # Return as 1D array of shape (n_assets,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 5. Usage examples -----------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage with proper lags\n",
    "    # Assuming you have Y_df (returns) and W_df (confounders) defined\n",
    "    \n",
    "    # Test with different numbers of lags\n",
    "    for p in [1, 3, 5]:\n",
    "        print(f\"\\n=== Testing with {p} lags ===\")\n",
    "        results = predict_double_ml_full_with_lags(\n",
    "            Y_df, W_df, \n",
    "            p=p,\n",
    "            model_y_name='extra_trees',\n",
    "            model_t_name='extra_trees',\n",
    "            train_ratio=0.7\n",
    "        )\n",
    "        print(f\"RMSE with {p} lags: {results['rmse_overall']:.6f}\")\n",
    "    \n",
    "    # Rolling window evaluation with lags\n",
    "    print(f\"\\n=== Rolling Window Evaluation with Lags ===\")\n",
    "    rolling_results = rolling_window_evaluation_lookback_with_lags(\n",
    "        Y_df, W_df,\n",
    "        p=3,  # Use 3 lags\n",
    "        window_size=20,\n",
    "        lookback_days=252,\n",
    "        train_ratio = 0.95,\n",
    "    )\n",
    "    print(f\"Rolling window RMSE: {rolling_results['overall_rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5fa4e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rolling Window Evaluation with Lags ===\n",
      "Created 20 treatment variables from 1 lags of 20 assets\n",
      "Sample treatment columns: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Starting rolling window evaluation with 1 lags...\n",
      "Lookback window: 252 days\n",
      "Initial training size: 5014\n",
      "Test period: 264 days\n",
      "RMSE window size: 20 days\n",
      "Processing day 1/264 (0.0%)\n",
      "Processing day 11/264 (3.8%)\n",
      "Processing day 21/264 (7.6%)\n",
      "Processing day 31/264 (11.4%)\n",
      "Processing day 41/264 (15.2%)\n",
      "Processing day 51/264 (18.9%)\n",
      "Processing day 61/264 (22.7%)\n",
      "Processing day 71/264 (26.5%)\n",
      "Processing day 81/264 (30.3%)\n",
      "Processing day 91/264 (34.1%)\n",
      "Processing day 101/264 (37.9%)\n",
      "Processing day 111/264 (41.7%)\n",
      "Processing day 121/264 (45.5%)\n",
      "Processing day 131/264 (49.2%)\n",
      "Processing day 141/264 (53.0%)\n",
      "Processing day 151/264 (56.8%)\n",
      "Processing day 161/264 (60.6%)\n",
      "Processing day 171/264 (64.4%)\n",
      "Processing day 181/264 (68.2%)\n",
      "Processing day 191/264 (72.0%)\n",
      "Processing day 201/264 (75.8%)\n",
      "Processing day 211/264 (79.5%)\n",
      "Processing day 221/264 (83.3%)\n",
      "Processing day 231/264 (87.1%)\n",
      "Processing day 241/264 (90.9%)\n",
      "Processing day 251/264 (94.7%)\n",
      "Processing day 261/264 (98.5%)\n",
      "\n",
      "Evaluation complete!\n",
      "Overall RMSE: 0.034236\n",
      "Average window RMSE: 0.030607\n",
      "Min window RMSE: 0.012368\n",
      "Max window RMSE: 0.087453\n",
      "Average training time per day: 1.00 seconds\n",
      "Total training time: 4.42 minutes\n",
      "Rolling window RMSE: 0.034236\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"\\n=== Rolling Window Evaluation with Lags ===\")\n",
    "    rolling_results_2 = rolling_window_evaluation_lookback_with_lags(\n",
    "        Y_df, W_df,\n",
    "        p=1,  # Use 1 lags\n",
    "        window_size=20,\n",
    "        lookback_days=252,\n",
    "        train_ratio = 0.95,\n",
    "    )\n",
    "    print(f\"Rolling window RMSE: {rolling_results_2['overall_rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def plot_rolling_rmse_detailed(results, figsize=(15, 10)):\n",
    "    \"\"\"Create a comprehensive visualization of rolling window RMSE results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle(f'Rolling Window RMSE Analysis (Window Size: {results[\"window_size\"]} days)', \n",
    "                 fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. Rolling RMSE over time\n",
    "    ax1 = axes[0, 0]\n",
    "    window_rmses = results['window_rmses']\n",
    "    window_start_indices = range(len(window_rmses))\n",
    "    \n",
    "    ax1.plot(window_start_indices, window_rmses, 'b-', linewidth=2, alpha=0.7)\n",
    "    ax1.axhline(y=results['overall_rmse'], color='r', linestyle='--', \n",
    "                linewidth=2, label=f'Overall RMSE: {results[\"overall_rmse\"]:.6f}')\n",
    "    ax1.axhline(y=np.mean(window_rmses), color='g', linestyle='--', \n",
    "                linewidth=2, label=f'Avg Window RMSE: {np.mean(window_rmses):.6f}')\n",
    "    \n",
    "    # Add confidence bands\n",
    "    rmse_std = np.std(window_rmses)\n",
    "    rmse_mean = np.mean(window_rmses)\n",
    "    ax1.fill_between(window_start_indices, \n",
    "                     rmse_mean - rmse_std, \n",
    "                     rmse_mean + rmse_std, \n",
    "                     alpha=0.2, color='gray', \n",
    "                     label=f'±1 STD: {rmse_std:.6f}')\n",
    "    \n",
    "    ax1.set_xlabel('Window Number')\n",
    "    ax1.set_ylabel('RMSE')\n",
    "    ax1.set_title('Rolling Window RMSE Over Time')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. RMSE Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(window_rmses, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.axvline(x=np.mean(window_rmses), color='g', linestyle='--', linewidth=2)\n",
    "    ax2.axvline(x=np.median(window_rmses), color='orange', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('RMSE')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Window RMSEs')\n",
    "    ax2.legend(['Mean', 'Median', 'Distribution'])\n",
    "    \n",
    "    # Add statistics text\n",
    "    stats_text = f'Mean: {np.mean(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Median: {np.median(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Std: {np.std(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Min: {np.min(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Max: {np.max(window_rmses):.6f}'\n",
    "    ax2.text(0.98, 0.97, stats_text, transform=ax2.transAxes, \n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 3. Per-Asset RMSE\n",
    "    ax3 = axes[1, 0]\n",
    "    rmse_per_asset = results['rmse_per_asset']\n",
    "    asset_names = results['asset_names']\n",
    "    \n",
    "    # Show top 10 assets by RMSE\n",
    "    n_assets_to_show = min(10, len(asset_names))\n",
    "    sorted_indices = np.argsort(rmse_per_asset)[::-1][:n_assets_to_show]\n",
    "    \n",
    "    y_pos = np.arange(n_assets_to_show)\n",
    "    ax3.barh(y_pos, rmse_per_asset[sorted_indices], alpha=0.7)\n",
    "    ax3.set_yticks(y_pos)\n",
    "    ax3.set_yticklabels([asset_names[i] for i in sorted_indices])\n",
    "    ax3.set_xlabel('RMSE')\n",
    "    ax3.set_title(f'Top {n_assets_to_show} Assets by RMSE')\n",
    "    ax3.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add average line\n",
    "    ax3.axvline(x=np.mean(rmse_per_asset), color='r', linestyle='--', \n",
    "                linewidth=2, label='Average')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Cumulative prediction error over time\n",
    "    ax4 = axes[1, 1]\n",
    "    predictions = results['predictions']\n",
    "    actuals = results['actuals']\n",
    "    \n",
    "    # Calculate cumulative squared error\n",
    "    squared_errors = (predictions - actuals) ** 2\n",
    "    cumulative_mse = np.cumsum(np.mean(squared_errors, axis=1))\n",
    "    cumulative_rmse = np.sqrt(cumulative_mse / np.arange(1, len(cumulative_mse) + 1))\n",
    "    \n",
    "    ax4.plot(cumulative_rmse, 'b-', linewidth=2)\n",
    "    ax4.set_xlabel('Prediction Day')\n",
    "    ax4.set_ylabel('Cumulative RMSE')\n",
    "    ax4.set_title('Cumulative RMSE Over Test Period')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add final value annotation\n",
    "    final_rmse = cumulative_rmse[-1]\n",
    "    ax4.text(0.98, 0.02, f'Final: {final_rmse:.6f}', \n",
    "             transform=ax4.transAxes, \n",
    "             verticalalignment='bottom', \n",
    "             horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_rmse_time_series(results, figsize=(14, 8)):\n",
    "    \"\"\"Plot RMSE time series with dates on x-axis\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, \n",
    "                                   gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Extract data\n",
    "    window_rmses = results['window_rmses']\n",
    "    window_dates = results['window_dates']\n",
    "    \n",
    "    # Get start dates for each window\n",
    "    window_start_dates = [dates[0] for dates in window_dates]\n",
    "    \n",
    "    # Convert to pandas for easier plotting\n",
    "    rmse_series = pd.Series(window_rmses, index=pd.to_datetime(window_start_dates))\n",
    "    \n",
    "    # Plot 1: RMSE over time\n",
    "    ax1.plot(rmse_series.index, rmse_series.values, 'b-', linewidth=2, label='Window RMSE')\n",
    "    \n",
    "    # Add moving average\n",
    "    ma_window = 10\n",
    "    rmse_ma = rmse_series.rolling(window=ma_window, center=True).mean()\n",
    "    ax1.plot(rmse_ma.index, rmse_ma.values, 'r-', linewidth=2, \n",
    "             label=f'{ma_window}-window MA', alpha=0.8)\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax1.axhline(y=results['overall_rmse'], color='green', linestyle='--', \n",
    "                linewidth=1.5, label=f'Overall RMSE: {results[\"overall_rmse\"]:.6f}')\n",
    "    \n",
    "    # Highlight high RMSE periods\n",
    "    threshold = np.percentile(window_rmses, 90)\n",
    "    high_rmse_mask = rmse_series > threshold\n",
    "    if high_rmse_mask.any():\n",
    "        ax1.fill_between(rmse_series.index, 0, 1, \n",
    "                        where=high_rmse_mask,\n",
    "                        transform=ax1.get_xaxis_transform(),\n",
    "                        alpha=0.2, color='red', \n",
    "                        label=f'Top 10% RMSE (>{threshold:.6f})')\n",
    "    \n",
    "    ax1.set_ylabel('RMSE')\n",
    "    ax1.set_title(f'Rolling {results[\"window_size\"]}-Day Window RMSE Over Time')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 2: Number of days with RMSE above/below average\n",
    "    ax2.bar(rmse_series.index, \n",
    "           (rmse_series > rmse_series.mean()).astype(int),\n",
    "           width=1, alpha=0.6, color='red', label='Above Average')\n",
    "    ax2.bar(rmse_series.index, \n",
    "           (rmse_series <= rmse_series.mean()).astype(int) * -1,\n",
    "           width=1, alpha=0.6, color='green', label='Below Average')\n",
    "    \n",
    "    ax2.set_ylabel('Above/Below\\nAverage')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax2.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_asset_heatmap(results, figsize=(12, 8)):\n",
    "    \"\"\"Create a heatmap of per-asset RMSE over time windows\"\"\"\n",
    "    \n",
    "    per_asset_window_rmses = results.get('per_asset_window_rmses')\n",
    "    if per_asset_window_rmses is None:\n",
    "        print(\"No per-asset window RMSE data available\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame for heatmap\n",
    "    asset_names = results['asset_names']\n",
    "    n_windows = len(per_asset_window_rmses)\n",
    "    \n",
    "    # Reshape data for heatmap\n",
    "    heatmap_data = pd.DataFrame(\n",
    "        per_asset_window_rmses.T,  # Transpose to have assets as rows\n",
    "        index=asset_names,\n",
    "        columns=[f'W{i}' for i in range(n_windows)]\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data, \n",
    "                cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'RMSE'},\n",
    "                ax=ax,\n",
    "                fmt='.6f',\n",
    "                linewidths=0.5)\n",
    "    \n",
    "    ax.set_title('Per-Asset RMSE Across Time Windows')\n",
    "    ax.set_xlabel('Window Number')\n",
    "    ax.set_ylabel('Asset')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Show every nth label if too many windows\n",
    "    if n_windows > 50:\n",
    "        nth = n_windows // 20\n",
    "        for i, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "            if i % nth != 0:\n",
    "                label.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_prediction_vs_actual_sample(results, n_assets_to_show=3, n_days_to_show=100):\n",
    "    \"\"\"Plot sample predictions vs actuals for selected assets\"\"\"\n",
    "    \n",
    "    predictions = results['predictions']\n",
    "    actuals = results['actuals']\n",
    "    asset_names = results['asset_names']\n",
    "    dates = results['prediction_dates']\n",
    "    \n",
    "    # Select assets to show (best, median, worst by RMSE)\n",
    "    rmse_per_asset = results['rmse_per_asset']\n",
    "    sorted_indices = np.argsort(rmse_per_asset)\n",
    "    \n",
    "    assets_to_show = [\n",
    "        sorted_indices[0],  # Best\n",
    "        sorted_indices[len(sorted_indices)//2],  # Median\n",
    "        sorted_indices[-1]  # Worst\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_assets_to_show, 1, figsize=(14, 4*n_assets_to_show))\n",
    "    if n_assets_to_show == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Limit days to show\n",
    "    end_idx = min(n_days_to_show, len(dates))\n",
    "    date_range = dates[:end_idx]\n",
    "    \n",
    "    for idx, (ax, asset_idx) in enumerate(zip(axes, assets_to_show)):\n",
    "        asset_name = asset_names[asset_idx]\n",
    "        asset_rmse = rmse_per_asset[asset_idx]\n",
    "        \n",
    "        # Plot predictions and actuals\n",
    "        ax.plot(date_range, actuals[:end_idx, asset_idx], \n",
    "                'b-', label='Actual', linewidth=2, alpha=0.7)\n",
    "        ax.plot(date_range, predictions[:end_idx, asset_idx], \n",
    "                'r--', label='Predicted', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Add error bands\n",
    "        errors = predictions[:end_idx, asset_idx] - actuals[:end_idx, asset_idx]\n",
    "        ax.fill_between(date_range, \n",
    "                       predictions[:end_idx, asset_idx] - np.std(errors),\n",
    "                       predictions[:end_idx, asset_idx] + np.std(errors),\n",
    "                       alpha=0.2, color='red', label='±1 STD Error')\n",
    "        \n",
    "        ax.set_title(f'{asset_name} - RMSE: {asset_rmse:.6f}')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format x-axis\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    axes[-1].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage with the results from rolling window evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have results from the rolling window evaluation\n",
    "    # results = rolling_window_evaluation_lookback(...)\n",
    "    \n",
    "    # Plot all visualizations\n",
    "    fig1 = plot_rolling_rmse_detailed(rolling_results)\n",
    "    plt.show()\n",
    "    \n",
    "    fig2 = plot_rmse_time_series(rolling_results)\n",
    "    plt.show()\n",
    "    \n",
    "    fig3 = plot_asset_heatmap(rolling_results)\n",
    "    plt.show()\n",
    "    \n",
    "    fig4 = plot_prediction_vs_actual_sample(rolling_results, n_assets_to_show=3)\n",
    "    plt.show()\n",
    "    \n",
    "    # print(\"Plotting functions ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
