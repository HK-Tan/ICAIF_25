{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2a9b7b",
   "metadata": {},
   "source": [
    "## CORACLE-VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90181f3",
   "metadata": {},
   "source": [
    "### Data Processing Step for Confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c889cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\hktan\\OneDrive - University of California\\Codes\\ICAIF_25\\New Code\\Script\n",
      "Data loaded and cleaned. Sample (first 5 rows/cols):\n",
      "ticker            AA       ABM       ABT       ADI       ADM\n",
      "date                                                        \n",
      "2000-01-03 -0.013042 -0.009188 -0.007117 -0.036071  0.000000\n",
      "2000-01-04  0.010043  0.012346 -0.012786 -0.044261  0.005277\n",
      "2000-01-05  0.047628 -0.006192  0.011111  0.014493 -0.015915\n",
      "2000-01-06 -0.011713  0.000000  0.032553 -0.027719  0.010695\n",
      "2000-01-07 -0.016118  0.003091  0.028573  0.033654  0.005249\n",
      "Shape of the cleaned data: (5279, 663)\n",
      "Confounding Variables DataFrame constructed and cleaned. Sample (first 5 rows/cols):\n",
      "             DFF  T5YIE  USEPUINDXD  VIXCLS  DCOILWTICO  DTWEXBGS  DTWEXEMEGS\n",
      "date                                                                         \n",
      "2000-01-01  3.99    1.3       68.04   24.21       25.56  101.4155    100.9386\n",
      "2000-01-02  3.99    1.3      119.36   24.21       25.56  101.4155    100.9386\n",
      "2000-01-03  5.43    1.3       35.73   24.21       25.56  101.4155    100.9386\n",
      "2000-01-04  5.38    1.3      109.31   27.01       25.56  101.4155    100.9386\n",
      "2000-01-05  5.41    1.3      123.22   26.41       24.65  101.4155    100.9386\n",
      "Shape of the cleaned confounding variables data: (7671, 7)\n",
      "Filtered Confounding Variables DataFrame:\n",
      "             DFF  T5YIE  USEPUINDXD  VIXCLS  DCOILWTICO  DTWEXBGS  DTWEXEMEGS\n",
      "date                                                                         \n",
      "2000-01-03  5.43    1.3       35.73   24.21       25.56  101.4155    100.9386\n",
      "2000-01-04  5.38    1.3      109.31   27.01       25.56  101.4155    100.9386\n",
      "2000-01-05  5.41    1.3      123.22   26.41       24.65  101.4155    100.9386\n",
      "2000-01-06  5.54    1.3       53.61   25.73       24.79  101.4155    100.9386\n",
      "2000-01-07  5.61    1.3       42.03   21.72       24.79  101.4155    100.9386\n",
      "Shape of the filtered confounding variables data: (5279, 7)\n"
     ]
    }
   ],
   "source": [
    "## Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Helper imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal # For NYSE trading calendar\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "print(f\"Current Working Directory: {current_working_dir}\")\n",
    "project_root = os.path.dirname(current_working_dir)\n",
    "modules_path = os.path.join(project_root, 'Modules')\n",
    "if modules_path not in sys.path:\n",
    "    sys.path.append(modules_path)\n",
    "    print(f\"Added to sys.path for custom modules: {modules_path}\")\n",
    "\n",
    "####################################################################\n",
    "#### NYSE Daily Open-Close Returns\n",
    "####################################################################\n",
    "data_folder_path = os.path.join(project_root, 'Data')\n",
    "data_file_name = \"OPCL_20000103_20201231.csv\"   \n",
    "data_file_path = os.path.join(data_folder_path, data_file_name) # So that we get to the file itself and not the folder it is in\n",
    "returns_df = pd.read_csv(data_file_path) # Assumes file exists and is readable\n",
    "returns_df.set_index('ticker', inplace=True)\n",
    "returns_df.columns = pd.to_datetime(returns_df.columns.str.lstrip('X'), format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "returns_df_cleaned = returns_df.dropna().transpose() # Assumes dropna results in non-empty returns_df\n",
    "returns_df_cleaned.index = pd.to_datetime(returns_df_cleaned.index)\n",
    "returns_df_cleaned.index.name = 'date'\n",
    "print(\"Data loaded and cleaned. Sample (first 5 rows/cols):\")\n",
    "print(returns_df_cleaned.iloc[0:5,0:5])\n",
    "print(f\"Shape of the cleaned data: {returns_df_cleaned.shape}\")\n",
    "\n",
    "####################################################################\n",
    "#### Constructing the dataframe for the Confounding Variables\n",
    "####################################################################\n",
    "\n",
    "\"\"\"\n",
    "Data Source: Federal Reserve Economic Data (FRED)\n",
    "\n",
    "DFF -> Federal Funds Effective Rate\n",
    "T5YIE -> 5-Year Breakeven Inflation Rate (only from 2003)\n",
    "USEPUINDXD -> Economic Policy Uncertainty Index for United States; News-based, 7 days moving average\n",
    "VIX -> CBOE Volatility Index (VIX)\n",
    "DCOILWTICO -> West Texas Intermediate (WTI) Crude Oil Prices: Cushing, Oklahoma\n",
    "DTWEXBGS -> Broad U.S. Dollar Index: Trade Weighted Exchange Rate Index for Major Currencies (only from 2006)\n",
    "DTWEXEMEGS-> Broad U.S. Dollar Index: Trade Weighted Exchange Rate Index for Emerging Market Economies (only from 2006)\n",
    "\n",
    "Other Potential Data Sources:\n",
    "Gold Prices (Futures/Spot), etc.\n",
    "\"\"\"\n",
    "\n",
    "files = [\"DFF_20000103_20201231.csv\",\n",
    "         \"T5YIE_20030102_20201231.csv\",\n",
    "         \"USEPUINDXD_20000103_20201231.csv\",\n",
    "         \"VIX_20000103_20201231.csv\",\n",
    "         \"DCOILWTICO_20000103_20201231.csv\",\n",
    "         \"DTWEXBGS_20060102_20201231.csv\",\n",
    "         \"DTWEXEMEGS_20060102_20201231.csv\"\n",
    "         ]\n",
    "\n",
    "\n",
    "merged_confound_df = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=\"observation_date\", how=\"outer\"),\n",
    "    (pd.read_csv(os.path.join(data_folder_path, f), parse_dates=[0]) for f in files)\n",
    ")\n",
    "imputed_confound_df = merged_confound_df.interpolate(method='linear', limit_direction='both')\n",
    "imputed_confound_df.set_index('observation_date', inplace=True)\n",
    "imputed_confound_df.index.name = 'date'  # Renaming index to 'date'\n",
    "print(\"Confounding Variables DataFrame constructed and cleaned. Sample (first 5 rows/cols):\")\n",
    "print(imputed_confound_df.iloc[0:5,:])\n",
    "print(f\"Shape of the cleaned confounding variables data: {imputed_confound_df.shape}\")\n",
    "filtered_confound_df = imputed_confound_df[imputed_confound_df.index.isin(returns_df_cleaned.index)]\n",
    "print(\"Filtered Confounding Variables DataFrame:\")\n",
    "print(filtered_confound_df.head())\n",
    "print(f\"Shape of the filtered confounding variables data: {filtered_confound_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66985d",
   "metadata": {},
   "source": [
    "### Applying DML for orthogonalization\n",
    "\n",
    "Remarks on installing econml to run the code with the correct dependencies will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2678e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering packages etc\n",
    "import seaborn as sns\n",
    "from parallelized_runs import run_sliding_window_var_evaluation_vectorized\n",
    "import multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='statsmodels')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "try:\n",
    "    from signet.cluster import Cluster\n",
    "except ImportError:\n",
    "    print(\"Signet package not found. Attempting to install from GitHub...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/alan-turing-institute/SigNet.git\"]\n",
    "        )\n",
    "        # This part of the code should go first since importing parallelized_runs already requires the signet package\n",
    "        from signet.cluster import Cluster\n",
    "        print(\"Signet package installed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing Signet package: {e}\")\n",
    "        print(\"Please install it manually: pip install git+https://github.com/alan-turing-institute/SigNet.git\")\n",
    "\n",
    "\n",
    "from econml.dml import LinearDML, SparseLinearDML\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor \n",
    "from sklearn.base import clone\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbe1cf",
   "metadata": {},
   "source": [
    "### OR-VAR(p)\n",
    "\n",
    "We first look at a version of this without the ACLE (adaptive causal lag esimation) portion. This assumes that lag values are fixed, and we correct for confounding and fit for the relevant nuisance functions directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f8d9f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2000-01-07', '2014-09-17'], dtype='datetime64[ns]', name='date', freq=None)\n",
      "DatetimeIndex(['2014-09-18', '2020-12-31'], dtype='datetime64[ns]', name='date', freq=None)\n",
      "Y_base_array shape: (5, 1, 20)\n",
      "Y_base shape after averaging: (1, 20)\n",
      "Theta shape: (20, 80)\n",
      "Y_hat_next shape: (1, 20)\n",
      "1‑step‑ahead forecast: [0.   0.   0.   0.   0.01] ...\n",
      "RMSE for 1-step-ahead point: 0.0135\n"
     ]
    }
   ],
   "source": [
    "p = 4                           # how many lags\n",
    "def make_lags(df, p):\n",
    "    return pd.concat([df.shift(k).add_suffix(f'_lag{k}') for k in range(1, p+1)], axis=1)\n",
    "\n",
    "#Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "T_df = make_lags(Y_df, p)\n",
    "W_df = make_lags(filtered_confound_df, p)\n",
    "#W_df = make_lags(filtered_confound_df,p)\n",
    "\n",
    "full = pd.concat([Y_df, T_df, W_df], axis=1).dropna()   # drop the first p rows with NaNs\n",
    "train_size = int(len(full)*0.7)\n",
    "\n",
    "train, test = full.iloc[:train_size], full.iloc[train_size:]\n",
    "print(train.index[[0,-1]])   # sanity‑check the split dates\n",
    "print(test.index[[0,-1]])\n",
    "\n",
    "\n",
    "# --- 2. align everything & turn into np.arrays ----------------------------\n",
    "Y_cols = Y_df.columns\n",
    "T_cols = T_df.columns\n",
    "W_cols = W_df.columns\n",
    "\n",
    "def to_arrays(df):\n",
    "    return df[Y_cols].values, df[T_cols].values, df[W_cols].values\n",
    "\n",
    "Y_tr, T_tr, W_tr = to_arrays(train)\n",
    "Y_te, T_te, W_te = to_arrays(test)\n",
    "\n",
    "# --- 3. fit Double ML ---------------------------------------------------\n",
    "tscv = TimeSeriesSplit(5)          # same blocked CV inside train only\n",
    "\n",
    "\n",
    "fast_tree = ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "est = LinearDML(model_y=fast_tree,\n",
    "                model_t=fast_tree,        \n",
    "                cv=tscv,\n",
    "                discrete_treatment=False,\n",
    "                random_state=0)\n",
    "\n",
    "est.fit(Y_tr, T_tr, X=None, W=W_tr)\n",
    "\n",
    "# --- 4. evaluate the model - CORRECTED --------------\n",
    "T_next = T_te[0, :].reshape(1, -1)\n",
    "W_next = W_te[0, :].reshape(1, -1)\n",
    "\n",
    "# The structure is: est.models_y[0] contains the 5 CV fold models\n",
    "Y_base_folds = []\n",
    "for model in est.models_y[0]:  # Note: iterate through est.models_y[0], not est.models_y\n",
    "    pred = model.predict(W_next)  # Shape: (1, 20)\n",
    "    Y_base_folds.append(pred)\n",
    "\n",
    "# Convert to array and average across folds\n",
    "Y_base_array = np.array(Y_base_folds)  # Shape: (5, 1, 20)\n",
    "print(f\"Y_base_array shape: {Y_base_array.shape}\")\n",
    "\n",
    "# Average across folds (axis=0)\n",
    "Y_base = np.mean(Y_base_array, axis=0)  # Shape: (1, 20)\n",
    "print(f\"Y_base shape after averaging: {Y_base.shape}\")\n",
    "\n",
    "# Get treatment effects\n",
    "theta = est.const_marginal_ate()  # Shape: (20, 60)\n",
    "print(f\"Theta shape: {theta.shape}\")\n",
    "\n",
    "# Compute prediction\n",
    "Y_hat_next = Y_base + T_next @ theta.T\n",
    "print(f\"Y_hat_next shape: {Y_hat_next.shape}\")\n",
    "print(\"1‑step‑ahead forecast:\", Y_hat_next.flatten()[:5], \"...\")\n",
    "\n",
    "# Compare with realized returns\n",
    "Y_true_next = Y_te[0, :]\n",
    "rmse_next = root_mean_squared_error(Y_true_next, Y_hat_next.flatten())\n",
    "print(f\"RMSE for 1-step-ahead point: {rmse_next:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3ab90e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with 1 lags ===\n",
      "Y shape: 20, T shape: 20, W shape: 7\n",
      "Number of lags: 1\n",
      "T_cols sample: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Training shapes - Y: (3694, 20), T: (3694, 20), W: (3694, 7)\n",
      "Test shapes - Y: (1584, 20), T: (1584, 20), W: (1584, 7)\n",
      "Fitting DoubleML model...\n",
      "Treatment effects shape: (20, 20)\n",
      "Inside predict - theta shape: (20, 20)\n",
      "First prediction shapes:\n",
      "  Y_base: (1, 20)\n",
      "  treatment_effect: (1, 20)\n",
      "  Y_hat_t: (1, 20)\n",
      "RMSE with 1 lags: 0.017453\n",
      "\n",
      "=== Testing with 3 lags ===\n",
      "Y shape: 20, T shape: 60, W shape: 21\n",
      "Number of lags: 3\n",
      "T_cols sample: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Training shapes - Y: (3693, 20), T: (3693, 60), W: (3693, 21)\n",
      "Test shapes - Y: (1583, 20), T: (1583, 60), W: (1583, 21)\n",
      "Fitting DoubleML model...\n",
      "Treatment effects shape: (20, 60)\n",
      "Inside predict - theta shape: (20, 60)\n",
      "First prediction shapes:\n",
      "  Y_base: (1, 20)\n",
      "  treatment_effect: (1, 20)\n",
      "  Y_hat_t: (1, 20)\n",
      "RMSE with 3 lags: 0.017852\n",
      "\n",
      "=== Testing with 5 lags ===\n",
      "Y shape: 20, T shape: 100, W shape: 35\n",
      "Number of lags: 5\n",
      "T_cols sample: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Training shapes - Y: (3691, 20), T: (3691, 100), W: (3691, 35)\n",
      "Test shapes - Y: (1583, 20), T: (1583, 100), W: (1583, 35)\n",
      "Fitting DoubleML model...\n",
      "Treatment effects shape: (20, 100)\n",
      "Inside predict - theta shape: (20, 100)\n",
      "First prediction shapes:\n",
      "  Y_base: (1, 20)\n",
      "  treatment_effect: (1, 20)\n",
      "  Y_hat_t: (1, 20)\n",
      "RMSE with 5 lags: 0.018395\n",
      "\n",
      "=== Rolling Window Evaluation with Lags ===\n",
      "Created 60 treatment variables from 3 lags of 20 assets\n",
      "Sample treatment columns: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Starting rolling window evaluation with 3 lags...\n",
      "Lookback window: 252 days\n",
      "Initial training size: 5012\n",
      "Test period: 264 days\n",
      "RMSE window size: 20 days\n",
      "Processing day 1/264 (0.0%)\n",
      "Processing day 11/264 (3.8%)\n",
      "Processing day 21/264 (7.6%)\n",
      "Processing day 31/264 (11.4%)\n",
      "Processing day 41/264 (15.2%)\n",
      "Processing day 51/264 (18.9%)\n",
      "Processing day 61/264 (22.7%)\n",
      "Processing day 71/264 (26.5%)\n",
      "Processing day 81/264 (30.3%)\n",
      "Processing day 91/264 (34.1%)\n",
      "Processing day 101/264 (37.9%)\n",
      "Processing day 111/264 (41.7%)\n",
      "Processing day 121/264 (45.5%)\n",
      "Processing day 131/264 (49.2%)\n",
      "Processing day 141/264 (53.0%)\n",
      "Processing day 151/264 (56.8%)\n",
      "Processing day 161/264 (60.6%)\n",
      "Processing day 171/264 (64.4%)\n",
      "Processing day 181/264 (68.2%)\n",
      "Processing day 191/264 (72.0%)\n",
      "Processing day 201/264 (75.8%)\n",
      "Processing day 211/264 (79.5%)\n",
      "Processing day 221/264 (83.3%)\n",
      "Processing day 231/264 (87.1%)\n",
      "Processing day 241/264 (90.9%)\n",
      "Processing day 251/264 (94.7%)\n",
      "Processing day 261/264 (98.5%)\n",
      "\n",
      "Evaluation complete!\n",
      "Overall RMSE: 0.036631\n",
      "Average window RMSE: 0.033500\n",
      "Min window RMSE: 0.014051\n",
      "Max window RMSE: 0.086589\n",
      "Average training time per day: 0.60 seconds\n",
      "Total training time: 2.63 minutes\n",
      "Rolling window RMSE: 0.036631\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from econml.dml import LinearDML\n",
    "\n",
    "def make_lags(df, p):\n",
    "    \"\"\"Create lagged variables for p periods\"\"\"\n",
    "    return pd.concat([df.shift(k).add_suffix(f'_lag{k}') for k in range(1, p+1)], axis=1)\n",
    "\n",
    "Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "T_df = make_lags(Y_df, p)\n",
    "W_df = filtered_confound_df\n",
    "\n",
    "# --- 1. Wrapper for MultiOutput regressors ----------------------------\n",
    "def get_regressor(regressor_name, force_multioutput=True, **kwargs):\n",
    "    \"\"\"Factory function to create different regressors with MultiOutput wrapper\"\"\"\n",
    "    base_regressors = {\n",
    "        'extra_trees': ExtraTreesRegressor(\n",
    "            n_estimators=kwargs.get('n_estimators', 100),\n",
    "            max_depth=kwargs.get('max_depth', 5),\n",
    "            min_samples_split=kwargs.get('min_samples_split', 20),\n",
    "            random_state=kwargs.get('random_state', 0)\n",
    "        ),\n",
    "        'random_forest': RandomForestRegressor(\n",
    "            n_estimators=kwargs.get('n_estimators', 100),\n",
    "            max_depth=kwargs.get('max_depth', 5),\n",
    "            min_samples_split=kwargs.get('min_samples_split', 20),\n",
    "            random_state=kwargs.get('random_state', 0)\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    base_model = base_regressors[regressor_name]\n",
    "    \n",
    "    # For Y model (multiple outputs), we might need MultiOutputRegressor\n",
    "    if force_multioutput:\n",
    "        return MultiOutputRegressor(base_model)\n",
    "    else:\n",
    "        return base_model\n",
    "\n",
    "# --- 2. Corrected prediction function --------------------------------\n",
    "def predict_double_ml_full_with_lags(Y_df, W_df, \n",
    "                                    p=3,  # number of lags\n",
    "                                    model_y_name='extra_trees', \n",
    "                                    model_t_name='extra_trees',\n",
    "                                    model_y_params=None,\n",
    "                                    model_t_params=None,\n",
    "                                    cv_folds=5,\n",
    "                                    train_ratio=0.7):\n",
    "    \n",
    "    if model_y_params is None:\n",
    "        model_y_params = {}\n",
    "    if model_t_params is None:\n",
    "        model_t_params = {}\n",
    "    \n",
    "    # CREATE LAGGED TREATMENT VARIABLES\n",
    "    T_df = make_lags(Y_df, p)  # This creates lagged versions of Y as treatments\n",
    "    W_df = make_lags(W_df, p)  # Assuming W_df also needs lags\n",
    "    \n",
    "    # Create full dataset and drop NAs (first p rows will have NAs due to lags)\n",
    "    full = pd.concat([Y_df, T_df, W_df], axis=1).dropna()\n",
    "    train_size = int(len(full) * train_ratio)\n",
    "    \n",
    "    train, test = full.iloc[:train_size], full.iloc[train_size:]\n",
    "    \n",
    "    # Get column names\n",
    "    Y_cols = Y_df.columns  # 20 assets\n",
    "    T_cols = T_df.columns  # 20 assets × p lags\n",
    "    W_cols = W_df.columns  # confounders\n",
    "    \n",
    "    print(f\"Y shape: {len(Y_cols)}, T shape: {len(T_cols)}, W shape: {len(W_cols)}\")\n",
    "    print(f\"Number of lags: {p}\")\n",
    "    print(f\"T_cols sample: {T_cols[:5].tolist()}\")  # Show first 5 treatment columns\n",
    "    \n",
    "    def to_arrays(df):\n",
    "        return df[Y_cols].values, df[T_cols].values, df[W_cols].values\n",
    "    \n",
    "    Y_tr, T_tr, W_tr = to_arrays(train)\n",
    "    Y_te, T_te, W_te = to_arrays(test)\n",
    "    \n",
    "    print(f\"Training shapes - Y: {Y_tr.shape}, T: {T_tr.shape}, W: {W_tr.shape}\")\n",
    "    print(f\"Test shapes - Y: {Y_te.shape}, T: {T_te.shape}, W: {W_te.shape}\")\n",
    "    \n",
    "    # Create regressors\n",
    "    model_y = get_regressor(model_y_name, force_multioutput=False, **model_y_params)\n",
    "    model_t = get_regressor(model_t_name, force_multioutput=False, **model_t_params)\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    tscv = TimeSeriesSplit(cv_folds)\n",
    "    \n",
    "    # Fit DoubleML\n",
    "    est = LinearDML(\n",
    "        model_y=model_y,\n",
    "        model_t=model_t,\n",
    "        cv=tscv,\n",
    "        discrete_treatment=False,\n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    print(\"Fitting DoubleML model...\")\n",
    "    est.fit(Y_tr, T_tr, X=None, W=W_tr)\n",
    "    \n",
    "    # Get treatment effects and check dimensions\n",
    "    theta = est.const_marginal_ate()\n",
    "    print(f\"Treatment effects shape: {theta.shape}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = corrected_predict(est, T_te, W_te, Y_te.shape)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse_per_asset = np.mean((Y_te - predictions) ** 2, axis=0)\n",
    "    overall_rmse = np.sqrt(np.mean(mse_per_asset))\n",
    "    \n",
    "    results = {\n",
    "        'rmse_overall': overall_rmse,\n",
    "        'mse_per_asset': mse_per_asset,\n",
    "        'predictions': predictions,\n",
    "        'actuals': Y_te,\n",
    "        'model': est,\n",
    "        'theta_shape': theta.shape,\n",
    "        'asset_names': Y_cols,\n",
    "        'treatment_names': T_cols,\n",
    "        'n_lags': p\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Modified rolling window function with lags -------------------\n",
    "def rolling_window_evaluation_lookback_with_lags(Y_df, W_df,\n",
    "                                                p=3,  # number of lags\n",
    "                                                window_size=20,\n",
    "                                                lookback_days=252,\n",
    "                                                model_y_name='extra_trees',\n",
    "                                                model_t_name='extra_trees',\n",
    "                                                model_y_params=None,\n",
    "                                                model_t_params=None,\n",
    "                                                cv_folds=5,\n",
    "                                                train_ratio=0.7,\n",
    "                                                verbose=True):\n",
    "    \"\"\"\n",
    "    Rolling window evaluation with proper lagged treatment variables\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_y_params is None:\n",
    "        model_y_params = {}\n",
    "    if model_t_params is None:\n",
    "        model_t_params = {}\n",
    "    \n",
    "    # CREATE LAGGED TREATMENT VARIABLES\n",
    "    T_df = make_lags(Y_df, p)\n",
    "    \n",
    "    # Create full dataset and drop NAs\n",
    "    full = pd.concat([Y_df, T_df, W_df], axis=1).dropna()\n",
    "    \n",
    "    # Adjust for the fact that we lose p rows due to lagging\n",
    "    if train_ratio is None:\n",
    "        initial_train_size = lookback_days\n",
    "    else:\n",
    "        initial_train_size = int(max(train_ratio*full.shape[0], lookback_days))\n",
    "    \n",
    "    # Get column names\n",
    "    Y_cols = Y_df.columns\n",
    "    T_cols = T_df.columns\n",
    "    W_cols = W_df.columns\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Created {len(T_cols)} treatment variables from {p} lags of {len(Y_cols)} assets\")\n",
    "        print(f\"Sample treatment columns: {T_cols[:5].tolist()}\")\n",
    "    \n",
    "    # Storage for predictions and actuals\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    prediction_dates = []\n",
    "    training_times = []\n",
    "    \n",
    "    # Start from initial_train_size and go through each day\n",
    "    test_start = initial_train_size\n",
    "    test_end = len(full)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Starting rolling window evaluation with {p} lags...\")\n",
    "        print(f\"Lookback window: {lookback_days} days\")\n",
    "        print(f\"Initial training size: {initial_train_size}\")\n",
    "        print(f\"Test period: {test_end - test_start} days\")\n",
    "        print(f\"RMSE window size: {window_size} days\")\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for current_day in range(test_start, test_end):\n",
    "        if verbose and (current_day - test_start) % 10 == 0:\n",
    "            progress = (current_day - test_start) / (test_end - test_start) * 100\n",
    "            print(f\"Processing day {current_day - test_start + 1}/{test_end - test_start} ({progress:.1f}%)\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use only the last lookback_days for training\n",
    "        train_start_idx = max(0, current_day - lookback_days)\n",
    "        train_data = full.iloc[train_start_idx:current_day]\n",
    "        test_data = full.iloc[current_day:current_day+1]  # Just one day\n",
    "        \n",
    "        # Extract arrays\n",
    "        Y_tr = train_data[Y_cols].values\n",
    "        T_tr = train_data[T_cols].values\n",
    "        W_tr = train_data[W_cols].values\n",
    "        \n",
    "        Y_te = test_data[Y_cols].values[0]  # Get 1D array for single day\n",
    "        T_te = test_data[T_cols].values[0]\n",
    "        W_te = test_data[W_cols].values[0]\n",
    "        \n",
    "        # Create and fit model\n",
    "        model_y = get_regressor(model_y_name, force_multioutput=False, **model_y_params)\n",
    "        model_t = get_regressor(model_t_name, force_multioutput=False, **model_t_params)\n",
    "        \n",
    "        tscv = TimeSeriesSplit(cv_folds)\n",
    "        \n",
    "        est = LinearDML(\n",
    "            model_y=model_y,\n",
    "            model_t=model_t,\n",
    "            cv=tscv,\n",
    "            discrete_treatment=False,\n",
    "            random_state=0\n",
    "        )\n",
    "        \n",
    "        # Fit model\n",
    "        est.fit(Y_tr, T_tr, X=None, W=W_tr)\n",
    "        \n",
    "        # Make prediction for current day\n",
    "        prediction = corrected_predict_single_day(est, T_te, W_te)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.append(prediction)\n",
    "        all_actuals.append(Y_te)\n",
    "        prediction_dates.append(full.index[current_day])\n",
    "        \n",
    "        # Track timing\n",
    "        training_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actuals = np.array(all_actuals)\n",
    "    \n",
    "    # Calculate rolling window RMSE\n",
    "    window_rmses = []\n",
    "    window_dates = []\n",
    "    per_asset_window_rmses = []\n",
    "    \n",
    "    for i in range(len(all_predictions) - window_size + 1):\n",
    "        window_pred = all_predictions[i:i+window_size]\n",
    "        window_actual = all_actuals[i:i+window_size]\n",
    "        \n",
    "        # Calculate RMSE for this window\n",
    "        mse = np.mean((window_actual - window_pred) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Also calculate per-asset RMSE for this window\n",
    "        mse_per_asset = np.mean((window_actual - window_pred) ** 2, axis=0)\n",
    "        rmse_per_asset = np.sqrt(mse_per_asset)\n",
    "        \n",
    "        window_rmses.append(rmse)\n",
    "        per_asset_window_rmses.append(rmse_per_asset)\n",
    "        window_dates.append((prediction_dates[i], prediction_dates[i+window_size-1]))\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_mse_per_asset = np.mean((all_actuals - all_predictions) ** 2, axis=0)\n",
    "    overall_rmse_per_asset = np.sqrt(overall_mse_per_asset)\n",
    "    overall_mse = np.mean((all_actuals - all_predictions) ** 2)\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    \n",
    "    results = {\n",
    "        'predictions': all_predictions,\n",
    "        'actuals': all_actuals,\n",
    "        'prediction_dates': prediction_dates,\n",
    "        'window_rmses': np.array(window_rmses),\n",
    "        'per_asset_window_rmses': np.array(per_asset_window_rmses),\n",
    "        'window_dates': window_dates,\n",
    "        'rmse_per_asset': overall_rmse_per_asset,\n",
    "        'overall_rmse': overall_rmse,\n",
    "        'asset_names': Y_cols,\n",
    "        'treatment_names': T_cols,\n",
    "        'window_size': window_size,\n",
    "        'lookback_days': lookback_days,\n",
    "        'n_lags': p,\n",
    "        'avg_training_time': np.mean(training_times),\n",
    "        'total_training_time': np.sum(training_times)\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nEvaluation complete!\")\n",
    "        print(f\"Overall RMSE: {overall_rmse:.6f}\")\n",
    "        print(f\"Average window RMSE: {np.mean(window_rmses):.6f}\")\n",
    "        print(f\"Min window RMSE: {np.min(window_rmses):.6f}\")\n",
    "        print(f\"Max window RMSE: {np.max(window_rmses):.6f}\")\n",
    "        print(f\"Average training time per day: {np.mean(training_times):.2f} seconds\")\n",
    "        print(f\"Total training time: {np.sum(training_times)/60:.2f} minutes\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- 3. Corrected prediction logic -----------------------------------\n",
    "def corrected_predict(est, T_te, W_te, Y_shape):\n",
    "    \"\"\"Make predictions with proper dimension handling\"\"\"\n",
    "    n_test, n_assets = Y_shape\n",
    "    \n",
    "    predictions = np.zeros((n_test, n_assets))\n",
    "    \n",
    "    # Get treatment effects\n",
    "    theta = est.const_marginal_ate()\n",
    "    print(f\"Inside predict - theta shape: {theta.shape}\")\n",
    "    \n",
    "    for t in range(n_test):\n",
    "        T_t = T_te[t, :].reshape(1, -1)  # Shape: (1, 90)\n",
    "        W_t = W_te[t, :].reshape(1, -1)  # Shape: (1, n_confounders)\n",
    "        \n",
    "        # Get baseline prediction\n",
    "        Y_base_folds = []\n",
    "        for model in est.models_y[0]:\n",
    "            pred = model.predict(W_t)  # Should be shape (1, 20)\n",
    "            Y_base_folds.append(pred)\n",
    "        \n",
    "        Y_base = np.mean(Y_base_folds, axis=0)  # Shape: (1, 20)\n",
    "        \n",
    "        treatment_effect = T_t @ theta.T  # (1, 20p) @ (20p, 20) = (1, 20)\n",
    "        \n",
    "        # Compute final prediction\n",
    "        Y_hat_t = Y_base + treatment_effect  # Both should be (1, 20)\n",
    "        predictions[t, :] = Y_hat_t.flatten()\n",
    "        \n",
    "        if t == 0:  # Debug first prediction\n",
    "            print(f\"First prediction shapes:\")\n",
    "            print(f\"  Y_base: {Y_base.shape}\")\n",
    "            print(f\"  treatment_effect: {treatment_effect.shape}\")\n",
    "            print(f\"  Y_hat_t: {Y_hat_t.shape}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def corrected_predict_single_day(est, T_t, W_t):\n",
    "    \"\"\"Make prediction for a single day - more efficient version\"\"\"\n",
    "    # Ensure inputs are 2D\n",
    "    T_t = T_t.reshape(1, -1) if T_t.ndim == 1 else T_t  # Shape: (1, n_features)\n",
    "    W_t = W_t.reshape(1, -1) if W_t.ndim == 1 else W_t  # Shape: (1, n_confounders)\n",
    "    \n",
    "    # Get treatment effects\n",
    "    theta = est.const_marginal_ate()\n",
    "    \n",
    "    # Get baseline prediction by averaging across CV folds\n",
    "    Y_base_folds = []\n",
    "    for model in est.models_y[0]:\n",
    "        pred = model.predict(W_t)  # Shape: (1, n_assets)\n",
    "        Y_base_folds.append(pred)\n",
    "    \n",
    "    Y_base = np.mean(Y_base_folds, axis=0)  # Shape: (1, n_assets)\n",
    "    \n",
    "    # Calculate treatment effect\n",
    "    treatment_effect = T_t @ theta.T  # (1, n_features) @ (n_features, n_assets) = (1, n_assets)\n",
    "    \n",
    "    # Final prediction\n",
    "    Y_hat = Y_base + treatment_effect  # Both are (1, n_assets)\n",
    "    \n",
    "    return Y_hat.flatten()  # Return as 1D array of shape (n_assets,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 5. Usage examples -----------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage with proper lags\n",
    "    # Assuming you have Y_df (returns) and W_df (confounders) defined\n",
    "    \n",
    "    # Test with different numbers of lags\n",
    "    for p in [1, 3, 5]:\n",
    "        print(f\"\\n=== Testing with {p} lags ===\")\n",
    "        results = predict_double_ml_full_with_lags(\n",
    "            Y_df, filtered_confound_df, \n",
    "            p=p,\n",
    "            model_y_name='extra_trees',\n",
    "            model_t_name='extra_trees',\n",
    "            train_ratio=0.7\n",
    "        )\n",
    "        print(f\"RMSE with {p} lags: {results['rmse_overall']:.6f}\")\n",
    "    \n",
    "    # Rolling window evaluation with lags\n",
    "    print(f\"\\n=== Rolling Window Evaluation with Lags ===\")\n",
    "    rolling_results = rolling_window_evaluation_lookback_with_lags(\n",
    "        Y_df, filtered_confound_df,\n",
    "        p=3,  # Use 3 lags\n",
    "        window_size=20,\n",
    "        lookback_days=252,\n",
    "        train_ratio = 0.95,\n",
    "    )\n",
    "    print(f\"Rolling window RMSE: {rolling_results['overall_rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5fa4e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rolling Window Evaluation with Lags ===\n",
      "Created 20 treatment variables from 1 lags of 20 assets\n",
      "Sample treatment columns: ['AA_lag1', 'ABM_lag1', 'ABT_lag1', 'ADI_lag1', 'ADM_lag1']\n",
      "Starting rolling window evaluation with 1 lags...\n",
      "Lookback window: 252 days\n",
      "Initial training size: 5014\n",
      "Test period: 264 days\n",
      "RMSE window size: 20 days\n",
      "Processing day 1/264 (0.0%)\n",
      "Processing day 11/264 (3.8%)\n",
      "Processing day 21/264 (7.6%)\n",
      "Processing day 31/264 (11.4%)\n",
      "Processing day 41/264 (15.2%)\n",
      "Processing day 51/264 (18.9%)\n",
      "Processing day 61/264 (22.7%)\n",
      "Processing day 71/264 (26.5%)\n",
      "Processing day 81/264 (30.3%)\n",
      "Processing day 91/264 (34.1%)\n",
      "Processing day 101/264 (37.9%)\n",
      "Processing day 111/264 (41.7%)\n",
      "Processing day 121/264 (45.5%)\n",
      "Processing day 131/264 (49.2%)\n",
      "Processing day 141/264 (53.0%)\n",
      "Processing day 151/264 (56.8%)\n",
      "Processing day 161/264 (60.6%)\n",
      "Processing day 171/264 (64.4%)\n",
      "Processing day 181/264 (68.2%)\n",
      "Processing day 191/264 (72.0%)\n",
      "Processing day 201/264 (75.8%)\n",
      "Processing day 211/264 (79.5%)\n",
      "Processing day 221/264 (83.3%)\n",
      "Processing day 231/264 (87.1%)\n",
      "Processing day 241/264 (90.9%)\n",
      "Processing day 251/264 (94.7%)\n",
      "Processing day 261/264 (98.5%)\n",
      "\n",
      "Evaluation complete!\n",
      "Overall RMSE: 0.034236\n",
      "Average window RMSE: 0.030607\n",
      "Min window RMSE: 0.012368\n",
      "Max window RMSE: 0.087453\n",
      "Average training time per day: 1.00 seconds\n",
      "Total training time: 4.42 minutes\n",
      "Rolling window RMSE: 0.034236\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"\\n=== Rolling Window Evaluation with Lags ===\")\n",
    "    rolling_results_2 = rolling_window_evaluation_lookback_with_lags(\n",
    "        Y_df, W_df,\n",
    "        p=1,  # Use 1 lags\n",
    "        window_size=20,\n",
    "        lookback_days=252,\n",
    "        train_ratio = 0.95,\n",
    "    )\n",
    "    print(f\"Rolling window RMSE: {rolling_results_2['overall_rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def plot_rolling_rmse_detailed(results, figsize=(15, 10)):\n",
    "    \"\"\"Create a comprehensive visualization of rolling window RMSE results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle(f'Rolling Window RMSE Analysis (Window Size: {results[\"window_size\"]} days)', \n",
    "                 fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. Rolling RMSE over time\n",
    "    ax1 = axes[0, 0]\n",
    "    window_rmses = results['window_rmses']\n",
    "    window_start_indices = range(len(window_rmses))\n",
    "    \n",
    "    ax1.plot(window_start_indices, window_rmses, 'b-', linewidth=2, alpha=0.7)\n",
    "    ax1.axhline(y=results['overall_rmse'], color='r', linestyle='--', \n",
    "                linewidth=2, label=f'Overall RMSE: {results[\"overall_rmse\"]:.6f}')\n",
    "    ax1.axhline(y=np.mean(window_rmses), color='g', linestyle='--', \n",
    "                linewidth=2, label=f'Avg Window RMSE: {np.mean(window_rmses):.6f}')\n",
    "    \n",
    "    # Add confidence bands\n",
    "    rmse_std = np.std(window_rmses)\n",
    "    rmse_mean = np.mean(window_rmses)\n",
    "    ax1.fill_between(window_start_indices, \n",
    "                     rmse_mean - rmse_std, \n",
    "                     rmse_mean + rmse_std, \n",
    "                     alpha=0.2, color='gray', \n",
    "                     label=f'±1 STD: {rmse_std:.6f}')\n",
    "    \n",
    "    ax1.set_xlabel('Window Number')\n",
    "    ax1.set_ylabel('RMSE')\n",
    "    ax1.set_title('Rolling Window RMSE Over Time')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. RMSE Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(window_rmses, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.axvline(x=np.mean(window_rmses), color='g', linestyle='--', linewidth=2)\n",
    "    ax2.axvline(x=np.median(window_rmses), color='orange', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('RMSE')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Window RMSEs')\n",
    "    ax2.legend(['Mean', 'Median', 'Distribution'])\n",
    "    \n",
    "    # Add statistics text\n",
    "    stats_text = f'Mean: {np.mean(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Median: {np.median(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Std: {np.std(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Min: {np.min(window_rmses):.6f}\\n'\n",
    "    stats_text += f'Max: {np.max(window_rmses):.6f}'\n",
    "    ax2.text(0.98, 0.97, stats_text, transform=ax2.transAxes, \n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 3. Per-Asset RMSE\n",
    "    ax3 = axes[1, 0]\n",
    "    rmse_per_asset = results['rmse_per_asset']\n",
    "    asset_names = results['asset_names']\n",
    "    \n",
    "    # Show top 10 assets by RMSE\n",
    "    n_assets_to_show = min(10, len(asset_names))\n",
    "    sorted_indices = np.argsort(rmse_per_asset)[::-1][:n_assets_to_show]\n",
    "    \n",
    "    y_pos = np.arange(n_assets_to_show)\n",
    "    ax3.barh(y_pos, rmse_per_asset[sorted_indices], alpha=0.7)\n",
    "    ax3.set_yticks(y_pos)\n",
    "    ax3.set_yticklabels([asset_names[i] for i in sorted_indices])\n",
    "    ax3.set_xlabel('RMSE')\n",
    "    ax3.set_title(f'Top {n_assets_to_show} Assets by RMSE')\n",
    "    ax3.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add average line\n",
    "    ax3.axvline(x=np.mean(rmse_per_asset), color='r', linestyle='--', \n",
    "                linewidth=2, label='Average')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Cumulative prediction error over time\n",
    "    ax4 = axes[1, 1]\n",
    "    predictions = results['predictions']\n",
    "    actuals = results['actuals']\n",
    "    \n",
    "    # Calculate cumulative squared error\n",
    "    squared_errors = (predictions - actuals) ** 2\n",
    "    cumulative_mse = np.cumsum(np.mean(squared_errors, axis=1))\n",
    "    cumulative_rmse = np.sqrt(cumulative_mse / np.arange(1, len(cumulative_mse) + 1))\n",
    "    \n",
    "    ax4.plot(cumulative_rmse, 'b-', linewidth=2)\n",
    "    ax4.set_xlabel('Prediction Day')\n",
    "    ax4.set_ylabel('Cumulative RMSE')\n",
    "    ax4.set_title('Cumulative RMSE Over Test Period')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add final value annotation\n",
    "    final_rmse = cumulative_rmse[-1]\n",
    "    ax4.text(0.98, 0.02, f'Final: {final_rmse:.6f}', \n",
    "             transform=ax4.transAxes, \n",
    "             verticalalignment='bottom', \n",
    "             horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_rmse_time_series(results, figsize=(14, 8)):\n",
    "    \"\"\"Plot RMSE time series with dates on x-axis\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, \n",
    "                                   gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Extract data\n",
    "    window_rmses = results['window_rmses']\n",
    "    window_dates = results['window_dates']\n",
    "    \n",
    "    # Get start dates for each window\n",
    "    window_start_dates = [dates[0] for dates in window_dates]\n",
    "    \n",
    "    # Convert to pandas for easier plotting\n",
    "    rmse_series = pd.Series(window_rmses, index=pd.to_datetime(window_start_dates))\n",
    "    \n",
    "    # Plot 1: RMSE over time\n",
    "    ax1.plot(rmse_series.index, rmse_series.values, 'b-', linewidth=2, label='Window RMSE')\n",
    "    \n",
    "    # Add moving average\n",
    "    ma_window = 10\n",
    "    rmse_ma = rmse_series.rolling(window=ma_window, center=True).mean()\n",
    "    ax1.plot(rmse_ma.index, rmse_ma.values, 'r-', linewidth=2, \n",
    "             label=f'{ma_window}-window MA', alpha=0.8)\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax1.axhline(y=results['overall_rmse'], color='green', linestyle='--', \n",
    "                linewidth=1.5, label=f'Overall RMSE: {results[\"overall_rmse\"]:.6f}')\n",
    "    \n",
    "    # Highlight high RMSE periods\n",
    "    threshold = np.percentile(window_rmses, 90)\n",
    "    high_rmse_mask = rmse_series > threshold\n",
    "    if high_rmse_mask.any():\n",
    "        ax1.fill_between(rmse_series.index, 0, 1, \n",
    "                        where=high_rmse_mask,\n",
    "                        transform=ax1.get_xaxis_transform(),\n",
    "                        alpha=0.2, color='red', \n",
    "                        label=f'Top 10% RMSE (>{threshold:.6f})')\n",
    "    \n",
    "    ax1.set_ylabel('RMSE')\n",
    "    ax1.set_title(f'Rolling {results[\"window_size\"]}-Day Window RMSE Over Time')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 2: Number of days with RMSE above/below average\n",
    "    ax2.bar(rmse_series.index, \n",
    "           (rmse_series > rmse_series.mean()).astype(int),\n",
    "           width=1, alpha=0.6, color='red', label='Above Average')\n",
    "    ax2.bar(rmse_series.index, \n",
    "           (rmse_series <= rmse_series.mean()).astype(int) * -1,\n",
    "           width=1, alpha=0.6, color='green', label='Below Average')\n",
    "    \n",
    "    ax2.set_ylabel('Above/Below\\nAverage')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax2.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_asset_heatmap(results, figsize=(12, 8)):\n",
    "    \"\"\"Create a heatmap of per-asset RMSE over time windows\"\"\"\n",
    "    \n",
    "    per_asset_window_rmses = results.get('per_asset_window_rmses')\n",
    "    if per_asset_window_rmses is None:\n",
    "        print(\"No per-asset window RMSE data available\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame for heatmap\n",
    "    asset_names = results['asset_names']\n",
    "    n_windows = len(per_asset_window_rmses)\n",
    "    \n",
    "    # Reshape data for heatmap\n",
    "    heatmap_data = pd.DataFrame(\n",
    "        per_asset_window_rmses.T,  # Transpose to have assets as rows\n",
    "        index=asset_names,\n",
    "        columns=[f'W{i}' for i in range(n_windows)]\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data, \n",
    "                cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'RMSE'},\n",
    "                ax=ax,\n",
    "                fmt='.6f',\n",
    "                linewidths=0.5)\n",
    "    \n",
    "    ax.set_title('Per-Asset RMSE Across Time Windows')\n",
    "    ax.set_xlabel('Window Number')\n",
    "    ax.set_ylabel('Asset')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Show every nth label if too many windows\n",
    "    if n_windows > 50:\n",
    "        nth = n_windows // 20\n",
    "        for i, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "            if i % nth != 0:\n",
    "                label.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_prediction_vs_actual_sample(results, n_assets_to_show=3, n_days_to_show=100):\n",
    "    \"\"\"Plot sample predictions vs actuals for selected assets\"\"\"\n",
    "    \n",
    "    predictions = results['predictions']\n",
    "    actuals = results['actuals']\n",
    "    asset_names = results['asset_names']\n",
    "    dates = results['prediction_dates']\n",
    "    \n",
    "    # Select assets to show (best, median, worst by RMSE)\n",
    "    rmse_per_asset = results['rmse_per_asset']\n",
    "    sorted_indices = np.argsort(rmse_per_asset)\n",
    "    \n",
    "    assets_to_show = [\n",
    "        sorted_indices[0],  # Best\n",
    "        sorted_indices[len(sorted_indices)//2],  # Median\n",
    "        sorted_indices[-1]  # Worst\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_assets_to_show, 1, figsize=(14, 4*n_assets_to_show))\n",
    "    if n_assets_to_show == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Limit days to show\n",
    "    end_idx = min(n_days_to_show, len(dates))\n",
    "    date_range = dates[:end_idx]\n",
    "    \n",
    "    for idx, (ax, asset_idx) in enumerate(zip(axes, assets_to_show)):\n",
    "        asset_name = asset_names[asset_idx]\n",
    "        asset_rmse = rmse_per_asset[asset_idx]\n",
    "        \n",
    "        # Plot predictions and actuals\n",
    "        ax.plot(date_range, actuals[:end_idx, asset_idx], \n",
    "                'b-', label='Actual', linewidth=2, alpha=0.7)\n",
    "        ax.plot(date_range, predictions[:end_idx, asset_idx], \n",
    "                'r--', label='Predicted', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Add error bands\n",
    "        errors = predictions[:end_idx, asset_idx] - actuals[:end_idx, asset_idx]\n",
    "        ax.fill_between(date_range, \n",
    "                       predictions[:end_idx, asset_idx] - np.std(errors),\n",
    "                       predictions[:end_idx, asset_idx] + np.std(errors),\n",
    "                       alpha=0.2, color='red', label='±1 STD Error')\n",
    "        \n",
    "        ax.set_title(f'{asset_name} - RMSE: {asset_rmse:.6f}')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format x-axis\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    axes[-1].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage with the results from rolling window evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have results from the rolling window evaluation\n",
    "    # results = rolling_window_evaluation_lookback(...)\n",
    "    \n",
    "    # Plot all visualizations\n",
    "    fig1 = plot_rolling_rmse_detailed(rolling_results)\n",
    "    plt.show()\n",
    "    \n",
    "    fig2 = plot_rmse_time_series(rolling_results)\n",
    "    plt.show()\n",
    "    \n",
    "    fig3 = plot_asset_heatmap(rolling_results)\n",
    "    plt.show()\n",
    "    \n",
    "    fig4 = plot_prediction_vs_actual_sample(rolling_results, n_assets_to_show=3)\n",
    "    plt.show()\n",
    "    \n",
    "    # print(\"Plotting functions ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599cfac4",
   "metadata": {},
   "source": [
    "### OR-VARACLE(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a41e0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from scipy.stats import norm\n",
    "from econml.inference import StatsModelsInference\n",
    "from econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "feb35df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker            AA       ABM       ABT       ADI       ADM       ADX  \\\n",
      "date                                                                     \n",
      "2000-01-03 -0.013042 -0.009188 -0.007117 -0.036071  0.000000 -0.001867   \n",
      "2000-01-04  0.010043  0.012346 -0.012786 -0.044261  0.005277 -0.005666   \n",
      "2000-01-05  0.047628 -0.006192  0.011111  0.014493 -0.015915  0.000000   \n",
      "2000-01-06 -0.011713  0.000000  0.032553 -0.027719  0.010695  0.005742   \n",
      "2000-01-07 -0.016118  0.003091  0.028573  0.033654  0.005249  0.003810   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2020-12-24 -0.017156 -0.011338  0.008434  0.002495  0.000607 -0.000584   \n",
      "2020-12-28  0.006315  0.009932 -0.010796 -0.011887 -0.002010  0.004078   \n",
      "2020-12-29 -0.004527 -0.024098 -0.001568 -0.010525 -0.008056 -0.004066   \n",
      "2020-12-30  0.039099 -0.002646 -0.002303  0.010522  0.009259  0.001745   \n",
      "2020-12-31  0.002172  0.005565  0.012129  0.008361  0.007367  0.002896   \n",
      "\n",
      "ticker           AEE       AEG       AEM       AEP       AES       AFG  \\\n",
      "date                                                                     \n",
      "2000-01-03 -0.007707  0.014739 -0.008658 -0.017734 -0.030563 -0.031594   \n",
      "2000-01-04  0.000000 -0.018361  0.018019  0.013848 -0.031805 -0.015038   \n",
      "2000-01-05  0.037955 -0.012089 -0.017858  0.036648  0.009817 -0.020514   \n",
      "2000-01-06 -0.003731 -0.003409  0.026907  0.013270  0.004415  0.012707   \n",
      "2000-01-07  0.012975  0.000667  0.026202  0.007463  0.022728  0.037087   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2020-12-24  0.008141  0.000000  0.012527  0.007149 -0.009835  0.008256   \n",
      "2020-12-28  0.001565 -0.005128 -0.027193 -0.005643 -0.007696 -0.015556   \n",
      "2020-12-29 -0.003650 -0.012772  0.004506 -0.003320 -0.002999 -0.014001   \n",
      "2020-12-30  0.004819  0.000000  0.005840  0.005659 -0.008741  0.016619   \n",
      "2020-12-31  0.010301  0.002535 -0.027694  0.021117  0.025427  0.005608   \n",
      "\n",
      "ticker           AFL       AIG       AIN       AIR       AIV       AJG  \\\n",
      "date                                                                     \n",
      "2000-01-03 -0.047725 -0.030207 -0.024491 -0.014135 -0.045024 -0.001963   \n",
      "2000-01-04  0.007199 -0.028734 -0.046917  0.003552 -0.013245 -0.036368   \n",
      "2000-01-05 -0.007189  0.002216  0.021787  0.003515  0.009950  0.014448   \n",
      "2000-01-06  0.024339  0.028627 -0.017468  0.017575  0.021260 -0.001795   \n",
      "2000-01-07  0.021024  0.055146  0.034486  0.034250  0.011263  0.019961   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2020-12-24  0.003885 -0.002939 -0.011157 -0.019532 -0.005994  0.003246   \n",
      "2020-12-28 -0.003636 -0.012865 -0.007210  0.000559  0.015873 -0.008901   \n",
      "2020-12-29 -0.009328 -0.005368 -0.017858 -0.014574 -0.012000 -0.015432   \n",
      "2020-12-30  0.003186  0.010988  0.013491  0.014196  0.017875 -0.003445   \n",
      "2020-12-31  0.010398  0.008755  0.005189 -0.003583  0.034686  0.014247   \n",
      "\n",
      "ticker           ALB       ALK  \n",
      "date                            \n",
      "2000-01-03 -0.070381 -0.014185  \n",
      "2000-01-04  0.013986 -0.017922  \n",
      "2000-01-05  0.017212 -0.001797  \n",
      "2000-01-06  0.040410  0.000899  \n",
      "2000-01-07 -0.013334  0.028121  \n",
      "...              ...       ...  \n",
      "2020-12-24 -0.001736  0.000000  \n",
      "2020-12-28 -0.011092 -0.007012  \n",
      "2020-12-29 -0.022050 -0.018094  \n",
      "2020-12-30  0.007530  0.017160  \n",
      "2020-12-31 -0.008370 -0.011662  \n",
      "\n",
      "[5279 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "print(Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-causal lag selection example:\n",
      "Alpha for lag 2: 0.2000\n",
      "Alpha for lag 3: 0.2000\n",
      "Alpha for lag 4: 0.2000\n",
      "Alpha for lag 5: 0.2000\n",
      "(999, 20) (999, 20) (999, 14)\n",
      "0.12154139808213671\n",
      "0.9999957149887668\n",
      "✓ Significance detected for lag 2 (p-value: 0.1215)\n",
      "(997, 20) (997, 40) (997, 21)\n",
      "0.0029005217773381275\n",
      "0.9997929620968218\n",
      "✓ Significance detected for lag 3 (p-value: 0.0029)\n",
      "(994, 20) (994, 60) (994, 28)\n",
      "0.06279661633682802\n",
      "0.999775610096513\n",
      "✓ Significance detected for lag 4 (p-value: 0.0628)\n",
      "(990, 20) (990, 80) (990, 35)\n",
      "0.3221580785496651\n",
      "0.9998516660691872\n",
      "✗ No significance or drift for lag 5 (p_sig: 0.3222, p_drift: 0.9999)\n",
      "{'t-1': 'Treatment', 't-2': 'Treatment', 't-3': 'Treatment', 't-4': 'Treatment'}\n"
     ]
    }
   ],
   "source": [
    "p_max = 5 # Maximum number of lags to consider\n",
    "\n",
    "def auto_causal_lag_selection(p_max, Y_df, confound_df, \n",
    "                              model_y_name='extra_trees', \n",
    "                              model_t_name='extra_trees',\n",
    "                              model_y_params=None,\n",
    "                              model_t_params=None,\n",
    "                              cv_folds=5):\n",
    "    \"\"\"\n",
    "    Automatically select the best number of lags for causal inference\n",
    "    using DML and OVB analysis.\n",
    "    \"\"\"\n",
    "    if model_y_params is None:\n",
    "        model_y_params = {}\n",
    "    if model_t_params is None:\n",
    "        model_t_params = {}\n",
    "\n",
    "    n_assets = Y_df.shape[1]\n",
    "\n",
    "    # Create lagged treatment variables\n",
    "    T_df = Y_df.shift(1).add_suffix('_lag1')\n",
    "    W_df = pd.concat([confound_df, confound_df.shift(1).add_suffix('_lag1')], axis=1)\n",
    "        \n",
    "    def realign(Y,T,W):\n",
    "        full = pd.concat([Y, T, W], axis=1).dropna()\n",
    "        Y_cols = Y.columns\n",
    "        T_cols = T.columns\n",
    "        W_cols = W.columns\n",
    "        return full[Y_cols], full[T_cols], full[W_cols]\n",
    "\n",
    "    lagged_assignments = {\"t-1\": \"Treatment\"}\n",
    "\n",
    "    if p_max > 1:\n",
    "        \"\"\" Alpha spending code \n",
    "\n",
    "        # We initialize the alpha-spending function for multiple-hypothesis testing downstream.\n",
    "        # This is done via controlling the FWER error rate (FWER) using the alpha-spending method.\n",
    "        # We do this via the O'Brien-Fleming approach, with each tests contributing \"equally\" to the total \"information\"\n",
    "        # Initialize the alpha-spending function\n",
    "        alpha_global = 0.05\n",
    "        # equally spaced information fractions\n",
    "        info = np.linspace(1/(p_max-1), 1, p_max-1)           \n",
    "        info_prev = np.hstack(([0.0], info[:-1]))             \n",
    "\n",
    "        def spend_OBF(t, alpha):\n",
    "            z = norm.ppf(1 - alpha/2)\n",
    "            return 2 * (1 - norm.cdf(z / np.sqrt(t)))\n",
    "        \n",
    "        # Calculate the alpha (significance level) for each test\n",
    "        alpha_all = spend_OBF(info, alpha_global) - spend_OBF(info_prev, alpha_global)\n",
    "        \"\"\" \n",
    "\n",
    "        alpha_all = [0.2]*(p_max-1)\n",
    "        for i in range(len(alpha_all)):\n",
    "            print(f\"Alpha for lag {i+2}: {alpha_all[i]:.4f}\")\n",
    "\n",
    "        def create_index_mapping(T_names_prev, T_names_post, d_y):\n",
    "            \"\"\"\n",
    "            Create index mapping as 2-tuples (pre_idx, post_idx) for coefficient comparison.\n",
    "        \n",
    "            Returns:\n",
    "            - idx_pairs: List of (pre_idx, post_idx) tuples for existing coefficients\n",
    "            - idx_new: List of post_idx for new coefficients\n",
    "            \"\"\"\n",
    "            d_T_prev = len(T_names_prev)\n",
    "            d_T_post = len(T_names_post)\n",
    "\n",
    "            idx_pairs = []\n",
    "            idx_new = []\n",
    "\n",
    "            # Create mapping for all coefficients\n",
    "            for y in range(d_y):\n",
    "                for j_post, name_post in enumerate(T_names_post):\n",
    "                    post_idx = y * d_T_post + j_post\n",
    "                    \n",
    "                    if name_post in T_names_prev:\n",
    "                        # This is an existing coefficient\n",
    "                        j_prev = T_names_prev.index(name_post)\n",
    "                        pre_idx = y * d_T_prev + j_prev\n",
    "                        idx_pairs.append((pre_idx, post_idx))\n",
    "                    else:\n",
    "                        # This is a new coefficient\n",
    "                        idx_new.append(post_idx)\n",
    "            \n",
    "            return idx_pairs, idx_new\n",
    "\n",
    "\n",
    "    for p in range(2, p_max + 1):\n",
    "        \n",
    "        #### Pre; ie p - 1\n",
    "        Y_df, T_df, W_df = realign(Y_df, T_df, W_df)\n",
    "        est_pre = LinearDML(\n",
    "            model_y=get_regressor(model_y_name, force_multioutput=False, **model_y_params),\n",
    "            model_t=get_regressor(model_t_name, force_multioutput=False, **model_t_params),\n",
    "            cv=TimeSeriesSplit(n_splits=cv_folds),\n",
    "            discrete_treatment=False,\n",
    "            random_state=0\n",
    "        )\n",
    "        print(Y_df.shape, T_df.shape, W_df.shape)\n",
    "\n",
    "        est_pre.fit(Y_df, T_df, X=None, W=W_df, inference=StatsModelsInference())\n",
    "        est_pre_inf = est_pre.const_marginal_ate_inference()\n",
    "        #print(est_pre_inf)\n",
    "        theta_pre = est_pre_inf.mean_point.ravel()  # Flatten to 1-D vector\n",
    "        cov_pre = est_pre_inf.mean_pred_stderr.ravel()\n",
    "\n",
    "        #### Post; ie p\n",
    "        T_df_temp = pd.concat([T_df, Y_df.shift(p).add_suffix(f'_lag{p}')], axis=1).dropna()\n",
    "        W_df = pd.concat([W_df, confound_df.shift(p).add_suffix(f'_lag{p}')], axis=1).dropna()\n",
    "        \n",
    "        Y_df, T_df_temp, W_df = realign(Y_df, T_df_temp, W_df)\n",
    "\n",
    "        est_post = LinearDML(\n",
    "            model_y=get_regressor(model_y_name, force_multioutput=False, **model_y_params),\n",
    "            model_t=get_regressor(model_t_name, force_multioutput=False, **model_t_params),\n",
    "            cv=TimeSeriesSplit(cv_folds),\n",
    "            discrete_treatment=False,\n",
    "            random_state=0,\n",
    "        )\n",
    "\n",
    "        est_post.fit(Y_df, T_df_temp, X=None, W=W_df)\n",
    "        est_post_inf = est_post.const_marginal_ate_inference()\n",
    "        #print(est_post_inf)\n",
    "        theta_post = est_post_inf.mean_point.ravel()  \n",
    "        cov_post = est_post_inf.mean_pred_stderr.ravel()\n",
    "        \n",
    "        T_names_pre = T_df.columns.tolist()\n",
    "        T_names_post = T_df_temp.columns.tolist()\n",
    "        d_y   = Y_df.shape[1]                    # number of outcome series\n",
    "\n",
    "        idx_pairs, idx_post = create_index_mapping(T_names_pre, T_names_post, d_y)\n",
    "\n",
    "        # Within each test, we perform a Benjamini–Hochberg FDR test\n",
    "        # p-values of signifinance\n",
    "        p_sig_store = []\n",
    "        for i in idx_post:\n",
    "            z_stat = theta_post[i] / cov_post[i]\n",
    "            p_value = 2 * (1 - norm.cdf(np.abs(z_stat)))\n",
    "            p_sig_store.append(p_value)\n",
    "        p_sig = min(multipletests(p_sig_store, alpha=0.05, method=\"fdr_bh\")[1])\n",
    "        print(p_sig)\n",
    "\n",
    "        # p-value of drift without significance\n",
    "        p_drift_store = []\n",
    "        for i,j in idx_pairs:\n",
    "            z_stat = (theta_post[j] - theta_pre[i])/ np.sqrt(cov_post[j]**2 + cov_pre[i]**2)\n",
    "            p_value = 2 * (1 - norm.cdf(np.abs(z_stat)))\n",
    "            p_drift_store.append(p_value)\n",
    "        p_drift = min(multipletests(p_drift_store, alpha=0.05, method=\"fdr_bh\")[1])\n",
    "        print(p_drift)\n",
    "\n",
    "        alpha = alpha_all[p-2]\n",
    "        # Case 1: Significance of new coefficients\n",
    "        if p_sig < alpha:\n",
    "            print(f\"✓ Significance detected for lag {p} (p-value: {p_sig:.4f})\")\n",
    "            lagged_assignments[f't-{p}'] = 'Treatment'\n",
    "            T_df = T_df_temp\n",
    "            continue\n",
    "\n",
    "        # Case 2: Drift without Significance\n",
    "        elif p_drift < alpha:\n",
    "            print(f\"✓ Drift detected for lag {p} (p-value: {p_drift:.4f})\")\n",
    "            lagged_assignments[f't-{p}'] = 'Confounding'\n",
    "            W_df = pd.concat([W_df, Y_df.shift(p).add_suffix(f'_lag{p}')], axis=1).dropna()\n",
    "            continue\n",
    "\n",
    "        # Case 3: Neither\n",
    "        else:\n",
    "            print(f\"✗ No significance or drift for lag {p} (p_sig: {p_sig:.4f}, p_drift: {p_drift:.4f})\")\n",
    "            p = p - 1\n",
    "            break\n",
    "\n",
    "    output = {\n",
    "        'lagged_assignments': lagged_assignments,\n",
    "        'final_lagged_treatment': T_df.columns.tolist(),\n",
    "        'final_confounders': W_df.columns.tolist(),\n",
    "        'W_df': W_df,\n",
    "        'T_df': T_df,\n",
    "        'Optimal p': p,\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "print(\"Auto-causal lag selection example:\")\n",
    "print(auto_causal_lag_selection(\n",
    "    p_max=5, \n",
    "    Y_df=Y_df, \n",
    "    confound_df=filtered_confound_df.iloc[-1000:,:],\n",
    "    model_y_name='extra_trees',\n",
    "    model_t_name='extra_trees',\n",
    "    model_y_params={'n_estimators': 100},\n",
    "    model_t_params={'n_estimators': 100},\n",
    "    cv_folds=5\n",
    ")['lagged_assignments'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fbbeeb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallelized_runs import calculate_pnl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2597dd",
   "metadata": {},
   "source": [
    "### Plotting Purposes (for Slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79a0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3692 5275\n",
      "DatetimeIndex(['2010-09-15', '2014-09-17'], dtype='datetime64[ns]', name='date', freq=None)\n",
      "DatetimeIndex(['2014-09-18', '2020-12-31'], dtype='datetime64[ns]', name='date', freq=None)\n",
      "Y_base_array shape: (5, 1, 20)\n",
      "Y_base shape after averaging: (1, 20)\n",
      "Theta shape: (20, 80)\n",
      "Y_hat_next shape: (1, 20)\n",
      "1‑step‑ahead forecast: [0.   0.01 0.   0.01 0.01] ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calculate_pnl() got an unexpected keyword argument 'pnl_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[177]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     85\u001b[39m Y_true_next = Y_te[\u001b[32m0\u001b[39m, :]\n\u001b[32m     86\u001b[39m rmse_next = root_mean_squared_error(Y_true_next, Y_hat_next.flatten())\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m pnl_next = \u001b[43mcalculate_pnl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_hat_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_true_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpnl_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweighted\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRMSE for 1-step-ahead point: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_next\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: calculate_pnl() got an unexpected keyword argument 'pnl_strategy'"
     ]
    }
   ],
   "source": [
    "p = 4                           # how many lags\n",
    "def make_lags(df, p):\n",
    "    return pd.concat([df.shift(k).add_suffix(f'_lag{k}') for k in range(1, p+1)], axis=1)\n",
    "\n",
    "#Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "Y_df = returns_df_cleaned.iloc[:, :20]\n",
    "T_df = make_lags(Y_df, p)\n",
    "W_df = make_lags(filtered_confound_df, p)\n",
    "#W_df = make_lags(filtered_confound_df,p)\n",
    "\n",
    "full = pd.concat([Y_df, T_df, W_df], axis=1).dropna()   # drop the first p rows with NaNs\n",
    "train_size = int(len(full)*0.7)\n",
    "print(train_size, len(full))\n",
    "\n",
    "train, test = full.iloc[train_size - 252*4:train_size], full.iloc[train_size:]\n",
    "print(train.index[[0,-1]])   # sanity‑check the split dates\n",
    "print(test.index[[0,-1]])\n",
    "\n",
    "\n",
    "# --- 2. align everything & turn into np.arrays ----------------------------\n",
    "Y_cols = Y_df.columns\n",
    "T_cols = T_df.columns\n",
    "W_cols = W_df.columns\n",
    "\n",
    "def to_arrays(df):\n",
    "    return df[Y_cols].values, df[T_cols].values, df[W_cols].values\n",
    "\n",
    "Y_tr, T_tr, W_tr = to_arrays(train)\n",
    "Y_te, T_te, W_te = to_arrays(test)\n",
    "\n",
    "# --- 3. fit Double ML ---------------------------------------------------\n",
    "tscv = TimeSeriesSplit(5)          # same blocked CV inside train only\n",
    "\n",
    "\n",
    "fast_tree = ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "#    fast_tree = RandomForestRegressor(\n",
    "#        n_estimators=100,\n",
    "#        max_depth=5,\n",
    "#        min_samples_split=20,\n",
    "#        random_state=0\n",
    "#    )\n",
    "\n",
    "est = LinearDML(model_y=fast_tree,\n",
    "                model_t=fast_tree,        \n",
    "                cv=tscv,\n",
    "                discrete_treatment=False,\n",
    "                random_state=0)\n",
    "\n",
    "est.fit(Y_tr, T_tr, X=None, W=W_tr)\n",
    "\n",
    "# --- 4. evaluate the model - CORRECTED --------------\n",
    "T_next = T_te[0, :].reshape(1, -1)\n",
    "W_next = W_te[0, :].reshape(1, -1)\n",
    "\n",
    "# The structure is: est.models_y[0] contains the 5 CV fold models\n",
    "Y_base_folds = []\n",
    "for model in est.models_y[0]:  # Note: iterate through est.models_y[0], not est.models_y\n",
    "    pred = model.predict(W_next)  # Shape: (1, 20)\n",
    "    Y_base_folds.append(pred)\n",
    "\n",
    "# Convert to array and average across folds\n",
    "Y_base_array = np.array(Y_base_folds)  # Shape: (5, 1, 20)\n",
    "print(f\"Y_base_array shape: {Y_base_array.shape}\")\n",
    "\n",
    "# Average across folds (axis=0)\n",
    "Y_base = np.mean(Y_base_array, axis=0)  # Shape: (1, 20)\n",
    "print(f\"Y_base shape after averaging: {Y_base.shape}\")\n",
    "\n",
    "# Get treatment effects\n",
    "theta = est.const_marginal_ate()  # Shape: (20, 60)\n",
    "print(f\"Theta shape: {theta.shape}\")\n",
    "\n",
    "# Compute prediction\n",
    "Y_hat_next = Y_base + T_next @ theta.T\n",
    "print(f\"Y_hat_next shape: {Y_hat_next.shape}\")\n",
    "print(\"1‑step‑ahead forecast:\", Y_hat_next.flatten()[:5], \"...\")\n",
    "\n",
    "# Compare with realized returns\n",
    "Y_true_next = Y_te[0, :]\n",
    "rmse_next = root_mean_squared_error(Y_true_next, Y_hat_next.flatten())\n",
    "pnl_next = calculate_pnl(Y_hat_next, Y_true_next)\n",
    "print(f\"RMSE for 1-step-ahead point: {rmse_next:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
